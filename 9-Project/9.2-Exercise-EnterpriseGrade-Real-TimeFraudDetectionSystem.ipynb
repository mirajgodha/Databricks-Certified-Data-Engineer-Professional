{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ff6945-2ec8-4d9e-a6aa-41d239f09c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Fraud Detection System - Databricks Training Exercise\n",
    "## Enterprise-Grade Streaming Analytics Pipeline\n",
    "\n",
    "### üéØ Objective\n",
    "Build a complete real-time fraud detection pipeline using only Databricks-native components. This exercise demonstrates:\n",
    "- Multi-table synthetic data generation\n",
    "- Continuous streaming ingestion and processing\n",
    "- Complex stateful fraud detection rules\n",
    "- Bronze ‚Üí Silver ‚Üí Gold Delta Lake architecture\n",
    "- Enterprise monitoring and fault tolerance\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Architecture Overview](#architecture-overview)\n",
    "2. [Setup and Initialization](#setup-and-initialization)\n",
    "3. [Reference Data Setup](#reference-data-setup)\n",
    "4. [Continuous Data Generators](#continuous-data-generators)\n",
    "5. [Bronze Layer: Raw Data Ingestion](#bronze-layer)\n",
    "6. [Silver Layer: Cleansing and Enrichment](#silver-layer)\n",
    "7. [Stateful Feature Engineering](#stateful-features)\n",
    "8. [Gold Layer: Advanced Fraud Rules](#gold-layer)\n",
    "9. [Monitoring and Operations](#monitoring)\n",
    "10. [Production Considerations](#production-considerations)\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Overview {#architecture-overview}\n",
    "\n",
    "### High-Level Data Flow\n",
    "```\n",
    "Data Generators ‚Üí Bronze (Raw) ‚Üí Silver (Cleansed/Enriched) ‚Üí Gold (Rules/Alerts)\n",
    "       ‚Üì              ‚Üì                    ‚Üì                        ‚Üì\n",
    "   Continuous      Streaming           Stateful              Complex Rules\n",
    "   Synthetic       Ingestion          Features               & Alerts\n",
    "     Data         Deduplication     Velocity/Device          Scoring\n",
    "```\n",
    "\n",
    "### Tables and Streams Architecture\n",
    "```\n",
    "Reference Tables (Static/Slowly Changing):\n",
    "‚îú‚îÄ‚îÄ customers_dim (risk tiers, KYC status)\n",
    "‚îú‚îÄ‚îÄ merchants_dim (categories, risk segments)\n",
    "‚îú‚îÄ‚îÄ device_dim (reputation scores)\n",
    "‚îú‚îÄ‚îÄ blacklist_dim (blocked entities)\n",
    "‚îî‚îÄ‚îÄ geoip_dim (IP geolocation)\n",
    "\n",
    "Bronze Layer (Raw Events):\n",
    "‚îú‚îÄ‚îÄ tx_events_raw (transactions)\n",
    "‚îú‚îÄ‚îÄ login_events_raw (authentication)\n",
    "‚îî‚îÄ‚îÄ chargebacks_raw (ground truth)\n",
    "\n",
    "Silver Layer (Cleansed/Enriched):\n",
    "‚îú‚îÄ‚îÄ tx_events_clean (validated transactions)\n",
    "‚îú‚îÄ‚îÄ tx_events_enriched (with reference data)\n",
    "‚îî‚îÄ‚îÄ ops.customer_velocity (stateful features)\n",
    "\n",
    "Gold Layer (Business Logic):\n",
    "‚îú‚îÄ‚îÄ fraud_suspicions (rule outputs)\n",
    "‚îú‚îÄ‚îÄ fraud_alerts (actionable alerts)\n",
    "‚îî‚îÄ‚îÄ ops.pipeline_metrics (monitoring)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Setup and Initialization {#setup-and-initialization}\n",
    "\n",
    "### Notebook 00: Initialize Objects\n",
    "\n",
    "```sql\n",
    "-- Create catalog and schema structure\n",
    "CREATE CATALOG IF NOT EXISTS fraud_lab;\n",
    "USE CATALOG fraud_lab;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS ref;       -- Reference/dimension tables\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;    -- Raw event streams\n",
    "CREATE SCHEMA IF NOT EXISTS silver;    -- Cleansed and enriched data\n",
    "CREATE SCHEMA IF NOT EXISTS gold;      -- Business logic and alerts\n",
    "CREATE SCHEMA IF NOT EXISTS ops;       -- Operational tables\n",
    "\n",
    "-- Set checkpoint locations\n",
    "```\n",
    "\n",
    "```python\n",
    "# Define checkpoint and storage paths\n",
    "base_path = \"dbfs:/mnt/fraud_lab\"\n",
    "checkpoint_base = f\"{base_path}/checkpoints\"\n",
    "\n",
    "# Configure Spark for streaming\n",
    "spark.conf.set(\"spark.sql.streaming.metricsEnabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.streaming.numRecentProgressUpdates\", \"10\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üóÉÔ∏è Reference Data Setup {#reference-data-setup}\n",
    "\n",
    "### Create Reference Tables\n",
    "\n",
    "```sql\n",
    "-- Customer dimension with risk tiers\n",
    "CREATE TABLE IF NOT EXISTS ref.customers_dim (\n",
    "  customer_id STRING PRIMARY KEY,\n",
    "  risk_tier STRING,                 -- low|medium|high\n",
    "  kyc_status STRING,                -- verified|pending|rejected\n",
    "  country STRING,\n",
    "  signup_timestamp TIMESTAMP,\n",
    "  credit_limit DOUBLE\n",
    ") USING DELTA\n",
    "TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported');\n",
    "\n",
    "-- Merchant dimension with risk categories\n",
    "CREATE TABLE IF NOT EXISTS ref.merchants_dim (\n",
    "  merchant_id STRING PRIMARY KEY,\n",
    "  merchant_name STRING,\n",
    "  category STRING,                  -- electronics, travel, gambling, etc.\n",
    "  country STRING,\n",
    "  risk_segment STRING,              -- normal|watchlist|high_risk\n",
    "  monthly_volume DOUBLE\n",
    ") USING DELTA;\n",
    "\n",
    "-- Device reputation tracking\n",
    "CREATE TABLE IF NOT EXISTS ref.device_dim (\n",
    "  device_id STRING PRIMARY KEY,\n",
    "  device_type STRING,               -- mobile|desktop|tablet\n",
    "  os STRING,\n",
    "  browser STRING,\n",
    "  reputation_score DOUBLE,          -- 0.0 (bad) to 1.0 (good)\n",
    "  first_seen TIMESTAMP,\n",
    "  last_seen TIMESTAMP\n",
    ") USING DELTA;\n",
    "\n",
    "-- Blacklist for known bad actors\n",
    "CREATE TABLE IF NOT EXISTS ref.blacklist_dim (\n",
    "  entity_type STRING,               -- 'customer'|'device'|'ip'|'merchant'|'card'\n",
    "  entity_id STRING,\n",
    "  reason STRING,\n",
    "  severity STRING,                  -- low|medium|high|critical\n",
    "  added_at TIMESTAMP,\n",
    "  expires_at TIMESTAMP\n",
    ") USING DELTA;\n",
    "\n",
    "-- Geolocation mapping (simplified)\n",
    "CREATE TABLE IF NOT EXISTS ref.geoip_dim (\n",
    "  ip_prefix STRING,                 -- e.g., 52.23.0.0/16\n",
    "  country STRING,\n",
    "  region STRING,\n",
    "  city STRING,\n",
    "  latitude DOUBLE,\n",
    "  longitude DOUBLE,\n",
    "  avg_latency_ms INT\n",
    ") USING DELTA;\n",
    "```\n",
    "\n",
    "### Seed Reference Data\n",
    "\n",
    "```sql\n",
    "-- Insert sample customers with different risk profiles\n",
    "INSERT OVERWRITE ref.customers_dim \n",
    "SELECT \n",
    "  CONCAT('C', CAST(id + 1000 AS STRING)) as customer_id,\n",
    "  CASE \n",
    "    WHEN id % 10 IN (0,1) THEN 'high'\n",
    "    WHEN id % 10 IN (2,3,4) THEN 'medium' \n",
    "    ELSE 'low' \n",
    "  END as risk_tier,\n",
    "  CASE \n",
    "    WHEN id % 20 = 0 THEN 'pending'\n",
    "    WHEN id % 50 = 0 THEN 'rejected'\n",
    "    ELSE 'verified'\n",
    "  END as kyc_status,\n",
    "  CASE \n",
    "    WHEN id % 4 = 0 THEN 'US'\n",
    "    WHEN id % 4 = 1 THEN 'IN'\n",
    "    WHEN id % 4 = 2 THEN 'GB'\n",
    "    ELSE 'CA'\n",
    "  END as country,\n",
    "  current_timestamp() - INTERVAL (id * 24) HOURS as signup_timestamp,\n",
    "  CASE \n",
    "    WHEN id % 10 IN (0,1) THEN 50000.0\n",
    "    WHEN id % 10 IN (2,3,4) THEN 25000.0\n",
    "    ELSE 10000.0\n",
    "  END as credit_limit\n",
    "FROM RANGE(0, 500);\n",
    "\n",
    "-- Insert merchants with various risk levels\n",
    "INSERT OVERWRITE ref.merchants_dim\n",
    "SELECT \n",
    "  CONCAT('M', CAST(id + 10 AS STRING)) as merchant_id,\n",
    "  CONCAT('Merchant_', id) as merchant_name,\n",
    "  CASE \n",
    "    WHEN id % 8 = 0 THEN 'gambling'\n",
    "    WHEN id % 8 = 1 THEN 'electronics'\n",
    "    WHEN id % 8 = 2 THEN 'travel'\n",
    "    WHEN id % 8 = 3 THEN 'grocery'\n",
    "    WHEN id % 8 = 4 THEN 'luxury'\n",
    "    WHEN id % 8 = 5 THEN 'pharmacy'\n",
    "    WHEN id % 8 = 6 THEN 'gas_station'\n",
    "    ELSE 'restaurant'\n",
    "  END as category,\n",
    "  CASE \n",
    "    WHEN id % 3 = 0 THEN 'US'\n",
    "    WHEN id % 3 = 1 THEN 'IN'\n",
    "    ELSE 'GB'\n",
    "  END as country,\n",
    "  CASE \n",
    "    WHEN id % 8 = 0 THEN 'high_risk'      -- gambling\n",
    "    WHEN id % 15 = 0 THEN 'watchlist'\n",
    "    ELSE 'normal'\n",
    "  END as risk_segment,\n",
    "  rand() * 1000000 as monthly_volume\n",
    "FROM RANGE(0, 100);\n",
    "\n",
    "-- Insert device profiles\n",
    "INSERT OVERWRITE ref.device_dim\n",
    "SELECT \n",
    "  CONCAT('D', CAST(id + 10000 AS STRING)) as device_id,\n",
    "  CASE \n",
    "    WHEN id % 3 = 0 THEN 'mobile'\n",
    "    WHEN id % 3 = 1 THEN 'desktop'\n",
    "    ELSE 'tablet'\n",
    "  END as device_type,\n",
    "  CASE \n",
    "    WHEN id % 4 = 0 THEN 'Android'\n",
    "    WHEN id % 4 = 1 THEN 'iOS'\n",
    "    WHEN id % 4 = 2 THEN 'Windows'\n",
    "    ELSE 'MacOS'\n",
    "  END as os,\n",
    "  CASE \n",
    "    WHEN id % 3 = 0 THEN 'Chrome'\n",
    "    WHEN id % 3 = 1 THEN 'Safari'\n",
    "    ELSE 'Firefox'\n",
    "  END as browser,\n",
    "  CASE \n",
    "    WHEN id % 20 = 0 THEN 0.1  -- Bad reputation\n",
    "    WHEN id % 15 = 0 THEN 0.3  -- Poor reputation\n",
    "    ELSE 0.7 + (rand() * 0.3)  -- Good reputation\n",
    "  END as reputation_score,\n",
    "  current_timestamp() - INTERVAL (id * 12) HOURS as first_seen,\n",
    "  current_timestamp() - INTERVAL (id % 48) HOURS as last_seen\n",
    "FROM RANGE(0, 2000);\n",
    "\n",
    "-- Insert some blacklisted entities\n",
    "INSERT OVERWRITE ref.blacklist_dim VALUES\n",
    "('ip', '192.168.1.100', 'Known botnet', 'critical', current_timestamp(), current_timestamp() + INTERVAL 365 DAYS),\n",
    "('ip', '10.0.0.50', 'Suspicious activity', 'high', current_timestamp(), current_timestamp() + INTERVAL 90 DAYS),\n",
    "('customer', 'C1005', 'Previous fraud', 'high', current_timestamp(), current_timestamp() + INTERVAL 180 DAYS),\n",
    "('device', 'D10050', 'Compromised device', 'medium', current_timestamp(), current_timestamp() + INTERVAL 30 DAYS),\n",
    "('merchant', 'M15', 'Money laundering', 'critical', current_timestamp(), current_timestamp() + INTERVAL 730 DAYS);\n",
    "\n",
    "-- Insert geo IP mapping (simplified examples)\n",
    "INSERT OVERWRITE ref.geoip_dim VALUES\n",
    "('192.168.0.0/16', 'US', 'California', 'San Francisco', 37.7749, -122.4194, 50),\n",
    "('10.0.0.0/8', 'IN', 'Maharashtra', 'Mumbai', 19.0760, 72.8777, 120),\n",
    "('172.16.0.0/12', 'GB', 'England', 'London', 51.5074, -0.1278, 80),\n",
    "('203.0.113.0/24', 'CA', 'Ontario', 'Toronto', 43.6532, -79.3832, 60);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Continuous Data Generators {#continuous-data-generators}\n",
    "\n",
    "### Notebook 01: Data Generators\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Enhanced synthetic data generation with realistic patterns\n",
    "\n",
    "def random_ip():\n",
    "    \"\"\"Generate random IP addresses with some clustering\"\"\"\n",
    "    # 70% from common ranges, 30% random\n",
    "    if random.random() < 0.7:\n",
    "        ranges = ['192.168', '10.0', '172.16', '203.0']\n",
    "        prefix = random.choice(ranges)\n",
    "        return f\"{prefix}.{random.randint(0,255)}.{random.randint(1,254)}\"\n",
    "    else:\n",
    "        return f\"{random.randint(1,255)}.{random.randint(0,255)}.{random.randint(0,255)}.{random.randint(1,254)}\"\n",
    "\n",
    "def generate_transaction_batch(n=500):\n",
    "    \"\"\"Generate realistic transaction patterns\"\"\"\n",
    "    now = datetime.utcnow()\n",
    "    rows = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Base transaction\n",
    "        customer_id = f\"C{random.randint(1000, 1499)}\"\n",
    "        merchant_id = f\"M{random.randint(10, 109)}\"\n",
    "        device_id = f\"D{random.randint(10000, 11999)}\"\n",
    "        \n",
    "        # Amount distribution (most small, some large)\n",
    "        if random.random() < 0.7:\n",
    "            amount = round(random.uniform(5, 500), 2)  # Normal purchases\n",
    "        elif random.random() < 0.9:\n",
    "            amount = round(random.uniform(500, 2000), 2)  # Medium purchases\n",
    "        else:\n",
    "            amount = round(random.uniform(2000, 15000), 2)  # Large purchases\n",
    "        \n",
    "        # Add some time variation\n",
    "        time_offset = random.randint(0, 300)  # Up to 5 minutes in the past\n",
    "        event_time = now - timedelta(seconds=time_offset)\n",
    "        \n",
    "        row = {\n",
    "            \"event_id\": str(uuid.uuid4()),\n",
    "            \"event_ts\": event_time,\n",
    "            \"customer_id\": customer_id,\n",
    "            \"merchant_id\": merchant_id,\n",
    "            \"amount\": amount,\n",
    "            \"currency\": random.choice([\"USD\", \"EUR\", \"INR\", \"GBP\", \"CAD\"]),\n",
    "            \"country\": random.choice([\"US\", \"IN\", \"GB\", \"DE\", \"CA\", \"FR\", \"AU\", \"JP\"]),\n",
    "            \"device_id\": device_id,\n",
    "            \"channel\": random.choice([\"web\", \"mobile\", \"pos\", \"atm\"]),\n",
    "            \"payment_method\": random.choice([\"credit_card\", \"debit_card\", \"upi\", \"wallet\", \"bank_transfer\"]),\n",
    "            \"ip\": random_ip(),\n",
    "            \"card_last4\": f\"{random.randint(1000, 9999)}\",\n",
    "            \"raw\": json.dumps({\"session_id\": str(uuid.uuid4()), \"user_agent\": \"browser\"})\n",
    "        }\n",
    "        \n",
    "        # Inject suspicious patterns (5% of transactions)\n",
    "        if random.random() < 0.05:\n",
    "            # High-risk scenarios\n",
    "            if random.random() < 0.3:\n",
    "                # International high-value\n",
    "                row[\"amount\"] = round(random.uniform(8000, 20000), 2)\n",
    "                row[\"country\"] = random.choice([\"RU\", \"CN\", \"NG\", \"PK\"])\n",
    "            elif random.random() < 0.6:\n",
    "                # Velocity spike (same customer, multiple rapid transactions)\n",
    "                if i > 0 and random.random() < 0.5:\n",
    "                    row[\"customer_id\"] = rows[-1][\"customer_id\"]\n",
    "                    row[\"device_id\"] = rows[-1][\"device_id\"]\n",
    "                    row[\"event_ts\"] = rows[-1][\"event_ts\"] + timedelta(seconds=random.randint(1, 30))\n",
    "            else:\n",
    "                # Device misuse (same device, different customer)\n",
    "                if i > 0:\n",
    "                    row[\"device_id\"] = rows[-1][\"device_id\"]\n",
    "                    row[\"ip\"] = rows[-1][\"ip\"]\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"event_id\", StringType()),\n",
    "        StructField(\"event_ts\", TimestampType()),\n",
    "        StructField(\"customer_id\", StringType()),\n",
    "        StructField(\"merchant_id\", StringType()),\n",
    "        StructField(\"amount\", DoubleType()),\n",
    "        StructField(\"currency\", StringType()),\n",
    "        StructField(\"country\", StringType()),\n",
    "        StructField(\"device_id\", StringType()),\n",
    "        StructField(\"channel\", StringType()),\n",
    "        StructField(\"payment_method\", StringType()),\n",
    "        StructField(\"ip\", StringType()),\n",
    "        StructField(\"card_last4\", StringType()),\n",
    "        StructField(\"raw\", StringType())\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "def generate_login_batch(n=300):\n",
    "    \"\"\"Generate login events with failure patterns\"\"\"\n",
    "    now = datetime.utcnow()\n",
    "    rows = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        customer_id = f\"C{random.randint(1000, 1499)}\"\n",
    "        device_id = f\"D{random.randint(10000, 11999)}\"\n",
    "        \n",
    "        # 15% login failures, clustered by customer (brute force simulation)\n",
    "        if random.random() < 0.15:\n",
    "            auth_result = \"failure\"\n",
    "            # Cluster failures for the same customer\n",
    "            if i > 0 and random.random() < 0.4:\n",
    "                customer_id = rows[-1][\"customer_id\"]\n",
    "                device_id = rows[-1][\"device_id\"]\n",
    "        else:\n",
    "            auth_result = \"success\"\n",
    "        \n",
    "        time_offset = random.randint(0, 600)  # Up to 10 minutes in the past\n",
    "        event_time = now - timedelta(seconds=time_offset)\n",
    "        \n",
    "        row = {\n",
    "            \"event_id\": str(uuid.uuid4()),\n",
    "            \"event_ts\": event_time,\n",
    "            \"customer_id\": customer_id,\n",
    "            \"device_id\": device_id,\n",
    "            \"ip\": random_ip(),\n",
    "            \"auth_result\": auth_result,\n",
    "            \"channel\": random.choice([\"web\", \"mobile\", \"api\"]),\n",
    "            \"raw\": json.dumps({\"session_duration\": random.randint(60, 3600)})\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"event_id\", StringType()),\n",
    "        StructField(\"event_ts\", TimestampType()),\n",
    "        StructField(\"customer_id\", StringType()),\n",
    "        StructField(\"device_id\", StringType()),\n",
    "        StructField(\"ip\", StringType()),\n",
    "        StructField(\"auth_result\", StringType()),\n",
    "        StructField(\"channel\", StringType()),\n",
    "        StructField(\"raw\", StringType())\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "def generate_chargeback_batch(n=10):\n",
    "    \"\"\"Generate occasional chargebacks\"\"\"\n",
    "    now = datetime.utcnow()\n",
    "    rows = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        row = {\n",
    "            \"case_id\": str(uuid.uuid4()),\n",
    "            \"case_ts\": now - timedelta(hours=random.randint(1, 72)),\n",
    "            \"customer_id\": f\"C{random.randint(1000, 1499)}\",\n",
    "            \"merchant_id\": f\"M{random.randint(10, 109)}\",\n",
    "            \"amount\": round(random.uniform(50, 5000), 2),\n",
    "            \"reason\": random.choice([\"fraud\", \"customer_dispute\", \"duplicate\", \"unauthorized\"])\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"case_id\", StringType()),\n",
    "        StructField(\"case_ts\", TimestampType()),\n",
    "        StructField(\"customer_id\", StringType()),\n",
    "        StructField(\"merchant_id\", StringType()),\n",
    "        StructField(\"amount\", DoubleType()),\n",
    "        StructField(\"reason\", StringType())\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "# Create Bronze tables\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze.tx_events_raw (\n",
    "  event_id STRING,\n",
    "  event_ts TIMESTAMP,\n",
    "  customer_id STRING,\n",
    "  merchant_id STRING,\n",
    "  amount DOUBLE,\n",
    "  currency STRING,\n",
    "  country STRING,\n",
    "  device_id STRING,\n",
    "  channel STRING,\n",
    "  payment_method STRING,\n",
    "  ip STRING,\n",
    "  card_last4 STRING,\n",
    "  raw STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze.login_events_raw (\n",
    "  event_id STRING,\n",
    "  event_ts TIMESTAMP,\n",
    "  customer_id STRING,\n",
    "  device_id STRING,\n",
    "  ip STRING,\n",
    "  auth_result STRING,\n",
    "  channel STRING,\n",
    "  raw STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze.chargebacks_raw (\n",
    "  case_id STRING,\n",
    "  case_ts TIMESTAMP,\n",
    "  customer_id STRING,\n",
    "  merchant_id STRING,\n",
    "  amount DOUBLE,\n",
    "  reason STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"Starting continuous data generation...\")\n",
    "print(\"Run this cell to generate data continuously. Stop the cell to halt generation.\")\n",
    "\n",
    "# Continuous generation loop\n",
    "iteration = 0\n",
    "while True:\n",
    "    try:\n",
    "        iteration += 1\n",
    "        print(f\"Generating batch {iteration} at {datetime.now()}\")\n",
    "        \n",
    "        # Generate transaction data\n",
    "        tx_df = generate_transaction_batch(400)\n",
    "        tx_df.write.mode(\"append\").saveAsTable(\"fraud_lab.bronze.tx_events_raw\")\n",
    "        \n",
    "        # Generate login data\n",
    "        login_df = generate_login_batch(250)\n",
    "        login_df.write.mode(\"append\").saveAsTable(\"fraud_lab.bronze.login_events_raw\")\n",
    "        \n",
    "        # Occasionally generate chargebacks (10% of iterations)\n",
    "        if random.random() < 0.1:\n",
    "            cb_df = generate_chargeback_batch(5)\n",
    "            cb_df.write.mode(\"append\").saveAsTable(\"fraud_lab.bronze.chargebacks_raw\")\n",
    "            print(\"Generated chargebacks\")\n",
    "        \n",
    "        print(f\"Batch {iteration} completed. Generated ~400 transactions, ~250 logins\")\n",
    "        \n",
    "        # Wait before next batch (simulate real-time flow)\n",
    "        time.sleep(10)  # 10-second intervals\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Data generation stopped by user\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {iteration}: {str(e)}\")\n",
    "        time.sleep(5)  # Wait before retrying\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ü•â Bronze Layer: Raw Data Ingestion {#bronze-layer}\n",
    "\n",
    "### Notebook 02: Bronze Streaming Ingestion\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Set up checkpoint locations\n",
    "tx_checkpoint = f\"{checkpoint_base}/bronze_tx_clean\"\n",
    "login_checkpoint = f\"{checkpoint_base}/bronze_login_clean\"\n",
    "\n",
    "print(\"Starting Bronze layer streaming ingestion...\")\n",
    "\n",
    "# Stream 1: Transaction Events Cleansing\n",
    "tx_raw = (spark.readStream\n",
    "          .table(\"fraud_lab.bronze.tx_events_raw\")\n",
    "          .withWatermark(\"event_ts\", \"2 minutes\"))\n",
    "\n",
    "# Basic cleansing and validation\n",
    "tx_clean = (tx_raw\n",
    "    .filter(\"\"\"\n",
    "        event_id IS NOT NULL \n",
    "        AND customer_id IS NOT NULL \n",
    "        AND merchant_id IS NOT NULL \n",
    "        AND amount IS NOT NULL \n",
    "        AND amount > 0 \n",
    "        AND amount <= 50000\n",
    "        AND event_ts IS NOT NULL\n",
    "    \"\"\")\n",
    "    .dropDuplicates([\"event_id\"])\n",
    "    .withColumn(\"is_valid\", F.lit(True))\n",
    "    .withColumn(\"processed_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Create Silver clean table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver.tx_events_clean (\n",
    "  event_id STRING,\n",
    "  event_ts TIMESTAMP,\n",
    "  customer_id STRING,\n",
    "  merchant_id STRING,\n",
    "  amount DOUBLE,\n",
    "  currency STRING,\n",
    "  country STRING,\n",
    "  device_id STRING,\n",
    "  channel STRING,\n",
    "  payment_method STRING,\n",
    "  ip STRING,\n",
    "  card_last4 STRING,\n",
    "  is_valid BOOLEAN,\n",
    "  processed_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Start transaction cleaning stream\n",
    "tx_clean_query = (tx_clean.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", tx_checkpoint)\n",
    "    .outputMode(\"append\")\n",
    "    .table(\"fraud_lab.silver.tx_events_clean\")\n",
    "    .queryName(\"bronze_to_silver_tx_clean\"))\n",
    "\n",
    "print(\"Transaction cleaning stream started\")\n",
    "\n",
    "# Stream 2: Login Events Cleansing\n",
    "login_raw = (spark.readStream\n",
    "             .table(\"fraud_lab.bronze.login_events_raw\")\n",
    "             .withWatermark(\"event_ts\", \"2 minutes\"))\n",
    "\n",
    "login_clean = (login_raw\n",
    "    .filter(\"\"\"\n",
    "        event_id IS NOT NULL \n",
    "        AND customer_id IS NOT NULL \n",
    "        AND device_id IS NOT NULL\n",
    "        AND auth_result IS NOT NULL\n",
    "        AND event_ts IS NOT NULL\n",
    "    \"\"\")\n",
    "    .dropDuplicates([\"event_id\"])\n",
    "    .withColumn(\"is_valid\", F.lit(True))\n",
    "    .withColumn(\"processed_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Create login clean table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver.login_events_clean (\n",
    "  event_id STRING,\n",
    "  event_ts TIMESTAMP,\n",
    "  customer_id STRING,\n",
    "  device_id STRING,\n",
    "  ip STRING,\n",
    "  auth_result STRING,\n",
    "  channel STRING,\n",
    "  is_valid BOOLEAN,\n",
    "  processed_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Start login cleaning stream\n",
    "login_clean_query = (login_clean.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", login_checkpoint)\n",
    "    .outputMode(\"append\")\n",
    "    .table(\"fraud_lab.silver.login_events_clean\")\n",
    "    .queryName(\"bronze_to_silver_login_clean\"))\n",
    "\n",
    "print(\"Login cleaning stream started\")\n",
    "\n",
    "# Monitor streams\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"Active streaming queries:\")\n",
    "for query in spark.streams.active:\n",
    "    print(f\"- {query.name}: {query.status}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ü•à Silver Layer: Cleansing and Enrichment {#silver-layer}\n",
    "\n",
    "### Notebook 03: Silver Enrichment\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Checkpoint locations\n",
    "enrichment_checkpoint = f\"{checkpoint_base}/silver_tx_enriched\"\n",
    "\n",
    "print(\"Starting Silver layer enrichment...\")\n",
    "\n",
    "# Read clean transaction stream\n",
    "tx_clean = (spark.readStream\n",
    "            .table(\"fraud_lab.silver.tx_events_clean\")\n",
    "            .withWatermark(\"event_ts\", \"3 minutes\"))\n",
    "\n",
    "# Load reference data for enrichment\n",
    "customers = spark.read.table(\"fraud_lab.ref.customers_dim\").alias(\"c\")\n",
    "merchants = spark.read.table(\"fraud_lab.ref.merchants_dim\").alias(\"m\") \n",
    "devices = spark.read.table(\"fraud_lab.ref.device_dim\").alias(\"d\")\n",
    "blacklist = spark.read.table(\"fraud_lab.ref.blacklist_dim\").alias(\"b\")\n",
    "geoip = spark.read.table(\"fraud_lab.ref.geoip_dim\").alias(\"g\")\n",
    "\n",
    "# Enrich transactions with reference data\n",
    "enriched = (tx_clean.alias(\"t\")\n",
    "    # Join with customer data\n",
    "    .join(customers, F.col(\"t.customer_id\") == F.col(\"c.customer_id\"), \"left\")\n",
    "    # Join with merchant data  \n",
    "    .join(merchants, F.col(\"t.merchant_id\") == F.col(\"m.merchant_id\"), \"left\")\n",
    "    # Join with device data\n",
    "    .join(devices, F.col(\"t.device_id\") == F.col(\"d.device_id\"), \"left\")\n",
    "    # Check blacklist for customer\n",
    "    .join(\n",
    "        blacklist.filter(\"entity_type = 'customer'\").alias(\"bc\"),\n",
    "        F.col(\"t.customer_id\") == F.col(\"bc.entity_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    # Check blacklist for device\n",
    "    .join(\n",
    "        blacklist.filter(\"entity_type = 'device'\").alias(\"bd\"), \n",
    "        F.col(\"t.device_id\") == F.col(\"bd.entity_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    # Check blacklist for IP\n",
    "    .join(\n",
    "        blacklist.filter(\"entity_type = 'ip'\").alias(\"bi\"),\n",
    "        F.col(\"t.ip\") == F.col(\"bi.entity_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    # Simple geo-IP lookup (in production, use proper CIDR matching)\n",
    "    .join(geoip, F.substring(F.col(\"t.ip\"), 1, 7) == F.substring(F.col(\"g.ip_prefix\"), 1, 7), \"left\")\n",
    "    \n",
    "    # Select and rename columns\n",
    "    .select(\n",
    "        F.col(\"t.event_id\"),\n",
    "        F.col(\"t.event_ts\"),\n",
    "        F.col(\"t.customer_id\"),\n",
    "        F.col(\"t.merchant_id\"),\n",
    "        F.col(\"t.device_id\"),\n",
    "        F.col(\"t.amount\"),\n",
    "        F.col(\"t.currency\"),\n",
    "        F.col(\"t.country\").alias(\"tx_country\"),\n",
    "        F.col(\"t.channel\"),\n",
    "        F.col(\"t.payment_method\"),\n",
    "        F.col(\"t.ip\"),\n",
    "        F.col(\"t.card_last4\"),\n",
    "        \n",
    "        # Customer enrichment\n",
    "        F.col(\"c.risk_tier\").alias(\"customer_risk_tier\"),\n",
    "        F.col(\"c.kyc_status\").alias(\"customer_kyc\"),\n",
    "        F.col(\"c.country\").alias(\"customer_country\"),\n",
    "        F.col(\"c.credit_limit\").alias(\"customer_credit_limit\"),\n",
    "        \n",
    "        # Merchant enrichment\n",
    "        F.col(\"m.merchant_name\"),\n",
    "        F.col(\"m.category\").alias(\"merchant_category\"),\n",
    "        F.col(\"m.risk_segment\").alias(\"merchant_risk_segment\"),\n",
    "        F.col(\"m.monthly_volume\").alias(\"merchant_monthly_volume\"),\n",
    "        \n",
    "        # Device enrichment\n",
    "        F.col(\"d.device_type\"),\n",
    "        F.col(\"d.os\").alias(\"device_os\"),\n",
    "        F.col(\"d.reputation_score\").alias(\"device_reputation\"),\n",
    "        \n",
    "        # Blacklist flags\n",
    "        F.when(F.col(\"bc.entity_id\").isNotNull(), True).otherwise(False).alias(\"customer_blacklisted\"),\n",
    "        F.when(F.col(\"bd.entity_id\").isNotNull(), True).otherwise(False).alias(\"device_blacklisted\"),\n",
    "        F.when(F.col(\"bi.entity_id\").isNotNull(), True).otherwise(False).alias(\"ip_blacklisted\"),\n",
    "        \n",
    "        # Geo enrichment\n",
    "        F.col(\"g.country\").alias(\"geo_country\"),\n",
    "        F.col(\"g.city\").alias(\"geo_city\"),\n",
    "        F.col(\"g.latitude\"),\n",
    "        F.col(\"g.longitude\"),\n",
    "        \n",
    "        # Derived features\n",
    "        F.hour(\"t.event_ts\").alias(\"hour_of_day\"),\n",
    "        F.dayofweek(\"t.event_ts\").alias(\"day_of_week\"),\n",
    "        F.when(F.col(\"t.country\") != F.col(\"c.country\"), True).otherwise(False).alias(\"international_tx\"),\n",
    "        F.when(F.hour(\"t.event_ts\").between(22, 6), True).otherwise(False).alias(\"night_tx\"),\n",
    "        F.when(F.col(\"t.amount\") > F.col(\"c.credit_limit\") * 0.8, True).otherwise(False).alias(\"high_amount_vs_limit\"),\n",
    "        \n",
    "        F.current_timestamp().alias(\"enriched_ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create enriched table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver.tx_events_enriched (\n",
    "  event_id STRING,\n",
    "  event_ts TIMESTAMP,\n",
    "  customer_id STRING,\n",
    "  merchant_id STRING,\n",
    "  device_id STRING,\n",
    "  amount DOUBLE,\n",
    "  currency STRING,\n",
    "  tx_country STRING,\n",
    "  channel STRING,\n",
    "  payment_method STRING,\n",
    "  ip STRING,\n",
    "  card_last4 STRING,\n",
    "  customer_risk_tier STRING,\n",
    "  customer_kyc STRING,\n",
    "  customer_country STRING,\n",
    "  customer_credit_limit DOUBLE,\n",
    "  merchant_name STRING,\n",
    "  merchant_category STRING,\n",
    "  merchant_risk_segment STRING,\n",
    "  merchant_monthly_volume DOUBLE,\n",
    "  device_type STRING,\n",
    "  device_os STRING,\n",
    "  device_reputation DOUBLE,\n",
    "  customer_blacklisted BOOLEAN,\n",
    "  device_blacklisted BOOLEAN,\n",
    "  ip_blacklisted BOOLEAN,\n",
    "  geo_country STRING,\n",
    "  geo_city STRING,\n",
    "  latitude DOUBLE,\n",
    "  longitude DOUBLE,\n",
    "  hour_of_day INT,\n",
    "  day_of_week INT,\n",
    "  international_tx BOOLEAN,\n",
    "  night_tx BOOLEAN,\n",
    "  high_amount_vs_limit BOOLEAN,\n",
    "  enriched_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Start enrichment stream\n",
    "enrichment_query = (enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", enrichment_checkpoint)\n",
    "    .outputMode(\"append\")\n",
    "    .table(\"fraud_lab.silver.tx_events_enriched\")\n",
    "    .queryName(\"silver_enrichment\"))\n",
    "\n",
    "print(\"Enrichment stream started\")\n",
    "\n",
    "# Monitor the stream\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"Active enrichment stream:\")\n",
    "for query in spark.streams.active:\n",
    "    if query.name == \"silver_enrichment\":\n",
    "        print(f\"- {query.name}: {query.status}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Stateful Feature Engineering {#stateful-features}\n",
    "\n",
    "### Notebook 04: Velocity and Behavioral Features\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"Starting stateful feature engineering...\")\n",
    "\n",
    "# Checkpoint locations\n",
    "velocity_checkpoint = f\"{checkpoint_base}/ops_velocity\"\n",
    "device_checkpoint = f\"{checkpoint_base}/ops_device\"\n",
    "geo_checkpoint = f\"{checkpoint_base}/ops_geo\"\n",
    "\n",
    "# Read enriched transaction stream\n",
    "enriched = (spark.readStream\n",
    "            .table(\"fraud_lab.silver.tx_events_enriched\")\n",
    "            .withWatermark(\"event_ts\", \"5 minutes\"))\n",
    "\n",
    "# Create operational tables for stateful features\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ops.customer_velocity (\n",
    "  customer_id STRING,\n",
    "  window_start TIMESTAMP,\n",
    "  window_end TIMESTAMP,\n",
    "  tx_count INT,\n",
    "  total_amount DOUBLE,\n",
    "  unique_merchants INT,\n",
    "  unique_countries INT,\n",
    "  unique_devices INT,\n",
    "  avg_amount DOUBLE,\n",
    "  max_amount DOUBLE,\n",
    "  last_tx_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ops.device_behavior (\n",
    "  device_id STRING,\n",
    "  window_start TIMESTAMP,\n",
    "  window_end TIMESTAMP,\n",
    "  unique_customers INT,\n",
    "  unique_countries INT,\n",
    "  total_transactions INT,\n",
    "  customer_list ARRAY<STRING>,\n",
    "  country_list ARRAY<STRING>,\n",
    "  last_seen_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ops.geo_velocity (\n",
    "  customer_id STRING,\n",
    "  window_start TIMESTAMP,\n",
    "  window_end TIMESTAMP,\n",
    "  locations ARRAY<STRUCT<country: STRING, city: STRING, lat: DOUBLE, lon: DOUBLE, ts: TIMESTAMP>>,\n",
    "  max_distance_km DOUBLE,\n",
    "  max_speed_kmh DOUBLE,\n",
    "  country_changes INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Feature 1: Customer Transaction Velocity (5-minute sliding windows)\n",
    "customer_velocity = (enriched\n",
    "    .groupBy(\n",
    "        F.window(\"event_ts\", \"5 minutes\", \"1 minute\"),\n",
    "        \"customer_id\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"tx_count\"),\n",
    "        F.sum(\"amount\").alias(\"total_amount\"),\n",
    "        F.countDistinct(\"merchant_id\").alias(\"unique_merchants\"),\n",
    "        F.countDistinct(\"tx_country\").alias(\"unique_countries\"),\n",
    "        F.countDistinct(\"device_id\").alias(\"unique_devices\"),\n",
    "        F.avg(\"amount\").alias(\"avg_amount\"),\n",
    "        F.max(\"amount\").alias(\"max_amount\"),\n",
    "        F.max(\"event_ts\").alias(\"last_tx_ts\")\n",
    "    )\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        F.col(\"window.start\").alias(\"window_start\"),\n",
    "        F.col(\"window.end\").alias(\"window_end\"),\n",
    "        \"tx_count\",\n",
    "        F.round(\"total_amount\", 2).alias(\"total_amount\"),\n",
    "        \"unique_merchants\",\n",
    "        \"unique_countries\", \n",
    "        \"unique_devices\",\n",
    "        F.round(\"avg_amount\", 2).alias(\"avg_amount\"),\n",
    "        \"max_amount\",\n",
    "        \"last_tx_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Start customer velocity stream\n",
    "velocity_query = (customer_velocity.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", velocity_checkpoint)\n",
    "    .outputMode(\"complete\")  # Complete mode for windowed aggregations\n",
    "    .table(\"fraud_lab.ops.customer_velocity\")\n",
    "    .queryName(\"customer_velocity\"))\n",
    "\n",
    "print(\"Customer velocity stream started\")\n",
    "\n",
    "# Feature 2: Device Sharing Behavior (10-minute sliding windows)\n",
    "device_behavior = (enriched\n",
    "    .groupBy(\n",
    "        F.window(\"event_ts\", \"10 minutes\", \"2 minutes\"),\n",
    "        \"device_id\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        F.countDistinct(\"tx_country\").alias(\"unique_countries\"),\n",
    "        F.count(\"*\").alias(\"total_transactions\"),\n",
    "        F.collect_set(\"customer_id\").alias(\"customer_list\"),\n",
    "        F.collect_set(\"tx_country\").alias(\"country_list\"),\n",
    "        F.max(\"event_ts\").alias(\"last_seen_ts\")\n",
    "    )\n",
    "    .select(\n",
    "        \"device_id\",\n",
    "        F.col(\"window.start\").alias(\"window_start\"),\n",
    "        F.col(\"window.end\").alias(\"window_end\"),\n",
    "        \"unique_customers\",\n",
    "        \"unique_countries\",\n",
    "        \"total_transactions\",\n",
    "        \"customer_list\",\n",
    "        \"country_list\",\n",
    "        \"last_seen_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Start device behavior stream\n",
    "device_query = (device_behavior.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", device_checkpoint)\n",
    "    .outputMode(\"complete\")\n",
    "    .table(\"fraud_lab.ops.device_behavior\")\n",
    "    .queryName(\"device_behavior\"))\n",
    "\n",
    "print(\"Device behavior stream started\")\n",
    "\n",
    "# Feature 3: Geo-velocity Analysis (15-minute windows)\n",
    "# Simplified version - in production, calculate actual distances and speeds\n",
    "geo_features = (enriched\n",
    "    .filter(\"latitude IS NOT NULL AND longitude IS NOT NULL\")\n",
    "    .groupBy(\n",
    "        F.window(\"event_ts\", \"15 minutes\", \"3 minutes\"),\n",
    "        \"customer_id\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.struct(\n",
    "                \"tx_country\", \"geo_city\", \"latitude\", \"longitude\", \"event_ts\"\n",
    "            )\n",
    "        ).alias(\"locations\"),\n",
    "        F.countDistinct(\"tx_country\").alias(\"country_changes\")\n",
    "    )\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        F.col(\"window.start\").alias(\"window_start\"),\n",
    "        F.col(\"window.end\").alias(\"window_end\"),\n",
    "        \"locations\",\n",
    "        F.lit(0.0).alias(\"max_distance_km\"),  # Placeholder - implement haversine distance\n",
    "        F.lit(0.0).alias(\"max_speed_kmh\"),   # Placeholder - calculate based on time/distance\n",
    "        \"country_changes\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Start geo-velocity stream\n",
    "geo_query = (geo_features.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", geo_checkpoint)\n",
    "    .outputMode(\"complete\")\n",
    "    .table(\"fraud_lab.ops.geo_velocity\")\n",
    "    .queryName(\"geo_velocity\"))\n",
    "\n",
    "print(\"Geo-velocity stream started\")\n",
    "\n",
    "# Monitor all stateful streams\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "print(\"\\nActive stateful feature streams:\")\n",
    "for query in spark.streams.active:\n",
    "    if query.name in [\"customer_velocity\", \"device_behavior\", \"geo_velocity\"]:\n",
    "        print(f\"- {query.name}: {query.status}\")\n",
    "        if query.lastProgress:\n",
    "            print(f\"  Batch: {query.lastProgress.get('batchId', 'N/A')}, \"\n",
    "                  f\"Input rows: {query.lastProgress.get('inputRowsPerSecond', 'N/A')}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ü•á Gold Layer: Advanced Fraud Rules {#gold-layer}\n",
    "\n",
    "### Notebook 05: Fraud Detection Rules Engine\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"Starting advanced fraud detection rules engine...\")\n",
    "\n",
    "# Checkpoint locations\n",
    "suspicions_checkpoint = f\"{checkpoint_base}/gold_fraud_suspicions\"\n",
    "alerts_checkpoint = f\"{checkpoint_base}/gold_fraud_alerts\"\n",
    "\n",
    "# Read enriched transactions\n",
    "tx_enriched = (spark.readStream\n",
    "               .table(\"fraud_lab.silver.tx_events_enriched\")\n",
    "               .withWatermark(\"event_ts\", \"10 minutes\"))\n",
    "\n",
    "# Read stateful features (as batch joins - get latest state)\n",
    "def get_latest_velocity():\n",
    "    return (spark.read.table(\"fraud_lab.ops.customer_velocity\")\n",
    "            .withColumn(\"rank\", \n",
    "                       F.row_number().over(\n",
    "                           Window.partitionBy(\"customer_id\")\n",
    "                           .orderBy(F.col(\"window_end\").desc())\n",
    "                       ))\n",
    "            .filter(\"rank = 1\")\n",
    "            .drop(\"rank\"))\n",
    "\n",
    "def get_latest_device_behavior():\n",
    "    return (spark.read.table(\"fraud_lab.ops.device_behavior\")\n",
    "            .withColumn(\"rank\",\n",
    "                       F.row_number().over(\n",
    "                           Window.partitionBy(\"device_id\")\n",
    "                           .orderBy(F.col(\"window_end\").desc())\n",
    "                       ))\n",
    "            .filter(\"rank = 1\")\n",
    "            .drop(\"rank\"))\n",
    "\n",
    "def get_latest_geo_velocity():\n",
    "    return (spark.read.table(\"fraud_lab.ops.geo_velocity\")\n",
    "            .withColumn(\"rank\",\n",
    "                       F.row_number().over(\n",
    "                           Window.partitionBy(\"customer_id\")\n",
    "                           .orderBy(F.col(\"window_end\").desc())\n",
    "                       ))\n",
    "            .filter(\"rank = 1\")\n",
    "            .drop(\"rank\"))\n",
    "\n",
    "# Create comprehensive fraud detection using foreachBatch\n",
    "def fraud_detection_rules(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing batch {batch_id} with {batch_df.count()} transactions\")\n",
    "    \n",
    "    # Join with latest stateful features\n",
    "    velocity_df = get_latest_velocity()\n",
    "    device_df = get_latest_device_behavior()\n",
    "    geo_df = get_latest_geo_velocity()\n",
    "    \n",
    "    # Read login failures for recent context\n",
    "    recent_login_failures = (spark.read.table(\"fraud_lab.silver.login_events_clean\")\n",
    "                            .filter(\"auth_result = 'failure'\")\n",
    "                            .filter(f\"event_ts >= current_timestamp() - INTERVAL 30 MINUTES\")\n",
    "                            .groupBy(\"customer_id\")\n",
    "                            .agg(\n",
    "                                F.count(\"*\").alias(\"recent_login_failures\"),\n",
    "                                F.max(\"event_ts\").alias(\"last_failure_ts\")\n",
    "                            ))\n",
    "    \n",
    "    # Join all features\n",
    "    enriched_batch = (batch_df.alias(\"tx\")\n",
    "                      .join(velocity_df.alias(\"vel\"), \"customer_id\", \"left\")\n",
    "                      .join(device_df.alias(\"dev\"), \"device_id\", \"left\") \n",
    "                      .join(geo_df.alias(\"geo\"), \"customer_id\", \"left\")\n",
    "                      .join(recent_login_failures.alias(\"login\"), \"customer_id\", \"left\"))\n",
    "    \n",
    "    # Define fraud rules\n",
    "    rules_df = enriched_batch.withColumn(\n",
    "        \"rule_results\",\n",
    "        F.array(\n",
    "            # R1: High-risk customer + international + high amount\n",
    "            F.struct(\n",
    "                F.lit(\"R1_high_risk_international\").alias(\"rule_name\"),\n",
    "                (\n",
    "                    (F.col(\"customer_risk_tier\") == \"high\") &\n",
    "                    F.col(\"international_tx\") &\n",
    "                    (F.col(\"amount\") > 5000)\n",
    "                ).alias(\"triggered\"),\n",
    "                F.lit(0.35).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R2: Blacklisted entities\n",
    "            F.struct(\n",
    "                F.lit(\"R2_blacklisted_entity\").alias(\"rule_name\"),\n",
    "                (\n",
    "                    F.col(\"customer_blacklisted\") |\n",
    "                    F.col(\"device_blacklisted\") |\n",
    "                    F.col(\"ip_blacklisted\")\n",
    "                ).alias(\"triggered\"),\n",
    "                F.lit(0.45).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R3: High-risk merchant + large amount\n",
    "            F.struct(\n",
    "                F.lit(\"R3_high_risk_merchant\").alias(\"rule_name\"),\n",
    "                (\n",
    "                    F.col(\"merchant_risk_segment\").isin([\"high_risk\", \"watchlist\"]) &\n",
    "                    (F.col(\"amount\") > 3000)\n",
    "                ).alias(\"triggered\"),\n",
    "                F.lit(0.25).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R4: Velocity spike\n",
    "            F.struct(\n",
    "                F.lit(\"R4_velocity_spike\").alias(\"rule_name\"),\n",
    "                (\n",
    "                    (F.coalesce(F.col(\"tx_count\"), F.lit(0)) > 15) |\n",
    "                    (F.coalesce(F.col(\"total_amount\"), F.lit(0)) > 25000) |\n",
    "                    (F.coalesce(F.col(\"unique_merchants\"), F.lit(0)) > 10)\n",
    "                ).alias(\"triggered\"),\n",
    "                F.lit(0.30).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R5: Device misuse (multiple customers)\n",
    "            F.struct(\n",
    "                F.lit(\"R5_device_misuse\").alias(\"rule_name\"),\n",
    "                (F.coalesce(F.col(\"unique_customers\"), F.lit(0)) > 7).alias(\"triggered\"),\n",
    "                F.lit(0.20).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R6: Poor device reputation\n",
    "            F.struct(\n",
    "                F.lit(\"R6_poor_device_reputation\").alias(\"rule_name\"),\n",
    "                (F.coalesce(F.col(\"device_reputation\"), F.lit(1.0)) < 0.3).alias(\"triggered\"),\n",
    "                F.lit(0.15).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R7: Night transaction + high amount\n",
    "            F.struct(\n",
    "                F.lit(\"R7_night_high_amount\").alias(\"rule_name\"),\n",
    "                (\n",
    "                    F.col(\"night_tx\") &\n",
    "                    (F.col(\"amount\") > 2000)\n",
    "                ).alias(\"triggered\"),\n",
    "                F.lit(0.15).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R8: Multiple countries in short time\n",
    "            F.struct(\n",
    "                F.lit(\"R8_geo_velocity\").alias(\"rule_name\"),\n",
    "                (F.coalesce(F.col(\"country_changes\"), F.lit(0)) >= 3).alias(\"triggered\"),\n",
    "                F.lit(0.25).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R9: Amount vs credit limit ratio\n",
    "            F.struct(\n",
    "                F.lit(\"R9_high_credit_utilization\").alias(\"rule_name\"),\n",
    "                F.col(\"high_amount_vs_limit\").alias(\"triggered\"),\n",
    "                F.lit(0.10).alias(\"weight\")\n",
    "            ),\n",
    "            \n",
    "            # R10: Recent login failures followed by transaction\n",
    "            F.struct(\n",
    "                F.lit(\"R10_failed_login_then_tx\").alias(\"rule_name\"),\n",
    "                (\n",
    "                    (F.coalesce(F.col(\"recent_login_failures\"), F.lit(0)) > 3) &\n",
    "                    (F.col(\"amount\") > 1000) &\n",
    "                    (\n",
    "                        F.col(\"event_ts\").cast(\"long\") - \n",
    "                        F.coalesce(F.col(\"last_failure_ts\"), F.lit(\"1970-01-01\")).cast(\"long\")\n",
    "                    ) < 1800  # 30 minutes\n",
    "                ).alias(\"triggered\"),\n",
    "                F.lit(0.20).alias(\"weight\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculate final risk score and extract triggered rules\n",
    "    final_results = rules_df.withColumn(\n",
    "        \"triggered_rules\",\n",
    "        F.filter(F.col(\"rule_results\"), lambda x: x.triggered)\n",
    "    ).withColumn(\n",
    "        \"risk_score\",\n",
    "        F.round(\n",
    "            F.aggregate(\n",
    "                F.col(\"triggered_rules\"),\n",
    "                F.lit(0.0),\n",
    "                lambda acc, x: acc + x.weight\n",
    "            ),\n",
    "            3\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"rule_names\",\n",
    "        F.transform(F.col(\"triggered_rules\"), lambda x: x.rule_name)\n",
    "    ).filter(\n",
    "        \"size(triggered_rules) > 0\"  # Only keep transactions with rule violations\n",
    "    )\n",
    "    \n",
    "    # Create fraud suspicions\n",
    "    fraud_suspicions = final_results.select(\n",
    "        \"event_id\",\n",
    "        \"event_ts\", \n",
    "        \"customer_id\",\n",
    "        \"merchant_id\",\n",
    "        \"device_id\",\n",
    "        \"ip\",\n",
    "        \"amount\",\n",
    "        \"currency\",\n",
    "        \"tx_country\",\n",
    "        F.col(\"rule_names\").alias(\"reasons\"),\n",
    "        \"risk_score\",\n",
    "        F.col(\"rule_names\").alias(\"rule_hits\"),\n",
    "        F.current_timestamp().alias(\"created_ts\")\n",
    "    )\n",
    "    \n",
    "    # Write to fraud suspicions table\n",
    "    if fraud_suspicions.count() > 0:\n",
    "        fraud_suspicions.write.mode(\"append\").saveAsTable(\"fraud_lab.gold.fraud_suspicions\")\n",
    "        print(f\"  - Wrote {fraud_suspicions.count()} fraud suspicions\")\n",
    "        \n",
    "        # Create alerts for high-risk cases\n",
    "        alerts = fraud_suspicions.filter(\"risk_score >= 0.3\").select(\n",
    "            F.expr(\"uuid()\").alias(\"alert_id\"),\n",
    "            \"event_id\",\n",
    "            F.current_timestamp().alias(\"alert_ts\"),\n",
    "            F.when(F.col(\"risk_score\") >= 0.6, \"critical\")\n",
    "             .when(F.col(\"risk_score\") >= 0.4, \"high\") \n",
    "             .when(F.col(\"risk_score\") >= 0.3, \"medium\")\n",
    "             .otherwise(\"low\").alias(\"severity\"),\n",
    "            F.concat_ws(\" | \",\n",
    "                       F.lit(\"Fraud detected - Customer:\"), F.col(\"customer_id\"),\n",
    "                       F.lit(\"Amount:\"), F.col(\"amount\"),\n",
    "                       F.lit(\"Risk Score:\"), F.col(\"risk_score\")\n",
    "                      ).alias(\"summary\"),\n",
    "            \"reasons\"\n",
    "        )\n",
    "        \n",
    "        if alerts.count() > 0:\n",
    "            alerts.write.mode(\"append\").saveAsTable(\"fraud_lab.gold.fraud_alerts\")\n",
    "            print(f\"  - Generated {alerts.count()} fraud alerts\")\n",
    "\n",
    "# Create Gold tables\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gold.fraud_suspicions (\n",
    "  event_id STRING,\n",
    "  event_ts TIMESTAMP,\n",
    "  customer_id STRING,\n",
    "  merchant_id STRING,\n",
    "  device_id STRING,\n",
    "  ip STRING,\n",
    "  amount DOUBLE,\n",
    "  currency STRING,\n",
    "  tx_country STRING,\n",
    "  reasons ARRAY<STRING>,\n",
    "  risk_score DOUBLE,\n",
    "  rule_hits ARRAY<STRING>,\n",
    "  created_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gold.fraud_alerts (\n",
    "  alert_id STRING,\n",
    "  event_id STRING,\n",
    "  alert_ts TIMESTAMP,\n",
    "  severity STRING,\n",
    "  summary STRING,\n",
    "  reasons ARRAY<STRING>\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Start fraud detection stream using foreachBatch\n",
    "fraud_query = (tx_enriched.writeStream\n",
    "    .foreachBatch(fraud_detection_rules)\n",
    "    .option(\"checkpointLocation\", suspicions_checkpoint)\n",
    "    .queryName(\"fraud_detection_engine\")\n",
    "    .start())\n",
    "\n",
    "print(\"Fraud detection engine started\")\n",
    "\n",
    "# Monitor the fraud detection stream\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "print(\"\\nFraud detection stream status:\")\n",
    "for query in spark.streams.active:\n",
    "    if query.name == \"fraud_detection_engine\":\n",
    "        print(f\"- {query.name}: {query.status}\")\n",
    "        if query.lastProgress:\n",
    "            print(f\"  Batch: {query.lastProgress.get('batchId', 'N/A')}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Monitoring and Operations {#monitoring}\n",
    "\n",
    "### Notebook 06: Real-time Monitoring Dashboard\n",
    "\n",
    "```sql\n",
    "-- Create operational monitoring tables\n",
    "CREATE TABLE IF NOT EXISTS ops.pipeline_metrics (\n",
    "  metric_ts TIMESTAMP,\n",
    "  pipeline_name STRING,\n",
    "  metric_name STRING,\n",
    "  metric_value DOUBLE,\n",
    "  metric_unit STRING\n",
    ") USING DELTA;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ops.data_quality_metrics (\n",
    "  check_ts TIMESTAMP,\n",
    "  table_name STRING,\n",
    "  total_records BIGINT,\n",
    "  null_records BIGINT,\n",
    "  duplicate_records BIGINT,\n",
    "  quality_score DOUBLE\n",
    ") USING DELTA;\n",
    "```\n",
    "\n",
    "```python\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def collect_pipeline_metrics():\n",
    "    \"\"\"Collect real-time pipeline metrics\"\"\"\n",
    "    \n",
    "    # Get current counts from each layer\n",
    "    bronze_tx_count = spark.sql(\"SELECT COUNT(*) as cnt FROM bronze.tx_events_raw\").collect()[0].cnt\n",
    "    silver_clean_count = spark.sql(\"SELECT COUNT(*) as cnt FROM silver.tx_events_clean\").collect()[0].cnt\n",
    "    silver_enriched_count = spark.sql(\"SELECT COUNT(*) as cnt FROM silver.tx_events_enriched\").collect()[0].cnt\n",
    "    gold_suspicions_count = spark.sql(\"SELECT COUNT(*) as cnt FROM gold.fraud_suspicions\").collect()[0].cnt\n",
    "    gold_alerts_count = spark.sql(\"SELECT COUNT(*) as cnt FROM gold.fraud_alerts\").collect()[0].cnt\n",
    "    \n",
    "    # Calculate processing rates\n",
    "    processing_rate = silver_clean_count / max(bronze_tx_count, 1) * 100\n",
    "    enrichment_rate = silver_enriched_count / max(silver_clean_count, 1) * 100\n",
    "    fraud_detection_rate = gold_suspicions_count / max(silver_enriched_count, 1) * 100\n",
    "    \n",
    "    # Get recent fraud statistics\n",
    "    recent_fraud_stats = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as recent_suspicions,\n",
    "            AVG(risk_score) as avg_risk_score,\n",
    "            COUNT(DISTINCT customer_id) as unique_customers_flagged\n",
    "        FROM gold.fraud_suspicions \n",
    "        WHERE created_ts >= current_timestamp() - INTERVAL 15 MINUTES\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    metrics = [\n",
    "        (\"bronze_tx_count\", bronze_tx_count, \"count\"),\n",
    "        (\"silver_clean_count\", silver_clean_count, \"count\"),\n",
    "        (\"silver_enriched_count\", silver_enriched_count, \"count\"),\n",
    "        (\"gold_suspicions_count\", gold_suspicions_count, \"count\"),\n",
    "        (\"gold_alerts_count\", gold_alerts_count, \"count\"),\n",
    "        (\"processing_rate\", processing_rate, \"percentage\"),\n",
    "        (\"enrichment_rate\", enrichment_rate, \"percentage\"),\n",
    "        (\"fraud_detection_rate\", fraud_detection_rate, \"percentage\"),\n",
    "        (\"recent_suspicions_15min\", recent_fraud_stats.recent_suspicions, \"count\"),\n",
    "        (\"avg_risk_score_15min\", recent_fraud_stats.avg_risk_score or 0, \"score\"),\n",
    "        (\"unique_customers_flagged_15min\", recent_fraud_stats.unique_customers_flagged, \"count\")\n",
    "    ]\n",
    "    \n",
    "    # Insert metrics\n",
    "    metrics_data = [(datetime.now(), \"fraud_detection_pipeline\", name, value, unit) \n",
    "                   for name, value, unit in metrics]\n",
    "    \n",
    "    metrics_df = spark.createDataFrame(\n",
    "        metrics_data, \n",
    "        [\"metric_ts\", \"pipeline_name\", \"metric_name\", \"metric_value\", \"metric_unit\"]\n",
    "    )\n",
    "    metrics_df.write.mode(\"append\").saveAsTable(\"fraud_lab.ops.pipeline_metrics\")\n",
    "    \n",
    "    return dict(metrics)\n",
    "\n",
    "# Real-time monitoring queries\n",
    "print(\"=== FRAUD DETECTION SYSTEM MONITORING DASHBOARD ===\\n\")\n",
    "\n",
    "# 1. Pipeline Health Overview\n",
    "print(\"1. PIPELINE HEALTH:\")\n",
    "metrics = collect_pipeline_metrics()\n",
    "print(f\"   Bronze Transactions: {metrics['bronze_tx_count']:,}\")\n",
    "print(f\"   Silver Clean: {metrics['silver_clean_count']:,} ({metrics['processing_rate']:.1f}% of bronze)\")\n",
    "print(f\"   Silver Enriched: {metrics['silver_enriched_count']:,} ({metrics['enrichment_rate']:.1f}% of clean)\")\n",
    "print(f\"   Gold Suspicions: {metrics['gold_suspicions_count']:,} ({metrics['fraud_detection_rate']:.1f}% flagged)\")\n",
    "print(f\"   Gold Alerts: {metrics['gold_alerts_count']:,}\")\n",
    "\n",
    "# 2. Recent Fraud Activity\n",
    "print(\"\\n2. RECENT FRAUD ACTIVITY (Last 15 minutes):\")\n",
    "print(f\"   Suspicions: {metrics['recent_suspicions_15min']}\")\n",
    "print(f\"   Average Risk Score: {metrics['avg_risk_score_15min']:.3f}\")\n",
    "print(f\"   Unique Customers Flagged: {metrics['unique_customers_flagged_15min']}\")\n",
    "\n",
    "# 3. Streaming Query Health\n",
    "print(\"\\n3. STREAMING QUERY STATUS:\")\n",
    "for query in spark.streams.active:\n",
    "    status = query.status\n",
    "    name = query.name or \"unnamed\"\n",
    "    print(f\"   {name}: {status}\")\n",
    "    if query.lastProgress:\n",
    "        progress = query.lastProgress\n",
    "        print(f\"     Batch: {progress.get('batchId', 'N/A')}\")\n",
    "        print(f\"     Input Rate: {progress.get('inputRowsPerSecond', 'N/A')} rows/sec\")\n",
    "        print(f\"     Processing Time: {progress.get('durationMs', {}).get('triggerExecution', 'N/A')} ms\")\n",
    "\n",
    "# 4. Top Fraud Rules\n",
    "print(\"\\n4. TOP FRAUD RULES (Last hour):\")\n",
    "top_rules = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        explode(reasons) as rule_name,\n",
    "        COUNT(*) as trigger_count,\n",
    "        AVG(risk_score) as avg_risk_score\n",
    "    FROM gold.fraud_suspicions \n",
    "    WHERE created_ts >= current_timestamp() - INTERVAL 1 HOUR\n",
    "    GROUP BY rule_name\n",
    "    ORDER BY trigger_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "top_rules.show(truncate=False)\n",
    "\n",
    "# 5. High-Risk Customers\n",
    "print(\"\\n5. HIGH-RISK CUSTOMERS (Last hour):\")\n",
    "high_risk_customers = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as suspicion_count,\n",
    "        MAX(risk_score) as max_risk_score,\n",
    "        SUM(amount) as total_amount,\n",
    "        array_distinct(flatten(collect_list(reasons))) as all_reasons\n",
    "    FROM gold.fraud_suspicions \n",
    "    WHERE created_ts >= current_timestamp() - INTERVAL 1 HOUR\n",
    "    GROUP BY customer_id\n",
    "    HAVING max_risk_score > 0.4\n",
    "    ORDER BY max_risk_score DESC, suspicion_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "high_risk_customers.show(truncate=False)\n",
    "\n",
    "# 6. Alert Summary by Severity\n",
    "print(\"\\n6. ALERTS BY SEVERITY (Last hour):\")\n",
    "alert_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        severity,\n",
    "        COUNT(*) as alert_count,\n",
    "        COUNT(DISTINCT event_id) as unique_events\n",
    "    FROM gold.fraud_alerts \n",
    "    WHERE alert_ts >= current_timestamp() - INTERVAL 1 HOUR\n",
    "    GROUP BY severity\n",
    "    ORDER BY \n",
    "        CASE severity \n",
    "            WHEN 'critical' THEN 1 \n",
    "            WHEN 'high' THEN 2 \n",
    "            WHEN 'medium' THEN 3 \n",
    "            ELSE 4 \n",
    "        END\n",
    "\"\"\")\n",
    "alert_summary.show()\n",
    "\n",
    "# 7. Data Quality Checks\n",
    "print(\"\\n7. DATA QUALITY METRICS:\")\n",
    "quality_check = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'silver.tx_events_clean' as table_name,\n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(*) - COUNT(customer_id) as null_customers,\n",
    "        COUNT(*) - COUNT(merchant_id) as null_merchants,\n",
    "        COUNT(*) - COUNT(amount) as null_amounts\n",
    "    FROM silver.tx_events_clean\n",
    "    WHERE processed_ts >= current_timestamp() - INTERVAL 1 HOUR\n",
    "\"\"\")\n",
    "quality_check.show()\n",
    "\n",
    "# 8. Velocity Analytics\n",
    "print(\"\\n8. VELOCITY PATTERNS (Current 5-minute windows):\")\n",
    "velocity_patterns = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        tx_count,\n",
    "        COUNT(*) as customers_with_count,\n",
    "        AVG(total_amount) as avg_total_amount,\n",
    "        MAX(total_amount) as max_total_amount\n",
    "    FROM ops.customer_velocity \n",
    "    WHERE window_end >= current_timestamp() - INTERVAL 10 MINUTES\n",
    "    GROUP BY tx_count\n",
    "    HAVING tx_count > 1\n",
    "    ORDER BY tx_count DESC\n",
    "\"\"\")\n",
    "velocity_patterns.show()\n",
    "\n",
    "print(\"\\n=== END OF MONITORING DASHBOARD ===\")\n",
    "```\n",
    "\n",
    "### Continuous Monitoring Loop\n",
    "\n",
    "```python\n",
    "# Run continuous monitoring (optional)\n",
    "def continuous_monitoring(interval_seconds=60):\n",
    "    \"\"\"Run monitoring dashboard every N seconds\"\"\"\n",
    "    \n",
    "    print(\"Starting continuous monitoring... (Press Ctrl+C to stop)\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"MONITORING UPDATE: {datetime.now()}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Collect and display key metrics\n",
    "            metrics = collect_pipeline_metrics()\n",
    "            \n",
    "            # Show streaming query health\n",
    "            active_queries = len(spark.streams.active)\n",
    "            print(f\"Active Streams: {active_queries}\")\n",
    "            \n",
    "            # Show recent fraud activity\n",
    "            recent_alerts = spark.sql(\"\"\"\n",
    "                SELECT severity, COUNT(*) as count \n",
    "                FROM gold.fraud_alerts \n",
    "                WHERE alert_ts >= current_timestamp() - INTERVAL 5 MINUTES\n",
    "                GROUP BY severity\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            if recent_alerts:\n",
    "                print(\"Recent Alerts (5 min):\", {row.severity: row.count for row in recent_alerts})\n",
    "            else:\n",
    "                print(\"Recent Alerts (5 min): None\")\n",
    "            \n",
    "            # Show top suspicious customers\n",
    "            top_suspicious = spark.sql(\"\"\"\n",
    "                SELECT customer_id, COUNT(*) as count, MAX(risk_score) as max_score\n",
    "                FROM gold.fraud_suspicions \n",
    "                WHERE created_ts >= current_timestamp() - INTERVAL 5 MINUTES\n",
    "                GROUP BY customer_id\n",
    "                ORDER BY max_score DESC\n",
    "                LIMIT 3\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            if top_suspicious:\n",
    "                print(\"Top Suspicious Customers:\")\n",
    "                for row in top_suspicious:\n",
    "                    print(f\"  {row.customer_id}: {row.count} events, max risk {row.max_score:.3f}\")\n",
    "            \n",
    "            time.sleep(interval_seconds)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped by user\")\n",
    "\n",
    "# Uncomment to run continuous monitoring\n",
    "# continuous_monitoring(30)  # Every 30 seconds\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Production Considerations {#production-considerations}\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "```python\n",
    "# Optimize streaming queries for production\n",
    "optimization_configs = {\n",
    "    # Adaptive Query Execution\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "    \n",
    "    # Streaming optimizations\n",
    "    \"spark.sql.streaming.metricsEnabled\": \"true\",\n",
    "    \"spark.sql.streaming.ui.enabled\": \"true\",\n",
    "    \"spark.sql.streaming.checkpointLocation.deleteOnExit\": \"false\",\n",
    "    \n",
    "    # Delta optimizations\n",
    "    \"spark.databricks.delta.autoCompact.enabled\": \"true\",\n",
    "    \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n",
    "    \"spark.databricks.delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "    \n",
    "    # Memory and performance\n",
    "    \"spark.sql.streaming.stateStore.providerClass\": \"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\n",
    "    \"spark.sql.streaming.minBatchesToRetain\": \"10\",\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"spark.sql.streaming.checkpointFileManagerClass\": \"org.apache.spark.sql.execution.streaming.CheckpointFileManager\"\n",
    "}\n",
    "\n",
    "for key, value in optimization_configs.items():\n",
    "    spark.conf.set(key, value)\n",
    "```\n",
    "\n",
    "### Error Handling and Recovery\n",
    "\n",
    "```python\n",
    "def robust_stream_starter(stream_func, checkpoint_path, query_name, max_retries=3):\n",
    "    \"\"\"Start a streaming query with error handling and retries\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            query = stream_func()\n",
    "            print(f\"Successfully started {query_name} on attempt {attempt + 1}\")\n",
    "            return query\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {query_name}: {str(e)}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"Failed to start {query_name} after {max_retries} attempts\")\n",
    "                raise e\n",
    "\n",
    "# Example usage:\n",
    "# fraud_query = robust_stream_starter(\n",
    "#     lambda: tx_enriched.writeStream.foreachBatch(fraud_detection_rules)\n",
    "#             .option(\"checkpointLocation\", suspicions_checkpoint)\n",
    "#             .start(),\n",
    "#     suspicions_checkpoint,\n",
    "#     \"fraud_detection_engine\"\n",
    "# )\n",
    "```\n",
    "\n",
    "### Data Archival and Cleanup\n",
    "\n",
    "```sql\n",
    "-- Automated cleanup procedures\n",
    "CREATE OR REPLACE FUNCTION cleanup_old_data(retention_days INT)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "AS\n",
    "$$\n",
    "  -- Archive old bronze data\n",
    "  DELETE FROM bronze.tx_events_raw \n",
    "  WHERE event_ts < current_timestamp() - INTERVAL retention_days DAYS;\n",
    "  \n",
    "  -- Clean up old operational metrics\n",
    "  DELETE FROM ops.pipeline_metrics \n",
    "  WHERE metric_ts < current_timestamp() - INTERVAL (retention_days * 2) DAYS;\n",
    "  \n",
    "  RETURN 'Cleanup completed for data older than ' || retention_days || ' days';\n",
    "$$;\n",
    "\n",
    "-- Schedule cleanup (run weekly)\n",
    "-- SELECT cleanup_old_data(30);\n",
    "```\n",
    "\n",
    "### Health Checks and Alerting\n",
    "\n",
    "```python\n",
    "def health_check_pipeline():\n",
    "    \"\"\"Comprehensive pipeline health check\"\"\"\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check if all streams are active\n",
    "    expected_streams = [\n",
    "        \"bronze_to_silver_tx_clean\",\n",
    "        \"silver_enrichment\", \n",
    "        \"customer_velocity\",\n",
    "        \"device_behavior\",\n",
    "        \"fraud_detection_engine\"\n",
    "    ]\n",
    "    \n",
    "    active_stream_names = [q.name for q in spark.streams.active if q.name]\n",
    "    \n",
    "    for expected in expected_streams:\n",
    "        if expected not in active_stream_names:\n",
    "            issues.append(f\"Stream '{expected}' is not active\")\n",
    "    \n",
    "    # Check data freshness\n",
    "    latest_bronze = spark.sql(\"\"\"\n",
    "        SELECT MAX(event_ts) as latest_ts \n",
    "        FROM bronze.tx_events_raw\n",
    "    \"\"\").collect()[0].latest_ts\n",
    "    \n",
    "    if latest_bronze:\n",
    "        freshness_minutes = (datetime.now() - latest_bronze).total_seconds() / 60\n",
    "        if freshness_minutes > 10:  # Alert if data is more than 10 minutes old\n",
    "            issues.append(f\"Data freshness issue: latest data is {freshness_minutes:.1f} minutes old\")\n",
    "    \n",
    "    # Check error rates\n",
    "    total_bronze = spark.sql(\"SELECT COUNT(*) as cnt FROM bronze.tx_events_raw\").collect()[0].cnt\n",
    "    total_silver = spark.sql(\"SELECT COUNT(*) as cnt FROM silver.tx_events_clean\").collect()[0].cnt\n",
    "    \n",
    "    if total_bronze > 0:\n",
    "        success_rate = total_silver / total_bronze * 100\n",
    "        if success_rate < 95:  # Alert if success rate below 95%\n",
    "            issues.append(f\"Low processing success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Return health status\n",
    "    if issues:\n",
    "        return {\"status\": \"UNHEALTHY\", \"issues\": issues}\n",
    "    else:\n",
    "        return {\"status\": \"HEALTHY\", \"issues\": []}\n",
    "\n",
    "# Run health check\n",
    "health_status = health_check_pipeline()\n",
    "print(f\"Pipeline Health: {health_status['status']}\")\n",
    "if health_status['issues']:\n",
    "    print(\"Issues found:\")\n",
    "    for issue in health_status['issues']:\n",
    "        print(f\"  - {issue}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Exercise Summary\n",
    "\n",
    "### What Students Will Learn\n",
    "\n",
    "1. **Enterprise Streaming Architecture**\n",
    "   - Multi-layer data architecture (Bronze ‚Üí Silver ‚Üí Gold)\n",
    "   - Real-time data ingestion and processing\n",
    "   - Stateful stream processing with watermarks\n",
    "\n",
    "2. **Advanced Fraud Detection**\n",
    "   - Complex rule-based detection logic\n",
    "   - Velocity analysis and behavioral patterns\n",
    "   - Multi-dimensional risk scoring\n",
    "\n",
    "3. **Production Operations**\n",
    "   - Checkpointing and fault tolerance\n",
    "   - Monitoring and alerting systems\n",
    "   - Performance optimization techniques\n",
    "\n",
    "4. **Databricks Platform Mastery**\n",
    "   - Delta Lake features and optimizations\n",
    "   - Structured Streaming capabilities\n",
    "   - Unity Catalog integration (if enabled)\n",
    "\n",
    "### Deployment Instructions\n",
    "\n",
    "1. **Sequential Notebook Execution:**\n",
    "   - Run notebooks 00-02 first to set up infrastructure\n",
    "   - Start notebook 01 (generators) and leave running\n",
    "   - Execute notebooks 03-05 to start streaming pipelines\n",
    "   - Use notebook 06 for ongoing monitoring\n",
    "\n",
    "2. **Resource Requirements:**\n",
    "   - Minimum: 2-node cluster (8 cores, 32GB RAM)\n",
    "   - Recommended: 4-node cluster (16 cores, 64GB RAM)\n",
    "   - Storage: 100GB+ for checkpoint and data files\n",
    "\n",
    "3. **Runtime Considerations:**\n",
    "   - Allow 2-3 hours for full exercise completion\n",
    "   - Streams should run for at least 30 minutes to see patterns\n",
    "   - Monitor resource usage and scale as needed\n",
    "\n",
    "This comprehensive exercise provides hands-on experience with enterprise-grade streaming analytics using only Databricks-native components, preparing students for real-world fraud detection implementations."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "9.2-Exercise-EnterpriseGrade-Real-TimeFraudDetectionSystem",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
