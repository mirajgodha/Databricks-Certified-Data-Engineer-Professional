{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027ab582-527c-4dc5-96a5-46bb1e1785a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Delta Lake Professional Lab - Answer Sheet\n",
    "\n",
    "## Overview\n",
    "This answer sheet provides detailed solutions and explanations for all questions posed in the Delta Lake Professional Lab Exercise. Each answer includes the expected response and additional context to deepen understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Creating Your First Delta Lake Table\n",
    "\n",
    "### Task 1 Questions & Answers\n",
    "\n",
    "**Q1: What files were created in the Delta Lake table directory?**\n",
    "\n",
    "**Answer:**\n",
    "When you create a Delta Lake table, the following files are created:\n",
    "- **Parquet data files**: Contain the actual table data (e.g., `part-00000-xxxx.c000.snappy.parquet`)\n",
    "- **_delta_log directory**: Contains transaction log files\n",
    "  - `00000000000000000000.json`: The first transaction log file\n",
    "  - `_last_checkpoint`: Metadata file for checkpointing\n",
    "- **_SUCCESS file**: Indicates successful write operation (may not always be present)\n",
    "\n",
    "Example directory structure:\n",
    "```\n",
    "/tmp/delta_lab/orders_delta/\n",
    "├── _delta_log/\n",
    "│   └── 00000000000000000000.json\n",
    "├── part-00000-c1a34b3a-4562-4d91-bf24-6b8a4c5d1234.c000.snappy.parquet\n",
    "├── part-00001-c1a34b3a-4562-4d91-bf24-6b8a4c5d1234.c000.snappy.parquet\n",
    "└── _SUCCESS\n",
    "```\n",
    "\n",
    "**Q2: How does a Delta Lake table differ from a regular Parquet table?**\n",
    "\n",
    "**Answer:**\n",
    "Key differences include:\n",
    "\n",
    "| Aspect | Regular Parquet | Delta Lake |\n",
    "|--------|----------------|------------|\n",
    "| **ACID Transactions** | No | Yes - full ACID compliance |\n",
    "| **Schema Evolution** | Manual | Automatic with schema enforcement |\n",
    "| **Time Travel** | Not supported | Built-in versioning and time travel |\n",
    "| **Metadata** | Limited | Rich metadata in transaction logs |\n",
    "| **Concurrent Writes** | Not safe | Optimistic concurrency control |\n",
    "| **Data Quality** | Manual checks | Built-in constraints and validation |\n",
    "| **File Management** | Manual | Automatic with OPTIMIZE and VACUUM |\n",
    "\n",
    "**Additional Context:** Delta Lake adds a transaction layer on top of Parquet, enabling enterprise-grade data lake capabilities while maintaining compatibility with existing Spark/Parquet ecosystems.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Understanding Delta Lake Architecture\n",
    "\n",
    "### Task 2 Questions & Answers\n",
    "\n",
    "**Q1: What information is stored in the Delta log files?**\n",
    "\n",
    "**Answer:**\n",
    "Delta log files (JSON format) contain:\n",
    "\n",
    "1. **Transaction Metadata:**\n",
    "   - Version number\n",
    "   - Timestamp\n",
    "   - Operation type (CREATE, INSERT, UPDATE, DELETE, etc.)\n",
    "   - User information and application details\n",
    "\n",
    "2. **File-level Information:**\n",
    "   - `add`: Records of new data files added\n",
    "   - `remove`: Records of files removed/deleted\n",
    "   - File paths, sizes, and modification times\n",
    "   - Partition information\n",
    "\n",
    "3. **Schema Information:**\n",
    "   - Table schema definition\n",
    "   - Column names, types, and metadata\n",
    "   - Schema evolution history\n",
    "\n",
    "4. **Table Properties:**\n",
    "   - Table configuration\n",
    "   - Statistics (min, max, null counts per column)\n",
    "   - Checkpoint information\n",
    "\n",
    "**Example log entry structure:**\n",
    "```json\n",
    "{\n",
    "  \"commitInfo\": {\n",
    "    \"timestamp\": 1696500000000,\n",
    "    \"operation\": \"WRITE\",\n",
    "    \"operationParameters\": {\"mode\": \"Overwrite\"},\n",
    "    \"readVersion\": 0,\n",
    "    \"isBlindAppend\": false\n",
    "  },\n",
    "  \"add\": {\n",
    "    \"path\": \"part-00000-xxx.parquet\",\n",
    "    \"partitionValues\": {},\n",
    "    \"size\": 12345,\n",
    "    \"modificationTime\": 1696500000000,\n",
    "    \"dataChange\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Q2: How many data files were created initially?**\n",
    "\n",
    "**Answer:**\n",
    "The number of data files depends on:\n",
    "- **Dataset size**: 10,000 records in the example\n",
    "- **Spark parallelism**: Number of partitions (typically 200 by default)\n",
    "- **Data distribution**: How Spark distributes the data\n",
    "\n",
    "**Typical result:** 4-8 parquet files for 10,000 records, but this can vary based on cluster configuration and data size per partition.\n",
    "\n",
    "**Q3: What does the commitInfo tell us about the transaction?**\n",
    "\n",
    "**Answer:**\n",
    "The `commitInfo` section provides:\n",
    "- **Timestamp**: When the transaction was committed\n",
    "- **Operation**: Type of operation (WRITE, UPDATE, DELETE, etc.)\n",
    "- **Operation Parameters**: Specific parameters like write mode\n",
    "- **Read Version**: The table version before this transaction\n",
    "- **User Information**: Who performed the operation\n",
    "- **Application Details**: Spark application info\n",
    "- **Performance Metrics**: Optional metrics like number of files written\n",
    "\n",
    "This information is crucial for:\n",
    "- Auditing and compliance\n",
    "- Debugging data pipeline issues\n",
    "- Understanding table evolution\n",
    "- Performance analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Time Travel and Versioning\n",
    "\n",
    "### Task 3 Questions & Answers\n",
    "\n",
    "**Q1: How many versions of the table exist now?**\n",
    "\n",
    "**Answer:**\n",
    "After completing Task 3.1, there should be **4 versions** (0-3):\n",
    "- **Version 0**: Initial table creation with 10,000 records\n",
    "- **Version 1**: After appending 2,000 new records (total: 12,000)\n",
    "- **Version 2**: After updating 'Pending' to 'Shipped' for North region\n",
    "- **Version 3**: After deleting cancelled orders\n",
    "\n",
    "Each DML operation (INSERT, UPDATE, DELETE) creates a new version in Delta Lake.\n",
    "\n",
    "**Q2: What operations triggered new versions?**\n",
    "\n",
    "**Answer:**\n",
    "Every write operation creates a new version:\n",
    "\n",
    "1. **CREATE/OVERWRITE**: Version 0\n",
    "   - `df.write.mode(\"overwrite\").saveAsTable()`\n",
    "   \n",
    "2. **INSERT/APPEND**: Version 1\n",
    "   - `df.write.mode(\"append\").saveAsTable()`\n",
    "   \n",
    "3. **UPDATE**: Version 2\n",
    "   - `UPDATE table SET column = value WHERE condition`\n",
    "   \n",
    "4. **DELETE**: Version 3\n",
    "   - `DELETE FROM table WHERE condition`\n",
    "\n",
    "**Other version-creating operations:**\n",
    "- `ALTER TABLE` (schema changes)\n",
    "- `OPTIMIZE` operations\n",
    "- `VACUUM` with retention period changes\n",
    "\n",
    "**Q3: How do version-based and timestamp-based time travel differ?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| Version-Based | Timestamp-Based |\n",
    "|---------------|----------------|\n",
    "| **Syntax**: `VERSION AS OF 2` | **Syntax**: `TIMESTAMP AS OF '2023-10-15 14:30:00'` |\n",
    "| **Precision**: Exact version number | **Precision**: Specific point in time |\n",
    "| **Use Case**: Known version changes | **Use Case**: Point-in-time recovery |\n",
    "| **Reliability**: Always available | **Reliability**: Subject to VACUUM retention |\n",
    "\n",
    "**Examples:**\n",
    "```sql\n",
    "-- Version-based\n",
    "SELECT * FROM table VERSION AS OF 2\n",
    "\n",
    "-- Timestamp-based  \n",
    "SELECT * FROM table TIMESTAMP AS OF '2023-10-15T14:30:00'\n",
    "SELECT * FROM table TIMESTAMP AS OF current_timestamp() - INTERVAL 1 HOUR\n",
    "```\n",
    "\n",
    "**Best Practice:** Use version-based for development/testing and timestamp-based for production point-in-time queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: File Management and Optimization\n",
    "\n",
    "### Task 4 Questions & Answers\n",
    "\n",
    "**Q1: Why do we have so many small files?**\n",
    "\n",
    "**Answer:**\n",
    "Small files are created due to:\n",
    "\n",
    "1. **Frequent Small Inserts**: Each small append operation creates new files\n",
    "2. **High Parallelism**: Spark creates one file per partition per write\n",
    "3. **Partition Strategy**: Over-partitioning leads to small files per partition  \n",
    "4. **UPDATE/DELETE Operations**: These operations can create small files due to copy-on-write semantics\n",
    "5. **Streaming Writes**: Micro-batches create many small files over time\n",
    "\n",
    "**Example Scenario:**\n",
    "- 20 insert operations × 100 records each\n",
    "- Each insert might create 2-4 small files\n",
    "- Total: 40-80 small files instead of a few optimally-sized files\n",
    "\n",
    "**Q2: What problems do small files cause?**\n",
    "\n",
    "**Answer:**\n",
    "Small files create several performance issues:\n",
    "\n",
    "1. **Query Performance:**\n",
    "   - More files to scan and open\n",
    "   - Increased I/O overhead\n",
    "   - Poor data locality\n",
    "\n",
    "2. **Metadata Overhead:**\n",
    "   - More entries in Delta log\n",
    "   - Increased planning time\n",
    "   - Higher memory usage for file metadata\n",
    "\n",
    "3. **Cloud Storage Costs:**\n",
    "   - More API calls (charged per request)\n",
    "   - Inefficient storage utilization\n",
    "   - Higher data transfer costs\n",
    "\n",
    "4. **Resource Utilization:**\n",
    "   - Poor CPU utilization\n",
    "   - Suboptimal compression ratios\n",
    "   - Increased network overhead\n",
    "\n",
    "**Performance Impact Example:**\n",
    "- 1000 small files (1MB each) vs. 10 large files (100MB each)\n",
    "- Query time can be 3-10x slower with small files\n",
    "\n",
    "**Q3: How many transaction log files do we have now?**\n",
    "\n",
    "**Answer:**\n",
    "After Task 4.2 completion:\n",
    "- **Initial operations**: 4 log files (versions 0-3)  \n",
    "- **Small inserts**: 20 additional log files (one per insert)\n",
    "- **Total**: Approximately 24 transaction log files\n",
    "\n",
    "Each file named sequentially:\n",
    "```\n",
    "00000000000000000000.json  (version 0)\n",
    "00000000000000000001.json  (version 1)\n",
    "...\n",
    "00000000000000000023.json  (version 23)\n",
    "```\n",
    "\n",
    "Delta Lake automatically creates checkpoints every 10 commits to optimize log reading performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 5: Table Optimization with OPTIMIZE\n",
    "\n",
    "### Task 5 Questions & Answers\n",
    "\n",
    "**Q1: How did OPTIMIZE affect the number of data files?**\n",
    "\n",
    "**Answer:**\n",
    "OPTIMIZE consolidates small files into larger, optimally-sized files:\n",
    "\n",
    "**Before OPTIMIZE:**\n",
    "- Many small files (40-80 files from multiple inserts)\n",
    "- File sizes: 1MB - 50MB each\n",
    "- Total size: ~500MB spread across many files\n",
    "\n",
    "**After OPTIMIZE:**\n",
    "- Fewer, larger files (4-8 optimized files)\n",
    "- File sizes: 128MB - 1GB each (target: 1GB per file)\n",
    "- Same total size: ~500MB in optimized layout\n",
    "\n",
    "**Typical Results:**\n",
    "- 80% reduction in file count\n",
    "- 2-5x improvement in query performance\n",
    "- Better compression ratios\n",
    "\n",
    "**Q2: What is the difference between OPTIMIZE and OPTIMIZE ZORDER BY?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "| OPTIMIZE | OPTIMIZE ZORDER BY |\n",
    "|----------|-------------------|\n",
    "| **Purpose** | File compaction only | File compaction + data layout optimization |\n",
    "| **File Size** | Creates optimally-sized files | Creates optimally-sized files |\n",
    "| **Data Layout** | Random data distribution | Clustered data distribution |\n",
    "| **Query Performance** | Improves due to fewer files | Improves dramatically for filtered queries |\n",
    "| **Cost** | Lower compute cost | Higher compute cost |\n",
    "| **Best For** | General file management | Queries with WHERE clauses |\n",
    "\n",
    "**Z-ORDER Benefits:**\n",
    "- **Data Skipping**: Skip files that don't contain queried values\n",
    "- **Better Compression**: Similar values clustered together\n",
    "- **Cache Efficiency**: Better data locality\n",
    "\n",
    "**Example Performance Impact:**\n",
    "```sql\n",
    "-- Query that benefits from Z-ORDER BY (region, product_category)\n",
    "SELECT * FROM orders \n",
    "WHERE region = 'North' AND product_category = 'Electronics'\n",
    "\n",
    "-- Without Z-ORDER: Scans all files\n",
    "-- With Z-ORDER: Might skip 70-90% of files\n",
    "```\n",
    "\n",
    "**Q3: When should you use Z-ordering?**\n",
    "\n",
    "**Answer:**\n",
    "Use Z-ordering when:\n",
    "\n",
    "1. **High-Cardinality Columns**: Columns with many distinct values\n",
    "2. **Common Filter Columns**: Columns frequently used in WHERE clauses\n",
    "3. **Range Queries**: Columns used in range filters (>, <, BETWEEN)\n",
    "4. **Large Tables**: Tables > 1TB where file skipping provides significant benefits\n",
    "\n",
    "**Z-ORDER Column Selection Guidelines:**\n",
    "- **Primary**: Most selective filter columns (highest cardinality)\n",
    "- **Secondary**: Secondary filter columns\n",
    "- **Limit**: Use 2-4 columns maximum (diminishing returns beyond this)\n",
    "- **Avoid**: Very low cardinality columns (< 100 unique values)\n",
    "\n",
    "**Examples:**\n",
    "```sql\n",
    "-- Good Z-ORDER candidates\n",
    "OPTIMIZE table ZORDER BY (customer_id, order_date)        -- High cardinality + time-based queries\n",
    "OPTIMIZE table ZORDER BY (region, product_category)       -- Common filter combinations\n",
    "OPTIMIZE table ZORDER BY (status, created_timestamp)      -- Status filtering + time queries\n",
    "\n",
    "-- Poor Z-ORDER candidates  \n",
    "OPTIMIZE table ZORDER BY (status)                         -- Too low cardinality alone\n",
    "OPTIMIZE table ZORDER BY (col1, col2, col3, col4, col5)  -- Too many columns\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 6: Vacuum Operations\n",
    "\n",
    "### Task 6 Questions & Answers\n",
    "\n",
    "**Q1: What happens to old data files after VACUUM?**\n",
    "\n",
    "**Answer:**\n",
    "VACUUM permanently removes:\n",
    "\n",
    "1. **Old Data Files**: \n",
    "   - Files no longer referenced by any table version within retention period\n",
    "   - Files from UPDATE/DELETE operations (copy-on-write creates new files)\n",
    "   - Files replaced by OPTIMIZE operations\n",
    "\n",
    "2. **Unreferenced Files**:\n",
    "   - Failed write attempts\n",
    "   - Temporary files from aborted transactions\n",
    "   - Files from rolled-back operations\n",
    "\n",
    "**Files NOT Removed**:\n",
    "- Files needed for versions within retention period\n",
    "- Transaction log files (never removed by VACUUM)\n",
    "- Files referenced by current table version\n",
    "- Checkpoint files\n",
    "\n",
    "**Example Before/After VACUUM:**\n",
    "```\n",
    "Before VACUUM (100 files):\n",
    "- 20 files needed for current version  \n",
    "- 60 files needed for versions within retention\n",
    "- 20 old files beyond retention period\n",
    "\n",
    "After VACUUM (80 files):\n",
    "- 20 files needed for current version\n",
    "- 60 files needed for versions within retention  \n",
    "- 0 old files (removed by VACUUM)\n",
    "```\n",
    "\n",
    "**Q2: Why is there a default retention period?**\n",
    "\n",
    "**Answer:**\n",
    "The default retention period (7 days/168 hours) exists for:\n",
    "\n",
    "1. **Time Travel Protection**: Ensures historical queries remain functional\n",
    "2. **Concurrent Operations**: Protects against conflicts with long-running queries\n",
    "3. **Disaster Recovery**: Maintains ability to restore recent data versions\n",
    "4. **Compliance Requirements**: Meets audit trail and data lineage needs\n",
    "5. **Safety Buffer**: Prevents accidental data loss from overly aggressive cleanup\n",
    "\n",
    "**Risk Mitigation:**\n",
    "- **Long-running Queries**: Jobs that started before VACUUM but finish after\n",
    "- **Streaming Jobs**: Continuous processing that might need historical data\n",
    "- **Analytics Workloads**: BI tools that cache query plans referencing old versions\n",
    "- **Cross-timezone Operations**: Global teams working across different time zones\n",
    "\n",
    "**Q3: What are the risks of setting retention period too low?**\n",
    "\n",
    "**Answer:**\n",
    "Risks of low retention periods:\n",
    "\n",
    "1. **Query Failures**:\n",
    "   ```sql\n",
    "   -- This query will fail if version 5 files were vacuumed\n",
    "   SELECT * FROM table VERSION AS OF 5\n",
    "   ```\n",
    "   Error: \"The table was created/last updated at a timestamp that cannot be found\"\n",
    "\n",
    "2. **Concurrent Job Failures**:\n",
    "   - Long-running ETL jobs that started before VACUUM\n",
    "   - Streaming applications with checkpoint lag\n",
    "   - Cross-cluster operations with network delays\n",
    "\n",
    "3. **Time Travel Loss**:\n",
    "   - Historical analysis becomes impossible\n",
    "   - Audit trail is broken\n",
    "   - Compliance violations possible\n",
    "\n",
    "4. **Recovery Limitations**:\n",
    "   - Cannot restore recent changes\n",
    "   - Limited debugging capabilities\n",
    "   - Reduced disaster recovery options\n",
    "\n",
    "**Production Best Practices:**\n",
    "- **Standard Retention**: 7 days (168 hours) minimum\n",
    "- **High-Volume Tables**: 30+ days for critical data\n",
    "- **Development**: Can use shorter periods (24-48 hours)\n",
    "- **Compliance Tables**: Extended retention (90+ days)\n",
    "\n",
    "**Safe VACUUM Command:**\n",
    "```sql\n",
    "-- Safe for production\n",
    "VACUUM table RETAIN 168 HOURS  -- 7 days\n",
    "\n",
    "-- Only for development/testing  \n",
    "VACUUM table RETAIN 24 HOURS   -- 1 day\n",
    "\n",
    "-- NEVER in production\n",
    "VACUUM table RETAIN 0 HOURS    -- Immediate cleanup\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 7: Advanced Monitoring and Analysis\n",
    "\n",
    "### Task 7 Questions & Answers\n",
    "\n",
    "**Q1: How do you monitor Delta Lake table health?**\n",
    "\n",
    "**Answer:**\n",
    "Monitor Delta Lake table health using multiple metrics:\n",
    "\n",
    "1. **File-Level Metrics**:\n",
    "   ```sql\n",
    "   -- Check file statistics\n",
    "   DESCRIBE DETAIL table_name\n",
    "   ```\n",
    "   Key metrics:\n",
    "   - `numFiles`: Number of data files\n",
    "   - `sizeInBytes`: Total table size\n",
    "   - `format`: Should be \"delta\"\n",
    "\n",
    "2. **Version History Monitoring**:\n",
    "   ```sql\n",
    "   -- Analyze table history\n",
    "   DESCRIBE HISTORY table_name\n",
    "   ```\n",
    "   Monitor:\n",
    "   - Version growth rate\n",
    "   - Operation frequency\n",
    "   - File changes over time\n",
    "\n",
    "3. **Query Performance Metrics**:\n",
    "   - Query execution time trends\n",
    "   - Files scanned vs. total files\n",
    "   - Data skipping effectiveness\n",
    "\n",
    "4. **Custom Health Dashboard**:\n",
    "   ```sql\n",
    "   WITH health_metrics AS (\n",
    "     SELECT \n",
    "       table_name,\n",
    "       num_files,\n",
    "       size_in_bytes / (1024*1024*1024) as size_gb,\n",
    "       num_files / (size_in_bytes / (128*1024*1024)) as file_size_ratio\n",
    "     FROM (DESCRIBE DETAIL table_name)\n",
    "   )\n",
    "   SELECT \n",
    "     *,\n",
    "     CASE \n",
    "       WHEN num_files > 1000 THEN 'HIGH FILE COUNT - OPTIMIZE NEEDED'\n",
    "       WHEN file_size_ratio < 0.1 THEN 'SMALL FILES - OPTIMIZE NEEDED'  \n",
    "       ELSE 'HEALTHY'\n",
    "     END as health_status\n",
    "   FROM health_metrics\n",
    "   ```\n",
    "\n",
    "**Q2: What metrics indicate a table needs optimization?**\n",
    "\n",
    "**Answer:**\n",
    "Key optimization indicators:\n",
    "\n",
    "1. **File Count Issues**:\n",
    "   - **Small Files**: > 1000 files for tables < 10GB\n",
    "   - **File Size**: Average file size < 128MB\n",
    "   - **File Size Variation**: High standard deviation in file sizes\n",
    "\n",
    "2. **Performance Degradation**:\n",
    "   - Query time increasing over time\n",
    "   - High \"files scanned\" vs \"files skipped\" ratio\n",
    "   - Increasing metadata scan time\n",
    "\n",
    "3. **Growth Patterns**:\n",
    "   - Frequent small inserts/updates\n",
    "   - Version count growing rapidly\n",
    "   - Size growth without proportional performance improvement\n",
    "\n",
    "**Optimization Thresholds:**\n",
    "```sql\n",
    "SELECT \n",
    "  table_name,\n",
    "  CASE\n",
    "    WHEN num_files > (size_in_bytes / (128 * 1024 * 1024)) * 2 \n",
    "      THEN 'OPTIMIZE NEEDED - TOO MANY SMALL FILES'\n",
    "    WHEN num_files < (size_in_bytes / (1024 * 1024 * 1024)) \n",
    "      THEN 'OPTIMIZE NEEDED - FILES TOO LARGE'\n",
    "    ELSE 'FILE SIZE OPTIMAL'\n",
    "  END as optimization_status\n",
    "FROM (DESCRIBE DETAIL table_name)\n",
    "```\n",
    "\n",
    "**Q3: How often should you run OPTIMIZE and VACUUM?**\n",
    "\n",
    "**Answer:**\n",
    "Optimization frequency depends on table usage patterns:\n",
    "\n",
    "**OPTIMIZE Frequency:**\n",
    "\n",
    "1. **High-Velocity Tables** (frequent writes):\n",
    "   - **Streaming Tables**: Daily or after every 100-500 versions\n",
    "   - **Batch ETL Tables**: After each major load\n",
    "   - **Trigger**: When file count > 10x optimal\n",
    "\n",
    "2. **Medium-Velocity Tables**:\n",
    "   - **Weekly**: For tables with daily updates\n",
    "   - **Trigger**: When query performance degrades > 20%\n",
    "\n",
    "3. **Low-Velocity Tables**:\n",
    "   - **Monthly or Quarterly**: For tables with infrequent updates\n",
    "   - **As-needed**: Based on performance monitoring\n",
    "\n",
    "**VACUUM Frequency:**\n",
    "\n",
    "1. **Production Tables**:\n",
    "   - **Weekly**: Standard maintenance schedule  \n",
    "   - **After Major Operations**: Post-OPTIMIZE, major migrations\n",
    "   - **Storage Cost Concerns**: More frequent for cost optimization\n",
    "\n",
    "2. **Development Tables**:\n",
    "   - **Daily**: Faster iteration, lower retention needs\n",
    "   - **Automated**: Part of CI/CD pipeline cleanup\n",
    "\n",
    "**Automated Scheduling Example:**\n",
    "```sql\n",
    "-- Daily optimization for high-velocity tables\n",
    "CREATE OR REPLACE FUNCTION optimize_high_velocity_tables()\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "AS $$\n",
    "  OPTIMIZE high_velocity_table ZORDER BY (common_filter_cols);\n",
    "  SELECT 'High velocity tables optimized'\n",
    "$$;\n",
    "\n",
    "-- Weekly VACUUM for all tables\n",
    "CREATE OR REPLACE FUNCTION vacuum_all_tables()  \n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "AS $$\n",
    "  VACUUM table1 RETAIN 168 HOURS;\n",
    "  VACUUM table2 RETAIN 168 HOURS;\n",
    "  SELECT 'All tables vacuumed'\n",
    "$$;\n",
    "```\n",
    "\n",
    "**Best Practice Schedule:**\n",
    "- **Monday**: VACUUM operations (start week with clean tables)\n",
    "- **Wednesday**: OPTIMIZE high-velocity tables\n",
    "- **Friday**: Performance monitoring and health checks\n",
    "- **Monthly**: Review optimization strategies and thresholds\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 8: Best Practices Implementation\n",
    "\n",
    "### Task 8 Questions & Answers\n",
    "\n",
    "**Q1: What maintenance schedule would you recommend for a production table?**\n",
    "\n",
    "**Answer:**\n",
    "Recommended production maintenance schedule:\n",
    "\n",
    "**Daily (Automated):**\n",
    "```sql\n",
    "-- High-frequency tables (>1000 operations/day)\n",
    "OPTIMIZE high_velocity_table ZORDER BY (primary_filter_columns)\n",
    "  WHERE num_files > optimal_file_count * 1.5;\n",
    "```\n",
    "\n",
    "**Weekly (Automated):**\n",
    "```sql  \n",
    "-- All production tables\n",
    "VACUUM production_table RETAIN 168 HOURS;\n",
    "\n",
    "-- Medium-frequency tables  \n",
    "OPTIMIZE medium_velocity_table ZORDER BY (filter_columns)\n",
    "  WHERE files_written_last_week > 100;\n",
    "```\n",
    "\n",
    "**Monthly (Semi-Automated):**\n",
    "```sql\n",
    "-- Review and adjust Z-ORDER columns\n",
    "ANALYZE TABLE production_table COMPUTE STATISTICS;\n",
    "\n",
    "-- Performance analysis and optimization strategy review\n",
    "DESCRIBE HISTORY production_table LIMIT 1000;\n",
    "```\n",
    "\n",
    "**Quarterly (Manual Review):**\n",
    "- Partition strategy evaluation\n",
    "- Z-ORDER column effectiveness analysis  \n",
    "- Retention policy review\n",
    "- Cost optimization assessment\n",
    "\n",
    "**Sample Maintenance Function:**\n",
    "```python\n",
    "def production_table_maintenance():\n",
    "    \"\"\"\n",
    "    Production-ready maintenance routine\n",
    "    \"\"\"\n",
    "    # Daily high-velocity table optimization\n",
    "    high_velocity_tables = [\"orders\", \"events\", \"transactions\"]\n",
    "    for table in high_velocity_tables:\n",
    "        if get_file_count(table) > get_optimal_file_count(table) * 1.5:\n",
    "            spark.sql(f\"OPTIMIZE {table} ZORDER BY (primary_filters)\")\n",
    "    \n",
    "    # Weekly vacuum (run on Sundays)\n",
    "    if datetime.now().weekday() == 6:  # Sunday\n",
    "        all_tables = get_production_tables()\n",
    "        for table in all_tables:\n",
    "            spark.sql(f\"VACUUM {table} RETAIN 168 HOURS\")\n",
    "    \n",
    "    # Log results to monitoring system\n",
    "    log_maintenance_results()\n",
    "```\n",
    "\n",
    "**Q2: How do you decide which columns to use for Z-ordering?**\n",
    "\n",
    "**Answer:**\n",
    "Z-ORDER column selection methodology:\n",
    "\n",
    "**1. Query Analysis:**\n",
    "```sql\n",
    "-- Analyze query patterns from query history\n",
    "SELECT \n",
    "  query_text,\n",
    "  COUNT(*) as frequency,\n",
    "  AVG(execution_time_ms) as avg_time\n",
    "FROM system.query_history \n",
    "WHERE query_text LIKE '%table_name%'\n",
    "  AND query_text LIKE '%WHERE%'\n",
    "GROUP BY query_text\n",
    "ORDER BY frequency DESC;\n",
    "```\n",
    "\n",
    "**2. Column Cardinality Analysis:**\n",
    "```sql\n",
    "-- Check column cardinality (uniqueness)\n",
    "SELECT \n",
    "  'customer_id' as column_name,\n",
    "  COUNT(DISTINCT customer_id) as unique_values,\n",
    "  COUNT(*) as total_rows,\n",
    "  COUNT(DISTINCT customer_id) * 1.0 / COUNT(*) as cardinality_ratio\n",
    "FROM table_name\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'region' as column_name,\n",
    "  COUNT(DISTINCT region),\n",
    "  COUNT(*),\n",
    "  COUNT(DISTINCT region) * 1.0 / COUNT(*)\n",
    "FROM table_name;\n",
    "```\n",
    "\n",
    "**3. Selection Criteria:**\n",
    "\n",
    "**Primary Z-ORDER Candidates (Choose 1-2):**\n",
    "- **High Cardinality** (cardinality ratio > 0.01)\n",
    "- **Frequent Filters** (used in >50% of queries)  \n",
    "- **Range Queries** (date columns, numeric ranges)\n",
    "- **Equality Filters** (customer_id, account_id)\n",
    "\n",
    "**Secondary Z-ORDER Candidates (Choose 0-2):**\n",
    "- **Medium Cardinality** (100-10,000 unique values)\n",
    "- **Commonly Combined** (used together in WHERE clauses)\n",
    "- **Business Critical** (important for main use cases)\n",
    "\n",
    "**Avoid These Columns:**\n",
    "- **Very Low Cardinality** (< 100 unique values)\n",
    "- **Very High Cardinality** (> 10M unique values, like UUIDs)\n",
    "- **Rarely Filtered** (< 10% of queries)\n",
    "- **Non-Selective** (doesn't eliminate many rows)\n",
    "\n",
    "**4. Example Decision Matrix:**\n",
    "```\n",
    "Column          | Cardinality | Query Freq | Selectivity | Z-ORDER Priority\n",
    "customer_id     | High        | High       | High        | 1 (Primary)\n",
    "order_date      | High        | High       | High        | 2 (Primary)  \n",
    "region          | Low         | Medium     | Medium      | 3 (Secondary)\n",
    "product_category| Medium      | High       | Medium      | 3 (Secondary)\n",
    "status          | Very Low    | Low        | Low         | No\n",
    "order_id        | Very High   | Low        | Very High   | No (too unique)\n",
    "```\n",
    "\n",
    "**5. Testing and Validation:**\n",
    "```sql\n",
    "-- Test Z-ORDER effectiveness\n",
    "-- Before Z-ORDER\n",
    "SELECT COUNT(*) FROM table WHERE customer_id = 'CUST001' AND order_date >= '2023-01-01';\n",
    "\n",
    "-- After Z-ORDER BY (customer_id, order_date)  \n",
    "SELECT COUNT(*) FROM table WHERE customer_id = 'CUST001' AND order_date >= '2023-01-01';\n",
    "\n",
    "-- Compare execution plans and files scanned\n",
    "```\n",
    "\n",
    "**Q3: What alerts would you set up for Delta Lake table health?**\n",
    "\n",
    "**Answer:**\n",
    "Comprehensive alerting strategy:\n",
    "\n",
    "**1. File Count Alerts:**\n",
    "```python\n",
    "# Alert when file count exceeds optimal threshold\n",
    "def check_file_count_alert(table_name, threshold_multiplier=2.0):\n",
    "    details = spark.sql(f\"DESCRIBE DETAIL {table_name}\").collect()[0]\n",
    "    num_files = details['numFiles']\n",
    "    size_gb = details['sizeInBytes'] / (1024**3)\n",
    "    \n",
    "    # Optimal files: ~1 file per 128MB-1GB  \n",
    "    optimal_files = max(1, int(size_gb))\n",
    "    \n",
    "    if num_files > optimal_files * threshold_multiplier:\n",
    "        return {\n",
    "            'alert': 'HIGH_FILE_COUNT',\n",
    "            'severity': 'WARNING',\n",
    "            'message': f'{table_name} has {num_files} files, optimal is ~{optimal_files}',\n",
    "            'action': f'Run OPTIMIZE {table_name}'\n",
    "        }\n",
    "```\n",
    "\n",
    "**2. Performance Degradation Alerts:**\n",
    "```python  \n",
    "# Alert on query performance degradation\n",
    "def check_performance_alert(table_name, performance_threshold=1.5):\n",
    "    recent_avg = get_recent_query_performance(table_name, days=7)\n",
    "    baseline_avg = get_baseline_query_performance(table_name, days=30)\n",
    "    \n",
    "    if recent_avg > baseline_avg * performance_threshold:\n",
    "        return {\n",
    "            'alert': 'PERFORMANCE_DEGRADATION', \n",
    "            'severity': 'CRITICAL',\n",
    "            'message': f'{table_name} queries {performance_threshold}x slower than baseline',\n",
    "            'action': f'Investigate and consider OPTIMIZE with ZORDER'\n",
    "        }\n",
    "```\n",
    "\n",
    "**3. Storage Growth Alerts:**\n",
    "```python\n",
    "# Alert on unexpected storage growth\n",
    "def check_storage_growth_alert(table_name, growth_threshold=2.0):\n",
    "    current_size = get_table_size(table_name)\n",
    "    expected_size = predict_table_size(table_name)\n",
    "    \n",
    "    if current_size > expected_size * growth_threshold:\n",
    "        return {\n",
    "            'alert': 'STORAGE_ANOMALY',\n",
    "            'severity': 'WARNING', \n",
    "            'message': f'{table_name} size {growth_threshold}x larger than expected',\n",
    "            'action': f'Check for data quality issues, consider VACUUM'\n",
    "        }\n",
    "```\n",
    "\n",
    "**4. Version Growth Alerts:**\n",
    "```python\n",
    "# Alert on rapid version accumulation\n",
    "def check_version_growth_alert(table_name, versions_per_day_threshold=100):\n",
    "    history = spark.sql(f\"DESCRIBE HISTORY {table_name} LIMIT 1000\").collect()\n",
    "    \n",
    "    if len(history) >= 1000:  # Max history retrieved\n",
    "        recent_versions = [h for h in history if h['timestamp'] > datetime.now() - timedelta(days=1)]\n",
    "        \n",
    "        if len(recent_versions) > versions_per_day_threshold:\n",
    "            return {\n",
    "                'alert': 'HIGH_VERSION_VELOCITY',\n",
    "                'severity': 'INFO',\n",
    "                'message': f'{table_name} created {len(recent_versions)} versions in 24h',\n",
    "                'action': 'Consider batch operations or more frequent OPTIMIZE'\n",
    "            }\n",
    "```\n",
    "\n",
    "**5. Comprehensive Monitoring Dashboard:**\n",
    "```sql\n",
    "-- Daily health check query\n",
    "WITH table_health AS (\n",
    "  SELECT \n",
    "    'orders' as table_name,\n",
    "    current_timestamp() as check_time,\n",
    "    (SELECT COUNT(*) FROM (DESCRIBE HISTORY orders LIMIT 100)) as recent_versions,\n",
    "    (SELECT numFiles FROM (DESCRIBE DETAIL orders)) as file_count,\n",
    "    (SELECT sizeInBytes FROM (DESCRIBE DETAIL orders)) as size_bytes\n",
    "),\n",
    "health_status AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CASE \n",
    "      WHEN file_count > size_bytes / (128 * 1024 * 1024) * 2 THEN 'OPTIMIZE_NEEDED'\n",
    "      WHEN recent_versions > 50 THEN 'HIGH_ACTIVITY' \n",
    "      ELSE 'HEALTHY'\n",
    "    END as status\n",
    "  FROM table_health\n",
    ")\n",
    "SELECT * FROM health_status WHERE status != 'HEALTHY';\n",
    "```\n",
    "\n",
    "**6. Automated Alert Integration:**\n",
    "```python\n",
    "# Integration with monitoring systems\n",
    "def send_delta_alerts():\n",
    "    \"\"\"\n",
    "    Main alerting function - run via scheduled job\n",
    "    \"\"\"\n",
    "    tables = get_production_tables()\n",
    "    alerts = []\n",
    "    \n",
    "    for table in tables:\n",
    "        # Run all health checks\n",
    "        alerts.extend([\n",
    "            check_file_count_alert(table),\n",
    "            check_performance_alert(table), \n",
    "            check_storage_growth_alert(table),\n",
    "            check_version_growth_alert(table)\n",
    "        ])\n",
    "    \n",
    "    # Filter out None results\n",
    "    active_alerts = [a for a in alerts if a is not None]\n",
    "    \n",
    "    # Send to monitoring system (Datadog, CloudWatch, etc.)\n",
    "    for alert in active_alerts:\n",
    "        send_to_monitoring_system(alert)\n",
    "    \n",
    "    # Create maintenance recommendations  \n",
    "    create_maintenance_recommendations(active_alerts)\n",
    "```\n",
    "\n",
    "**Alert Severity Levels:**\n",
    "- **CRITICAL**: Query failures, major performance issues\n",
    "- **WARNING**: Optimization needed, growing inefficiencies  \n",
    "- **INFO**: Monitoring information, trend awareness\n",
    "\n",
    "**Recommended Alert Thresholds:**\n",
    "- **File Count**: > 2x optimal file count\n",
    "- **Performance**: > 50% slower than baseline  \n",
    "- **Storage**: > 100% unexpected growth\n",
    "- **Versions**: > 100 versions per day\n",
    "- **Query Failures**: Any time travel query failures\n",
    "\n",
    "This comprehensive monitoring approach ensures proactive Delta Lake table health management and prevents performance issues before they impact users.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This answer sheet provides detailed explanations for all concepts covered in the Delta Lake Professional Lab:\n",
    "\n",
    "- **Delta Lake Architecture**: Understanding data files and transaction logs\n",
    "- **Time Travel**: Version and timestamp-based historical queries  \n",
    "- **File Management**: Identifying and resolving small file problems\n",
    "- **Optimization**: Using OPTIMIZE and Z-ORDER for performance\n",
    "- **Maintenance**: VACUUM operations and retention policies\n",
    "- **Monitoring**: Comprehensive health checks and alerting strategies\n",
    "- **Best Practices**: Production-ready maintenance procedures\n",
    "\n",
    "Each answer includes practical examples, code snippets, and real-world considerations to help students apply these concepts in their professional work with Databricks and Delta Lake."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Solution-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
