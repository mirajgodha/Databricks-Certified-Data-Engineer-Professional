{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a6e5ba-1342-46f5-b051-6d2f29016cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Unity Catalog and Table Management Lab Exercise\n",
    "\n",
    "## Overview\n",
    "This comprehensive lab covers Unity Catalog concepts, table types, views, CTAS operations, and Delta Lake constraints. Students will work with a realistic HR and sales dataset to understand enterprise data management patterns.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks workspace with Unity Catalog enabled\n",
    "- Running compute cluster with Unity Catalog access\n",
    "- Basic knowledge of SQL and Spark\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the difference between managed and external tables in Unity Catalog\n",
    "- Create and manage external locations and storage credentials\n",
    "- Work with different types of views (standard, temporary, global temporary)\n",
    "- Use CREATE TABLE AS SELECT (CTAS) statements effectively\n",
    "- Implement and understand Delta Lake constraints\n",
    "- Apply data governance best practices with Unity Catalog\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Setup\n",
    "\n",
    "### Step 1: Environment Setup\n",
    "Create a new notebook in your Databricks workspace and run the following setup commands:\n",
    "\n",
    "```python\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Clear any existing configurations\n",
    "spark.sql(\"SET spark.sql.execution.arrow.pyspark.enabled = false\")\n",
    "\n",
    "print(\"Lab environment initialized successfully\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Unity Catalog Setup and Managed vs External Tables\n",
    "\n",
    "### Task 1.1: Create Catalog and Schema Structure\n",
    "```sql\n",
    "-- Create a catalog for this lab\n",
    "CREATE CATALOG IF NOT EXISTS hr_analytics_lab;\n",
    "\n",
    "-- Use the catalog\n",
    "USE CATALOG hr_analytics_lab;\n",
    "\n",
    "-- Create schemas for different business units\n",
    "CREATE SCHEMA IF NOT EXISTS hr_data;\n",
    "CREATE SCHEMA IF NOT EXISTS sales_data;\n",
    "CREATE SCHEMA IF NOT EXISTS analytics;\n",
    "\n",
    "-- Set default schema\n",
    "USE hr_analytics_lab.hr_data;\n",
    "```\n",
    "\n",
    "### Task 1.2: Create Sample Employee Data\n",
    "```python\n",
    "# Generate comprehensive employee dataset\n",
    "def generate_employee_data(num_employees=1000):\n",
    "    cities = [\"New York\", \"San Francisco\", \"Chicago\", \"Austin\", \"Seattle\", \"Boston\", \"Los Angeles\", \"Miami\"]\n",
    "    departments = [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\", \"Operations\"]\n",
    "    job_levels = [\"Junior\", \"Senior\", \"Lead\", \"Manager\", \"Director\"]\n",
    "    \n",
    "    employees = []\n",
    "    for i in range(1, num_employees + 1):\n",
    "        birth_date = (datetime.now() - timedelta(days=random.randint(8000, 15000))).strftime(\"%Y-%m-%d\")\n",
    "        hire_date = (datetime.now() - timedelta(days=random.randint(30, 2000))).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        employee = {\n",
    "            \"employee_id\": i,\n",
    "            \"first_name\": f\"Employee_{i}\",\n",
    "            \"last_name\": f\"LastName_{i}\",\n",
    "            \"email\": f\"employee{i}@company.com\",\n",
    "            \"phone\": f\"555-{random.randint(1000, 9999)}\",\n",
    "            \"hire_date\": hire_date,\n",
    "            \"birth_date\": birth_date,\n",
    "            \"salary\": random.randint(40000, 200000),\n",
    "            \"department\": random.choice(departments),\n",
    "            \"job_title\": f\"{random.choice(job_levels)} {random.choice(departments)} Specialist\",\n",
    "            \"city\": random.choice(cities),\n",
    "            \"is_active\": random.choice([True, True, True, False])  # 75% active\n",
    "        }\n",
    "        employees.append(employee)\n",
    "    \n",
    "    return spark.createDataFrame(employees)\n",
    "\n",
    "# Create the employee dataset\n",
    "employee_df = generate_employee_data(1000)\n",
    "employee_df.show(10)\n",
    "```\n",
    "\n",
    "### Task 1.3: Create Managed Table (Unity Catalog Default)\n",
    "```sql\n",
    "-- Drop table if exists for clean start\n",
    "DROP TABLE IF EXISTS hr_analytics_lab.hr_data.employees_managed;\n",
    "\n",
    "-- This will be created as a managed table automatically\n",
    "CREATE TABLE hr_analytics_lab.hr_data.employees_managed (\n",
    "    employee_id INT NOT NULL,\n",
    "    first_name STRING NOT NULL,\n",
    "    last_name STRING NOT NULL,\n",
    "    email STRING,\n",
    "    phone STRING,\n",
    "    hire_date DATE,\n",
    "    birth_date DATE,\n",
    "    salary DOUBLE,\n",
    "    department STRING,\n",
    "    job_title STRING,\n",
    "    city STRING,\n",
    "    is_active BOOLEAN\n",
    ") \n",
    "USING DELTA\n",
    "COMMENT \"Managed table containing employee information\"\n",
    "TBLPROPERTIES (\n",
    "    'department'='HR',\n",
    "    'data_classification'='PII',\n",
    "    'created_by'='data_engineering_team'\n",
    ");\n",
    "```\n",
    "\n",
    "### Task 1.4: Insert Data into Managed Table\n",
    "```python\n",
    "# Insert data into managed table\n",
    "employee_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"hr_analytics_lab.hr_data.employees_managed\")\n",
    "\n",
    "print(\"Data inserted into managed table\")\n",
    "```\n",
    "\n",
    "### Task 1.5: Examine Managed Table Properties\n",
    "```sql\n",
    "-- Describe the managed table in detail\n",
    "DESCRIBE EXTENDED hr_analytics_lab.hr_data.employees_managed;\n",
    "\n",
    "-- Show table properties\n",
    "SHOW TBLPROPERTIES hr_analytics_lab.hr_data.employees_managed;\n",
    "\n",
    "-- Check the location (notice it's managed by Unity Catalog)\n",
    "DESCRIBE DETAIL hr_analytics_lab.hr_data.employees_managed;\n",
    "```\n",
    "\n",
    "### Questions for Exercise 1:\n",
    "1. Where is the data for the managed table stored?\n",
    "2. What are the benefits of using managed tables?\n",
    "3. Who owns the lifecycle of managed table data?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: External Locations and Storage Credentials\n",
    "\n",
    "### Task 2.1: Understand External Location Concepts\n",
    "```sql\n",
    "-- Show existing external locations (if any)\n",
    "SHOW EXTERNAL LOCATIONS;\n",
    "\n",
    "-- Show storage credentials (if any)\n",
    "SHOW STORAGE CREDENTIALS;\n",
    "```\n",
    "\n",
    "### Task 2.2: Create External Table with Cloud Storage Path\n",
    "```sql\n",
    "-- For demonstration, we'll create an external table pointing to a specific path\n",
    "-- Note: In production, you would need proper external locations and storage credentials\n",
    "\n",
    "-- Create external table structure (this may require admin privileges for external locations)\n",
    "CREATE TABLE IF NOT EXISTS hr_analytics_lab.hr_data.employees_external (\n",
    "    employee_id INT NOT NULL,\n",
    "    first_name STRING NOT NULL,\n",
    "    last_name STRING NOT NULL,\n",
    "    email STRING,\n",
    "    phone STRING,\n",
    "    hire_date DATE,\n",
    "    birth_date DATE,\n",
    "    salary DOUBLE,\n",
    "    department STRING,\n",
    "    job_title STRING,\n",
    "    city STRING,\n",
    "    is_active BOOLEAN\n",
    ") \n",
    "USING DELTA\n",
    "COMMENT \"External table containing employee information\"\n",
    "LOCATION 's3://your-bucket/hr-data/employees/'  -- Replace with actual external location\n",
    "TBLPROPERTIES (\n",
    "    'department'='HR',\n",
    "    'data_classification'='PII',\n",
    "    'storage_type'='external'\n",
    ");\n",
    "```\n",
    "\n",
    "### Task 2.3: Benefits Analysis - External vs Managed Tables\n",
    "```sql\n",
    "-- Create a comparison query\n",
    "WITH table_comparison AS (\n",
    "    SELECT \n",
    "        'managed' as table_type,\n",
    "        COUNT(*) as record_count,\n",
    "        'Unity Catalog managed storage' as location_type\n",
    "    FROM hr_analytics_lab.hr_data.employees_managed\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'external' as table_type,\n",
    "        COUNT(*) as record_count,\n",
    "        'External cloud storage' as location_type\n",
    "    FROM hr_analytics_lab.hr_data.employees_managed  -- Using managed for demo since external might not be accessible\n",
    ")\n",
    "SELECT * FROM table_comparison;\n",
    "```\n",
    "\n",
    "### Questions for Exercise 2:\n",
    "1. What are the main benefits of external locations in Unity Catalog?\n",
    "2. When would you choose external tables over managed tables?\n",
    "3. What security advantages do storage credentials provide?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Working with Different Types of Views\n",
    "\n",
    "### Task 3.1: Create Standard (Stored) Views\n",
    "```sql\n",
    "-- Create a standard view for active employees\n",
    "CREATE OR REPLACE VIEW hr_analytics_lab.analytics.active_employees_view\n",
    "AS\n",
    "SELECT \n",
    "    employee_id,\n",
    "    CONCAT(first_name, ' ', last_name) as full_name,\n",
    "    email,\n",
    "    department,\n",
    "    job_title,\n",
    "    city,\n",
    "    salary,\n",
    "    hire_date,\n",
    "    DATEDIFF(CURRENT_DATE(), hire_date) as days_employed\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE is_active = true;\n",
    "\n",
    "-- Create a view for department statistics\n",
    "CREATE OR REPLACE VIEW hr_analytics_lab.analytics.department_stats_view\n",
    "AS\n",
    "SELECT \n",
    "    department,\n",
    "    COUNT(*) as employee_count,\n",
    "    AVG(salary) as avg_salary,\n",
    "    MIN(salary) as min_salary,\n",
    "    MAX(salary) as max_salary,\n",
    "    AVG(DATEDIFF(CURRENT_DATE(), hire_date)) as avg_tenure_days\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE is_active = true\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC;\n",
    "\n",
    "-- Query the views\n",
    "SELECT * FROM hr_analytics_lab.analytics.active_employees_view LIMIT 10;\n",
    "SELECT * FROM hr_analytics_lab.analytics.department_stats_view;\n",
    "```\n",
    "\n",
    "### Task 3.2: Create Temporary Views\n",
    "```sql\n",
    "-- Create a temporary view (session-scoped)\n",
    "CREATE OR REPLACE TEMPORARY VIEW temp_salary_analysis\n",
    "AS\n",
    "SELECT \n",
    "    department,\n",
    "    city,\n",
    "    COUNT(*) as employee_count,\n",
    "    PERCENTILE_APPROX(salary, 0.5) as median_salary,\n",
    "    PERCENTILE_APPROX(salary, 0.9) as p90_salary\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE is_active = true\n",
    "GROUP BY department, city;\n",
    "\n",
    "-- Query temporary view\n",
    "SELECT * FROM temp_salary_analysis WHERE employee_count >= 5;\n",
    "```\n",
    "\n",
    "### Task 3.3: Create Global Temporary Views\n",
    "```sql\n",
    "-- Create a global temporary view (accessible across sessions)\n",
    "CREATE OR REPLACE GLOBAL TEMPORARY VIEW global_temp.employee_performance_metrics\n",
    "AS\n",
    "SELECT \n",
    "    employee_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    department,\n",
    "    salary,\n",
    "    CASE \n",
    "        WHEN salary >= 150000 THEN 'High Performer'\n",
    "        WHEN salary >= 100000 THEN 'Mid Performer'\n",
    "        ELSE 'Standard Performer'\n",
    "    END as performance_tier,\n",
    "    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_salary_rank\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE is_active = true;\n",
    "\n",
    "-- Query global temporary view\n",
    "SELECT * FROM global_temp.employee_performance_metrics \n",
    "WHERE performance_tier = 'High Performer' \n",
    "LIMIT 20;\n",
    "```\n",
    "\n",
    "### Task 3.4: View Metadata and Management\n",
    "```sql\n",
    "-- Show all views in the analytics schema\n",
    "SHOW VIEWS IN hr_analytics_lab.analytics;\n",
    "\n",
    "-- Describe view definition\n",
    "DESCRIBE EXTENDED hr_analytics_lab.analytics.active_employees_view;\n",
    "\n",
    "-- Show view dependencies\n",
    "SHOW TABLES IN hr_analytics_lab.analytics;\n",
    "```\n",
    "\n",
    "### Questions for Exercise 3:\n",
    "1. What is the difference between temporary and global temporary views?\n",
    "2. When would you use a stored view vs a temporary view?\n",
    "3. How do views contribute to data governance in Unity Catalog?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: CREATE TABLE AS SELECT (CTAS) Operations\n",
    "\n",
    "### Task 4.1: Basic CTAS with Filtering\n",
    "```sql\n",
    "-- Create a table for high-salary employees using CTAS\n",
    "CREATE OR REPLACE TABLE hr_analytics_lab.analytics.high_salary_employees\n",
    "COMMENT \"Employees with salary above 120K\"\n",
    "AS\n",
    "SELECT \n",
    "    employee_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    department,\n",
    "    job_title,\n",
    "    salary,\n",
    "    city,\n",
    "    hire_date\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE salary > 120000 \n",
    "  AND is_active = true;\n",
    "\n",
    "-- Verify the created table\n",
    "SELECT COUNT(*) as high_earners FROM hr_analytics_lab.analytics.high_salary_employees;\n",
    "SELECT * FROM hr_analytics_lab.analytics.high_salary_employees LIMIT 10;\n",
    "```\n",
    "\n",
    "### Task 4.2: CTAS with Column Renaming and Transformation\n",
    "```sql\n",
    "-- Create a marketing-friendly employee contact table\n",
    "CREATE OR REPLACE TABLE hr_analytics_lab.sales_data.employee_contacts\n",
    "COMMENT \"Employee contact information for internal directory\"\n",
    "AS\n",
    "SELECT \n",
    "    employee_id as emp_id,\n",
    "    CONCAT(first_name, ' ', last_name) as full_name,\n",
    "    UPPER(email) as email_address,\n",
    "    phone as contact_number,\n",
    "    department as dept_name,\n",
    "    CASE \n",
    "        WHEN job_title LIKE '%Manager%' OR job_title LIKE '%Director%' THEN 'Leadership'\n",
    "        WHEN job_title LIKE '%Senior%' OR job_title LIKE '%Lead%' THEN 'Senior Level'\n",
    "        ELSE 'Individual Contributor'\n",
    "    END as employee_level,\n",
    "    city as office_location\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE is_active = true;\n",
    "\n",
    "-- Check the results\n",
    "SELECT employee_level, COUNT(*) as count \n",
    "FROM hr_analytics_lab.sales_data.employee_contacts \n",
    "GROUP BY employee_level;\n",
    "```\n",
    "\n",
    "### Task 4.3: Advanced CTAS with Partitioning and Custom Location\n",
    "```sql\n",
    "-- Create a partitioned table with CTAS (demonstrates the example from your curriculum)\n",
    "CREATE OR REPLACE TABLE hr_analytics_lab.analytics.employees_by_location\n",
    "COMMENT \"Contains PII - Employee data partitioned by city and birth year\"\n",
    "PARTITIONED BY (city, birth_year)\n",
    "AS\n",
    "SELECT \n",
    "    employee_id as id,\n",
    "    CONCAT(first_name, ' ', last_name) as name,\n",
    "    email,\n",
    "    birth_date,\n",
    "    city,\n",
    "    YEAR(birth_date) as birth_year,\n",
    "    department,\n",
    "    salary\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE is_active = true;\n",
    "\n",
    "-- Verify partitioning\n",
    "SHOW PARTITIONS hr_analytics_lab.analytics.employees_by_location;\n",
    "\n",
    "-- Query specific partition\n",
    "SELECT COUNT(*) \n",
    "FROM hr_analytics_lab.analytics.employees_by_location \n",
    "WHERE city = 'New York' AND birth_year = 1990;\n",
    "```\n",
    "\n",
    "### Task 4.4: CTAS with Aggregations and Window Functions\n",
    "```sql\n",
    "-- Create department summary table using CTAS\n",
    "CREATE OR REPLACE TABLE hr_analytics_lab.analytics.department_analytics\n",
    "COMMENT \"Comprehensive department-level analytics\"\n",
    "AS\n",
    "SELECT \n",
    "    department,\n",
    "    COUNT(*) as total_employees,\n",
    "    AVG(salary) as avg_salary,\n",
    "    MEDIAN(salary) as median_salary,\n",
    "    MIN(salary) as min_salary,\n",
    "    MAX(salary) as max_salary,\n",
    "    STDDEV(salary) as salary_stddev,\n",
    "    COUNT(CASE WHEN salary > 100000 THEN 1 END) as high_earners,\n",
    "    AVG(DATEDIFF(CURRENT_DATE(), hire_date)) as avg_tenure_days,\n",
    "    COUNT(DISTINCT city) as cities_represented,\n",
    "    CURRENT_TIMESTAMP() as analysis_timestamp\n",
    "FROM hr_analytics_lab.hr_data.employees_managed\n",
    "WHERE is_active = true\n",
    "GROUP BY department;\n",
    "\n",
    "-- Query the analytics table\n",
    "SELECT * FROM hr_analytics_lab.analytics.department_analytics\n",
    "ORDER BY avg_salary DESC;\n",
    "```\n",
    "\n",
    "### Questions for Exercise 4:\n",
    "1. How does CTAS differ from regular CREATE TABLE followed by INSERT?\n",
    "2. What are the benefits of partitioning in the CTAS example?\n",
    "3. When would you use CTAS vs creating views?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 5: Delta Lake Constraints Implementation\n",
    "\n",
    "### Task 5.1: Understanding Constraint Enforcement Types\n",
    "```python\n",
    "# First, let's understand the enforcement concepts with a demonstration\n",
    "print(\"=== Delta Lake Constraint Enforcement ===\")\n",
    "print(\"WHO: Delta Engine performs the checks\")\n",
    "print(\"WHEN: At write time (when data is written to table)\")\n",
    "print(\"\\nConstraint Types:\")\n",
    "print(\"- NOT NULL: Enforced by Delta Engine at write time\")\n",
    "print(\"- CHECK: Enforced by Delta Engine at write time\") \n",
    "print(\"- PRIMARY KEY: Metadata-only (Unity Catalog)\")\n",
    "print(\"- FOREIGN KEY: Metadata-only (Unity Catalog)\")\n",
    "print(\"- UNIQUE: Metadata-only (Unity Catalog)\")\n",
    "print(\"- DEFAULT: Limited SQL support only\")\n",
    "```\n",
    "\n",
    "### Task 5.2: Create Table with NOT NULL Constraints\n",
    "```sql\n",
    "-- Create a table with NOT NULL constraints\n",
    "DROP TABLE IF EXISTS hr_analytics_lab.hr_data.employees_with_constraints;\n",
    "\n",
    "CREATE TABLE hr_analytics_lab.hr_data.employees_with_constraints (\n",
    "    employee_id INT NOT NULL,\n",
    "    first_name STRING NOT NULL,\n",
    "    last_name STRING NOT NULL,\n",
    "    email STRING NOT NULL,\n",
    "    department STRING NOT NULL,\n",
    "    salary DOUBLE NOT NULL,\n",
    "    hire_date DATE NOT NULL,\n",
    "    city STRING,\n",
    "    is_active BOOLEAN NOT NULL DEFAULT true\n",
    ") \n",
    "USING DELTA\n",
    "COMMENT \"Employee table with NOT NULL constraints\";\n",
    "```\n",
    "\n",
    "### Task 5.3: Test NOT NULL Constraint Enforcement\n",
    "```python\n",
    "# Test NOT NULL constraint - this should succeed\n",
    "valid_data = [(1, \"John\", \"Doe\", \"john.doe@company.com\", \"Engineering\", 75000.0, \"2023-01-15\", \"New York\", True)]\n",
    "valid_df = spark.createDataFrame(valid_data, \n",
    "    [\"employee_id\", \"first_name\", \"last_name\", \"email\", \"department\", \"salary\", \"hire_date\", \"city\", \"is_active\"])\n",
    "\n",
    "# Insert valid data\n",
    "valid_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"hr_analytics_lab.hr_data.employees_with_constraints\")\n",
    "print(\"Valid data inserted successfully\")\n",
    "\n",
    "# Test NOT NULL constraint violation - this should fail\n",
    "try:\n",
    "    invalid_data = [(None, \"Jane\", \"Smith\", \"jane.smith@company.com\", \"Sales\", 65000.0, \"2023-02-01\", \"Chicago\", True)]\n",
    "    invalid_df = spark.createDataFrame(invalid_data, \n",
    "        [\"employee_id\", \"first_name\", \"last_name\", \"email\", \"department\", \"salary\", \"hire_date\", \"city\", \"is_active\"])\n",
    "    \n",
    "    invalid_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"hr_analytics_lab.hr_data.employees_with_constraints\")\n",
    "    print(\"ERROR: Invalid data was inserted (this shouldn't happen)\")\n",
    "except Exception as e:\n",
    "    print(f\"NOT NULL constraint enforced successfully: {str(e)}\")\n",
    "```\n",
    "\n",
    "### Task 5.4: Implement CHECK Constraints\n",
    "```sql\n",
    "-- Add CHECK constraints to existing table\n",
    "ALTER TABLE hr_analytics_lab.hr_data.employees_with_constraints \n",
    "ADD CONSTRAINT salary_positive CHECK (salary > 0);\n",
    "\n",
    "ALTER TABLE hr_analytics_lab.hr_data.employees_with_constraints \n",
    "ADD CONSTRAINT valid_email CHECK (email LIKE '%@%.%');\n",
    "\n",
    "ALTER TABLE hr_analytics_lab.hr_data.employees_with_constraints \n",
    "ADD CONSTRAINT reasonable_salary CHECK (salary >= 30000 AND salary <= 500000);\n",
    "\n",
    "-- Show table constraints\n",
    "DESCRIBE EXTENDED hr_analytics_lab.hr_data.employees_with_constraints;\n",
    "```\n",
    "\n",
    "### Task 5.5: Test CHECK Constraint Enforcement\n",
    "```python\n",
    "# Test CHECK constraint enforcement\n",
    "\n",
    "# Test 1: Negative salary (should fail)\n",
    "try:\n",
    "    negative_salary_data = [(2, \"Bob\", \"Wilson\", \"bob.wilson@company.com\", \"Marketing\", -50000.0, \"2023-03-01\", \"Austin\", True)]\n",
    "    negative_salary_df = spark.createDataFrame(negative_salary_data, \n",
    "        [\"employee_id\", \"first_name\", \"last_name\", \"email\", \"department\", \"salary\", \"hire_date\", \"city\", \"is_active\"])\n",
    "    \n",
    "    negative_salary_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"hr_analytics_lab.hr_data.employees_with_constraints\")\n",
    "    print(\"ERROR: Negative salary was accepted\")\n",
    "except Exception as e:\n",
    "    print(f\"CHECK constraint (salary_positive) enforced: {str(e)[:100]}...\")\n",
    "\n",
    "# Test 2: Invalid email format (should fail)\n",
    "try:\n",
    "    invalid_email_data = [(3, \"Alice\", \"Johnson\", \"invalid-email\", \"HR\", 80000.0, \"2023-04-01\", \"Seattle\", True)]\n",
    "    invalid_email_df = spark.createDataFrame(invalid_email_data, \n",
    "        [\"employee_id\", \"first_name\", \"last_name\", \"email\", \"department\", \"salary\", \"hire_date\", \"city\", \"is_active\"])\n",
    "    \n",
    "    invalid_email_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"hr_analytics_lab.hr_data.employees_with_constraints\")\n",
    "    print(\"ERROR: Invalid email was accepted\")\n",
    "except Exception as e:\n",
    "    print(f\"CHECK constraint (valid_email) enforced: {str(e)[:100]}...\")\n",
    "\n",
    "# Test 3: Valid data (should succeed)\n",
    "try:\n",
    "    valid_data2 = [(4, \"Sarah\", \"Brown\", \"sarah.brown@company.com\", \"Finance\", 95000.0, \"2023-05-01\", \"Miami\", True)]\n",
    "    valid_df2 = spark.createDataFrame(valid_data2, \n",
    "        [\"employee_id\", \"first_name\", \"last_name\", \"email\", \"department\", \"salary\", \"hire_date\", \"city\", \"is_active\"])\n",
    "    \n",
    "    valid_df2.write.format(\"delta\").mode(\"append\").saveAsTable(\"hr_analytics_lab.hr_data.employees_with_constraints\")\n",
    "    print(\"Valid data inserted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {str(e)}\")\n",
    "```\n",
    "\n",
    "### Task 5.6: Metadata-Only Constraints (Unity Catalog)\n",
    "```sql\n",
    "-- Add metadata-only constraints (these are for documentation and Unity Catalog governance)\n",
    "ALTER TABLE hr_analytics_lab.hr_data.employees_with_constraints \n",
    "ADD CONSTRAINT pk_employee_id PRIMARY KEY (employee_id);\n",
    "\n",
    "-- Note: UNIQUE and FOREIGN KEY constraints would be added similarly\n",
    "-- These are metadata-only and provide governance information but are not enforced at write time\n",
    "\n",
    "-- Show all constraints\n",
    "SHOW TBLPROPERTIES hr_analytics_lab.hr_data.employees_with_constraints;\n",
    "```\n",
    "\n",
    "### Task 5.7: DEFAULT Constraint Example\n",
    "```sql\n",
    "-- Create a new table demonstrating DEFAULT constraints\n",
    "CREATE OR REPLACE TABLE hr_analytics_lab.hr_data.employee_status (\n",
    "    employee_id INT NOT NULL,\n",
    "    status_date DATE NOT NULL,\n",
    "    employment_status STRING NOT NULL DEFAULT 'Active',\n",
    "    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
    "    updated_by STRING DEFAULT 'system'\n",
    ") \n",
    "USING DELTA\n",
    "COMMENT \"Employee status tracking with DEFAULT constraints\";\n",
    "\n",
    "-- Insert data without specifying default columns\n",
    "INSERT INTO hr_analytics_lab.hr_data.employee_status (employee_id, status_date)\n",
    "VALUES (1, '2023-01-15'), (2, '2023-02-01');\n",
    "\n",
    "-- Check the results\n",
    "SELECT * FROM hr_analytics_lab.hr_data.employee_status;\n",
    "```\n",
    "\n",
    "### Questions for Exercise 5:\n",
    "1. What is the difference between enforced constraints and metadata-only constraints?\n",
    "2. Why are CHECK constraints enforced at write time by the Delta Engine?\n",
    "3. How do constraints contribute to data quality in a data lake?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 6: Advanced Constraint Scenarios\n",
    "\n",
    "### Task 6.1: Complex CHECK Constraints\n",
    "```sql\n",
    "-- Create a table with complex business rule constraints\n",
    "CREATE OR REPLACE TABLE hr_analytics_lab.hr_data.employee_compensation (\n",
    "    employee_id INT NOT NULL,\n",
    "    base_salary DOUBLE NOT NULL,\n",
    "    bonus_percentage DOUBLE,\n",
    "    commission_rate DOUBLE,\n",
    "    total_compensation DOUBLE,\n",
    "    effective_date DATE NOT NULL,\n",
    "    CONSTRAINT positive_base_salary CHECK (base_salary > 0),\n",
    "    CONSTRAINT reasonable_bonus CHECK (bonus_percentage >= 0 AND bonus_percentage <= 100),\n",
    "    CONSTRAINT reasonable_commission CHECK (commission_rate >= 0 AND commission_rate <= 50),\n",
    "    CONSTRAINT logical_compensation CHECK (total_compensation >= base_salary),\n",
    "    CONSTRAINT future_effective_date CHECK (effective_date >= '2020-01-01')\n",
    ") \n",
    "USING DELTA\n",
    "COMMENT \"Employee compensation with complex business rules\";\n",
    "```\n",
    "\n",
    "### Task 6.2: Test Complex Constraints\n",
    "```python\n",
    "# Test complex constraint scenarios\n",
    "test_cases = [\n",
    "    # Valid case\n",
    "    (1, 80000.0, 10.0, 2.5, 90000.0, \"2023-01-01\", \"Should succeed\"),\n",
    "    # Invalid: total_compensation less than base_salary\n",
    "    (2, 90000.0, 5.0, 1.0, 80000.0, \"2023-01-01\", \"Should fail - total < base\"),\n",
    "    # Invalid: bonus_percentage over 100\n",
    "    (3, 70000.0, 150.0, 2.0, 175000.0, \"2023-01-01\", \"Should fail - bonus > 100%\"),\n",
    "    # Invalid: future effective date before 2020\n",
    "    (4, 60000.0, 8.0, 1.5, 65000.0, \"2019-12-31\", \"Should fail - date too old\")\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    try:\n",
    "        test_data = [case[:6]]  # First 6 elements are the data\n",
    "        test_df = spark.createDataFrame(test_data, \n",
    "            [\"employee_id\", \"base_salary\", \"bonus_percentage\", \"commission_rate\", \"total_compensation\", \"effective_date\"])\n",
    "        \n",
    "        test_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"hr_analytics_lab.hr_data.employee_compensation\")\n",
    "        print(f\"✅ Case {case[0]}: {case[6]} - SUCCESS\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Case {case[0]}: {case[6]} - FAILED: {str(e)[:80]}...\")\n",
    "```\n",
    "\n",
    "### Questions for Exercise 6:\n",
    "1. How do complex CHECK constraints help enforce business rules?\n",
    "2. What happens when multiple constraints are violated simultaneously?\n",
    "3. How would you design constraints for a real-world scenario?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 7: Data Governance and Best Practices\n",
    "\n",
    "### Task 7.1: Table Documentation and Metadata\n",
    "```sql\n",
    "-- Add comprehensive metadata to tables\n",
    "ALTER TABLE hr_analytics_lab.hr_data.employees_managed \n",
    "SET TBLPROPERTIES (\n",
    "    'data_steward' = 'HR Data Team',\n",
    "    'data_classification' = 'PII',\n",
    "    'retention_policy' = '7_years',\n",
    "    'last_quality_check' = '2023-10-01',\n",
    "    'quality_score' = '95'\n",
    ");\n",
    "\n",
    "-- Add column comments\n",
    "ALTER TABLE hr_analytics_lab.hr_data.employees_managed \n",
    "ALTER COLUMN salary COMMENT \"Annual salary in USD\";\n",
    "\n",
    "ALTER TABLE hr_analytics_lab.hr_data.employees_managed \n",
    "ALTER COLUMN email COMMENT \"Primary business email address\";\n",
    "```\n",
    "\n",
    "### Task 7.2: Create Governance Dashboard\n",
    "```sql\n",
    "-- Create a view for data governance monitoring\n",
    "CREATE OR REPLACE VIEW hr_analytics_lab.analytics.data_governance_dashboard\n",
    "AS\n",
    "SELECT \n",
    "    'hr_analytics_lab.hr_data.employees_managed' as table_name,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(CASE WHEN email IS NULL THEN 1 END) as missing_emails,\n",
    "    COUNT(CASE WHEN salary < 30000 OR salary > 500000 THEN 1 END) as salary_outliers,\n",
    "    COUNT(CASE WHEN hire_date > CURRENT_DATE() THEN 1 END) as future_hire_dates,\n",
    "    ROUND((COUNT(*) - COUNT(CASE WHEN email IS NULL THEN 1 END)) * 100.0 / COUNT(*), 2) as email_completeness_pct,\n",
    "    CURRENT_TIMESTAMP() as last_checked\n",
    "FROM hr_analytics_lab.hr_data.employees_managed;\n",
    "\n",
    "-- Query the governance dashboard\n",
    "SELECT * FROM hr_analytics_lab.analytics.data_governance_dashboard;\n",
    "```\n",
    "\n",
    "### Task 7.3: Cleanup and Security\n",
    "```sql\n",
    "-- Show all objects created in this lab\n",
    "SHOW TABLES IN hr_analytics_lab.hr_data;\n",
    "SHOW TABLES IN hr_analytics_lab.analytics;\n",
    "SHOW VIEWS IN hr_analytics_lab.analytics;\n",
    "\n",
    "-- Example of dropping sensitive views (if needed)\n",
    "-- DROP VIEW IF EXISTS hr_analytics_lab.analytics.active_employees_view;\n",
    "```\n",
    "\n",
    "### Questions for Exercise 7:\n",
    "1. How do table properties contribute to data governance?\n",
    "2. What metadata should you track for production tables?\n",
    "3. How do Unity Catalog constraints support compliance requirements?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Review\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Managed vs External Tables**: Understanding Unity Catalog storage models\n",
    "2. **External Locations & Storage Credentials**: Security and governance for external data\n",
    "3. **Views Types**: Standard, temporary, and global temporary views\n",
    "4. **CTAS Operations**: Creating tables with filtering, renaming, and partitioning\n",
    "5. **Delta Constraints**: NOT NULL, CHECK (enforced), PRIMARY KEY, FOREIGN KEY, UNIQUE (metadata-only)\n",
    "6. **Data Governance**: Metadata management and quality monitoring\n",
    "\n",
    "### Lab Cleanup (Optional)\n",
    "```sql\n",
    "-- Clean up lab resources if needed\n",
    "-- DROP CATALOG hr_analytics_lab CASCADE;\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "- Practice creating external locations with proper storage credentials\n",
    "- Experiment with more complex CHECK constraints\n",
    "- Explore Unity Catalog security features (grants, row-level security)\n",
    "- Study partition strategies for large datasets\n",
    "- Learn about Delta Lake advanced features (CDC, liquid clustering)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.6-Exercise-UnityCatalog_and_TableManagement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
