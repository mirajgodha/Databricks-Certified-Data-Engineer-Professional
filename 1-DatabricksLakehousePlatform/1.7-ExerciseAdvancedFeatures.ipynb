{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "632ddaf0-48d3-4182-877a-e6b708bba262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Delta Lake Professional Lab Exercise\n",
    "\n",
    "## Overview\n",
    "This hands-on lab will guide you through essential Delta Lake concepts including Delta Lake fundamentals, time travel, delta logs, data files, compaction, vacuum, optimize, and Z-ordering. You'll work with a realistic e-commerce dataset to understand how these features work in practice.\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks workspace access\n",
    "- Basic knowledge of SQL and Spark\n",
    "- Running compute cluster\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Create and manage Delta Lake tables\n",
    "- Understand Delta Lake architecture (data files and transaction logs)\n",
    "- Use time travel to query historical data\n",
    "- Perform table optimization using OPTIMIZE and Z-ORDER\n",
    "- Clean up old files using VACUUM\n",
    "- Monitor table performance and file structure\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Setup\n",
    "\n",
    "### Step 1: Environment Setup\n",
    "Create a new notebook in your Databricks workspace and run the following setup commands:\n",
    "\n",
    "```python\n",
    "# Set up the database and location for this lab\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS delta_lab\")\n",
    "spark.sql(\"USE delta_lab\")\n",
    "\n",
    "# Define the base path for our tables\n",
    "base_path = \"/tmp/delta_lab/\"\n",
    "dbutils.fs.rm(base_path, True)  # Clean up if exists\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Creating Your First Delta Lake Table\n",
    "\n",
    "### Task 1.1: Create Sample E-commerce Data\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample e-commerce data\n",
    "def generate_ecommerce_data(num_records=10000):\n",
    "    categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home & Garden\", \"Sports\"]\n",
    "    regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_records):\n",
    "        record = {\n",
    "            \"order_id\": f\"ORD{i:06d}\",\n",
    "            \"customer_id\": f\"CUST{random.randint(1, 1000):04d}\",\n",
    "            \"product_category\": random.choice(categories),\n",
    "            \"region\": random.choice(regions),\n",
    "            \"order_amount\": round(random.uniform(10, 1000), 2),\n",
    "            \"quantity\": random.randint(1, 10),\n",
    "            \"order_date\": (datetime.now() - timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
    "            \"status\": random.choice([\"Completed\", \"Pending\", \"Cancelled\"])\n",
    "        }\n",
    "        data.append(record)\n",
    "    \n",
    "    return spark.createDataFrame(data)\n",
    "\n",
    "# Generate initial dataset\n",
    "df_orders = generate_ecommerce_data(10000)\n",
    "df_orders.show(10)\n",
    "```\n",
    "\n",
    "### Task 1.2: Create Delta Lake Table\n",
    "```python\n",
    "# Write data as Delta Lake table\n",
    "delta_table_path = f\"{base_path}orders_delta\"\n",
    "\n",
    "df_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", delta_table_path) \\\n",
    "    .saveAsTable(\"delta_lab.orders\")\n",
    "\n",
    "print(f\"Created Delta table at: {delta_table_path}\")\n",
    "```\n",
    "\n",
    "### Questions for Task 1:\n",
    "1. What files were created in the Delta Lake table directory?\n",
    "2. How does a Delta Lake table differ from a regular Parquet table?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Understanding Delta Lake Architecture\n",
    "\n",
    "### Task 2.1: Explore Data Files and Delta Logs\n",
    "```python\n",
    "# List files in the Delta table directory\n",
    "display(dbutils.fs.ls(delta_table_path))\n",
    "\n",
    "# Look at the _delta_log directory\n",
    "display(dbutils.fs.ls(f\"{delta_table_path}/_delta_log/\"))\n",
    "\n",
    "# Read the first commit log\n",
    "first_commit = spark.read.json(f\"{delta_table_path}/_delta_log/00000000000000000000.json\")\n",
    "display(first_commit)\n",
    "```\n",
    "\n",
    "### Task 2.2: Analyze Table Structure\n",
    "```python\n",
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE EXTENDED delta_lab.orders\").show(50, False)\n",
    "\n",
    "# Show table history\n",
    "spark.sql(\"DESCRIBE HISTORY delta_lab.orders\").show(10, False)\n",
    "```\n",
    "\n",
    "### Questions for Task 2:\n",
    "1. What information is stored in the Delta log files?\n",
    "2. How many data files were created initially?\n",
    "3. What does the commitInfo tell us about the transaction?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Time Travel and Versioning\n",
    "\n",
    "### Task 3.1: Make Changes to Create Versions\n",
    "```python\n",
    "# Version 1: Add new orders (INSERT)\n",
    "new_orders = generate_ecommerce_data(2000)\n",
    "new_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"delta_lab.orders\")\n",
    "\n",
    "print(\"Added 2000 new records - Version 1\")\n",
    "\n",
    "# Version 2: Update order status (UPDATE)\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE delta_lab.orders \n",
    "    SET status = 'Shipped' \n",
    "    WHERE status = 'Pending' AND region = 'North'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Updated order status - Version 2\")\n",
    "\n",
    "# Version 3: Delete cancelled orders (DELETE)\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM delta_lab.orders \n",
    "    WHERE status = 'Cancelled'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Deleted cancelled orders - Version 3\")\n",
    "```\n",
    "\n",
    "### Task 3.2: Time Travel Queries\n",
    "```python\n",
    "# Show table history\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY delta_lab.orders\")\n",
    "display(history_df)\n",
    "\n",
    "# Time travel by version\n",
    "print(\"=== Current Version ===\")\n",
    "current_count = spark.sql(\"SELECT COUNT(*) as count FROM delta_lab.orders\").collect()[0][0]\n",
    "print(f\"Current record count: {current_count}\")\n",
    "\n",
    "print(\"\\n=== Version 0 (Original) ===\")\n",
    "v0_count = spark.sql(\"SELECT COUNT(*) as count FROM delta_lab.orders VERSION AS OF 0\").collect()[0][0]\n",
    "print(f\"Version 0 record count: {v0_count}\")\n",
    "\n",
    "print(\"\\n=== Version 1 (After Insert) ===\")\n",
    "v1_count = spark.sql(\"SELECT COUNT(*) as count FROM delta_lab.orders VERSION AS OF 1\").collect()[0][0]\n",
    "print(f\"Version 1 record count: {v1_count}\")\n",
    "\n",
    "# Time travel by timestamp\n",
    "print(\"\\n=== Time Travel by Timestamp ===\")\n",
    "# Get timestamp from version 1\n",
    "timestamp_v1 = history_df.filter(col(\"version\") == 1).collect()[0][\"timestamp\"]\n",
    "timestamp_count = spark.sql(f\"SELECT COUNT(*) as count FROM delta_lab.orders TIMESTAMP AS OF '{timestamp_v1}'\").collect()[0][0]\n",
    "print(f\"Record count at {timestamp_v1}: {timestamp_count}\")\n",
    "```\n",
    "\n",
    "### Questions for Task 3:\n",
    "1. How many versions of the table exist now?\n",
    "2. What operations triggered new versions?\n",
    "3. How do version-based and timestamp-based time travel differ?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: File Management and Optimization\n",
    "\n",
    "### Task 4.1: Analyze Current File Structure\n",
    "```python\n",
    "# Check table details to see file statistics\n",
    "spark.sql(\"DESCRIBE DETAIL delta_lab.orders\").show(1, False)\n",
    "\n",
    "# Count data files\n",
    "data_files = dbutils.fs.ls(delta_table_path)\n",
    "parquet_files = [f for f in data_files if f.name.endswith('.parquet')]\n",
    "print(f\"Number of data files: {len(parquet_files)}\")\n",
    "\n",
    "# Show file sizes\n",
    "for file in parquet_files[:10]:  # Show first 10 files\n",
    "    print(f\"File: {file.name}, Size: {file.size} bytes\")\n",
    "```\n",
    "\n",
    "### Task 4.2: Create Small Files (Simulating Real-world Scenario)\n",
    "```python\n",
    "# Create many small inserts to simulate small file problem\n",
    "for i in range(20):\n",
    "    small_batch = generate_ecommerce_data(100)\n",
    "    small_batch.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"delta_lab.orders\")\n",
    "\n",
    "print(\"Created many small files through multiple small inserts\")\n",
    "\n",
    "# Check file count again\n",
    "data_files = dbutils.fs.ls(delta_table_path)\n",
    "parquet_files = [f for f in data_files if f.name.endswith('.parquet')]\n",
    "print(f\"Number of data files after small inserts: {len(parquet_files)}\")\n",
    "```\n",
    "\n",
    "### Questions for Task 4:\n",
    "1. Why do we have so many small files?\n",
    "2. What problems do small files cause?\n",
    "3. How many transaction log files do we have now?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 5: Table Optimization with OPTIMIZE\n",
    "\n",
    "### Task 5.1: Optimize Without Z-Order\n",
    "```python\n",
    "# Check performance before optimization\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = spark.sql(\"SELECT region, COUNT(*) FROM delta_lab.orders GROUP BY region\").collect()\n",
    "query_time_before = time.time() - start_time\n",
    "print(f\"Query time before optimization: {query_time_before:.2f} seconds\")\n",
    "\n",
    "# Run OPTIMIZE\n",
    "print(\"Running OPTIMIZE...\")\n",
    "spark.sql(\"OPTIMIZE delta_lab.orders\").show(1, False)\n",
    "\n",
    "# Check file count after optimization\n",
    "data_files_after = dbutils.fs.ls(delta_table_path)\n",
    "parquet_files_after = [f for f in data_files_after if f.name.endswith('.parquet')]\n",
    "print(f\"Number of data files after OPTIMIZE: {len(parquet_files_after)}\")\n",
    "\n",
    "# Check performance after optimization\n",
    "start_time = time.time()\n",
    "result = spark.sql(\"SELECT region, COUNT(*) FROM delta_lab.orders GROUP BY region\").collect()\n",
    "query_time_after = time.time() - start_time\n",
    "print(f\"Query time after optimization: {query_time_after:.2f} seconds\")\n",
    "```\n",
    "\n",
    "### Task 5.2: Optimize with Z-Order\n",
    "```python\n",
    "# Add more data with specific patterns for Z-ORDER demonstration\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO delta_lab.orders\n",
    "    SELECT \n",
    "        concat('ORD', cast(rand() * 1000000 as int)) as order_id,\n",
    "        concat('CUST', cast(rand() * 1000 as int)) as customer_id,\n",
    "        case when rand() < 0.3 then 'Electronics'\n",
    "             when rand() < 0.6 then 'Clothing'\n",
    "             else 'Books' end as product_category,\n",
    "        case when rand() < 0.2 then 'North'\n",
    "             when rand() < 0.4 then 'South' \n",
    "             when rand() < 0.6 then 'East'\n",
    "             when rand() < 0.8 then 'West'\n",
    "             else 'Central' end as region,\n",
    "        rand() * 1000 as order_amount,\n",
    "        cast(rand() * 10 as int) + 1 as quantity,\n",
    "        date_sub(current_date(), cast(rand() * 365 as int)) as order_date,\n",
    "        'Completed' as status\n",
    "    FROM range(5000)\n",
    "\"\"\")\n",
    "\n",
    "# Optimize with Z-ORDER on frequently queried columns\n",
    "print(\"Running OPTIMIZE with Z-ORDER...\")\n",
    "spark.sql(\"OPTIMIZE delta_lab.orders ZORDER BY (region, product_category)\").show(1, False)\n",
    "\n",
    "# Test query performance on Z-ordered columns\n",
    "start_time = time.time()\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * FROM delta_lab.orders \n",
    "    WHERE region = 'North' AND product_category = 'Electronics'\n",
    "    LIMIT 100\n",
    "\"\"\").collect()\n",
    "zorder_query_time = time.time() - start_time\n",
    "print(f\"Z-ordered query time: {zorder_query_time:.2f} seconds\")\n",
    "```\n",
    "\n",
    "### Questions for Task 5:\n",
    "1. How did OPTIMIZE affect the number of data files?\n",
    "2. What is the difference between OPTIMIZE and OPTIMIZE ZORDER BY?\n",
    "3. When should you use Z-ordering?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 6: Vacuum Operations\n",
    "\n",
    "### Task 6.1: Understanding Vacuum\n",
    "```python\n",
    "# Check current table history and versions\n",
    "history = spark.sql(\"DESCRIBE HISTORY delta_lab.orders\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\"))\n",
    "\n",
    "# Try to query an old version\n",
    "try:\n",
    "    old_version_count = spark.sql(\"SELECT COUNT(*) FROM delta_lab.orders VERSION AS OF 0\").collect()[0][0]\n",
    "    print(f\"Can still access version 0: {old_version_count} records\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing version 0: {str(e)}\")\n",
    "```\n",
    "\n",
    "### Task 6.2: Perform Vacuum\n",
    "```python\n",
    "# First, let's see what files exist\n",
    "all_files_before = dbutils.fs.ls(delta_table_path)\n",
    "print(f\"Total files before vacuum: {len(all_files_before)}\")\n",
    "\n",
    "# Set retention period to 0 for demonstration (DON'T do this in production!)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# Perform VACUUM with 0 hours retention (removes all unused files)\n",
    "print(\"Running VACUUM...\")\n",
    "spark.sql(\"VACUUM delta_lab.orders RETAIN 0 HOURS\").show(1, False)\n",
    "\n",
    "# Check files after vacuum\n",
    "all_files_after = dbutils.fs.ls(delta_table_path)\n",
    "print(f\"Total files after vacuum: {len(all_files_after)}\")\n",
    "\n",
    "# Try to access old version again\n",
    "try:\n",
    "    old_version_count = spark.sql(\"SELECT COUNT(*) FROM delta_lab.orders VERSION AS OF 0\").collect()[0][0]\n",
    "    print(f\"Can still access version 0: {old_version_count} records\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing version 0 after vacuum: {str(e)}\")\n",
    "```\n",
    "\n",
    "### Task 6.3: Vacuum with Different Retention Periods\n",
    "```python\n",
    "# Create some more versions for demonstration\n",
    "spark.sql(\"INSERT INTO delta_lab.orders SELECT * FROM delta_lab.orders LIMIT 1000\")\n",
    "spark.sql(\"UPDATE delta_lab.orders SET status = 'Processing' WHERE status = 'Pending'\")\n",
    "\n",
    "# Show current history\n",
    "current_history = spark.sql(\"DESCRIBE HISTORY delta_lab.orders\")\n",
    "display(current_history.select(\"version\", \"timestamp\", \"operation\").orderBy(desc(\"version\")))\n",
    "\n",
    "# Vacuum with 168 hours (7 days) retention - typical production setting\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "print(\"Running VACUUM with 7 days retention...\")\n",
    "spark.sql(\"VACUUM delta_lab.orders RETAIN 168 HOURS\").show(1, False)\n",
    "```\n",
    "\n",
    "### Questions for Task 6:\n",
    "1. What happens to old data files after VACUUM?\n",
    "2. Why is there a default retention period?\n",
    "3. What are the risks of setting retention period too low?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 7: Advanced Monitoring and Analysis\n",
    "\n",
    "### Task 7.1: Analyze Table Statistics\n",
    "```python\n",
    "# Get detailed table information\n",
    "spark.sql(\"DESCRIBE DETAIL delta_lab.orders\").show(1, False)\n",
    "\n",
    "# Show column statistics\n",
    "spark.sql(\"DESCRIBE EXTENDED delta_lab.orders\").show(50, False)\n",
    "\n",
    "# Analyze data distribution\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        product_category,\n",
    "        COUNT(*) as record_count,\n",
    "        AVG(order_amount) as avg_amount,\n",
    "        MIN(order_date) as min_date,\n",
    "        MAX(order_date) as max_date\n",
    "    FROM delta_lab.orders\n",
    "    GROUP BY region, product_category\n",
    "    ORDER BY region, product_category\n",
    "\"\"\").show(25, False)\n",
    "```\n",
    "\n",
    "### Task 7.2: Monitor File Statistics Over Time\n",
    "```python\n",
    "# Create a function to get file statistics\n",
    "def get_file_stats(table_path):\n",
    "    files = dbutils.fs.ls(table_path)\n",
    "    data_files = [f for f in files if f.name.endswith('.parquet')]\n",
    "    \n",
    "    if data_files:\n",
    "        total_size = sum(f.size for f in data_files)\n",
    "        avg_size = total_size / len(data_files)\n",
    "        min_size = min(f.size for f in data_files)\n",
    "        max_size = max(f.size for f in data_files)\n",
    "        \n",
    "        return {\n",
    "            'file_count': len(data_files),\n",
    "            'total_size_mb': round(total_size / (1024*1024), 2),\n",
    "            'avg_size_mb': round(avg_size / (1024*1024), 2),\n",
    "            'min_size_mb': round(min_size / (1024*1024), 2),\n",
    "            'max_size_mb': round(max_size / (1024*1024), 2)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Get current statistics\n",
    "current_stats = get_file_stats(delta_table_path)\n",
    "print(\"Current File Statistics:\")\n",
    "for key, value in current_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "```\n",
    "\n",
    "### Questions for Task 7:\n",
    "1. How do you monitor Delta Lake table health?\n",
    "2. What metrics indicate a table needs optimization?\n",
    "3. How often should you run OPTIMIZE and VACUUM?\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 8: Best Practices Implementation\n",
    "\n",
    "### Task 8.1: Implement Automated Optimization\n",
    "```python\n",
    "# Create a maintenance function\n",
    "def maintain_delta_table(table_name, table_path, zorder_columns=None, vacuum_hours=168):\n",
    "    \"\"\"\n",
    "    Perform maintenance on a Delta Lake table\n",
    "    \"\"\"\n",
    "    print(f\"Starting maintenance for {table_name}...\")\n",
    "    \n",
    "    # Get initial statistics\n",
    "    initial_stats = get_file_stats(table_path)\n",
    "    print(f\"Initial file count: {initial_stats['file_count']}\")\n",
    "    \n",
    "    # Run OPTIMIZE\n",
    "    if zorder_columns:\n",
    "        optimize_sql = f\"OPTIMIZE {table_name} ZORDER BY ({', '.join(zorder_columns)})\"\n",
    "    else:\n",
    "        optimize_sql = f\"OPTIMIZE {table_name}\"\n",
    "    \n",
    "    print(f\"Running: {optimize_sql}\")\n",
    "    spark.sql(optimize_sql).show(1, False)\n",
    "    \n",
    "    # Get post-optimize statistics\n",
    "    post_optimize_stats = get_file_stats(table_path)\n",
    "    print(f\"Files after OPTIMIZE: {post_optimize_stats['file_count']}\")\n",
    "    \n",
    "    # Run VACUUM\n",
    "    vacuum_sql = f\"VACUUM {table_name} RETAIN {vacuum_hours} HOURS\"\n",
    "    print(f\"Running: {vacuum_sql}\")\n",
    "    spark.sql(vacuum_sql).show(1, False)\n",
    "    \n",
    "    # Get final statistics\n",
    "    final_stats = get_file_stats(table_path)\n",
    "    print(f\"Final file count: {final_stats['file_count']}\")\n",
    "    \n",
    "    return {\n",
    "        'initial': initial_stats,\n",
    "        'post_optimize': post_optimize_stats,\n",
    "        'final': final_stats\n",
    "    }\n",
    "\n",
    "# Apply maintenance to our table\n",
    "maintenance_results = maintain_delta_table(\n",
    "    \"delta_lab.orders\", \n",
    "    delta_table_path, \n",
    "    zorder_columns=['region', 'product_category'],\n",
    "    vacuum_hours=0  # Only for demo - use 168 in production\n",
    ")\n",
    "```\n",
    "\n",
    "### Task 8.2: Create Monitoring Dashboard Query\n",
    "```python\n",
    "# Create a comprehensive table health query\n",
    "table_health_query = \"\"\"\n",
    "WITH table_stats AS (\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT region) as unique_regions,\n",
    "        COUNT(DISTINCT product_category) as unique_categories,\n",
    "        MIN(order_date) as min_date,\n",
    "        MAX(order_date) as max_date,\n",
    "        AVG(order_amount) as avg_order_value\n",
    "    FROM delta_lab.orders\n",
    "),\n",
    "history_stats AS (\n",
    "    SELECT \n",
    "        COUNT(*) as total_versions,\n",
    "        MAX(version) as latest_version,\n",
    "        MIN(timestamp) as first_commit,\n",
    "        MAX(timestamp) as last_commit\n",
    "    FROM (DESCRIBE HISTORY delta_lab.orders)\n",
    ")\n",
    "SELECT \n",
    "    t.*,\n",
    "    h.*,\n",
    "    datediff(current_date(), date(h.last_commit)) as days_since_last_update\n",
    "FROM table_stats t\n",
    "CROSS JOIN history_stats h\n",
    "\"\"\"\n",
    "\n",
    "print(\"Table Health Dashboard:\")\n",
    "spark.sql(table_health_query).show(1, False)\n",
    "```\n",
    "\n",
    "### Questions for Task 8:\n",
    "1. What maintenance schedule would you recommend for a production table?\n",
    "2. How do you decide which columns to use for Z-ordering?\n",
    "3. What alerts would you set up for Delta Lake table health?\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "```python\n",
    "# Clean up the lab resources\n",
    "spark.sql(\"DROP TABLE IF EXISTS delta_lab.orders\")\n",
    "spark.sql(\"DROP DATABASE IF EXISTS delta_lab CASCADE\")\n",
    "dbutils.fs.rm(base_path, True)\n",
    "print(\"Lab cleanup completed\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you have learned:\n",
    "\n",
    "1. **Delta Lake Basics**: Created and managed Delta Lake tables\n",
    "2. **Time Travel**: Used version and timestamp-based queries to access historical data\n",
    "3. **Delta Logs**: Understood the transaction log structure and metadata\n",
    "4. **Data Files**: Analyzed parquet file organization and small file problems\n",
    "5. **Compaction**: Used OPTIMIZE to consolidate small files\n",
    "6. **Vacuum**: Cleaned up old files while understanding retention policies\n",
    "7. **Z-Order**: Implemented data skipping optimization for better query performance\n",
    "8. **Best Practices**: Created maintenance procedures and monitoring queries\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Practice these concepts with your own datasets\n",
    "- Implement automated maintenance procedures\n",
    "- Explore advanced Delta Lake features like Change Data Feed and Liquid Clustering\n",
    "- Study partition strategies for large tables\n",
    "- Learn about Delta Lake security and access controls"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.7-ExerciseAdvancedFeatures",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
