{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bd9a1e5-07d4-4fb7-8ddb-d0c332d09df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks SQL Concepts - Multiple Choice Questions (MCQs)\n",
    "## Topics: Querying Files, Writing to Tables, Advanced Transformations, Higher-Order Functions & UDFs\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 1**\n",
    "What function replaces the deprecated `input_file_name()` to get file path information when querying files?\n",
    "\n",
    "**A)** `metadata.filename`  \n",
    "**B)** `metadata.filepath`  \n",
    "**C)** `file_path()`  \n",
    "**D)** `input_path()`\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: The metadata.filepath attribute provides the full path to the input file, replacing the deprecated input_file_name() function.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 2**\n",
    "Which command is used to refresh metadata cache for external tables when new files are added?\n",
    "\n",
    "**A)** `CACHE TABLE table_name`  \n",
    "**B)** `REFRESH TABLE table_name`  \n",
    "**C)** `RELOAD TABLE table_name`  \n",
    "**D)** `UPDATE TABLE table_name`\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: REFRESH TABLE invalidates and reloads the metadata cache, making Databricks re-check underlying data files and schema.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 3**\n",
    "What is the correct syntax to query CSV files directly using the `read_files` function?\n",
    "\n",
    "**A)** `SELECT * FROM read_files(\"path\", \"csv\", header=true)`  \n",
    "**B)** `SELECT * FROM read_files(\"path\", format=\"csv\", header=\"true\")`  \n",
    "**C)** `SELECT * FROM read_files(\"path\", format => \"csv\", header => \"true\")`  \n",
    "**D)** `SELECT * FROM read_files(\"path\", format => \"csv\", header => true)`\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: The read_files function uses the => operator for parameter assignment in SQL.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 4**\n",
    "Which statement about `CREATE OR REPLACE TABLE` is true?\n",
    "\n",
    "**A)** It fails if the table doesn't exist  \n",
    "**B)** It creates a new table version and allows parallel reads during execution  \n",
    "**C)** It requires the same schema as the original table  \n",
    "**D)** It cannot be used with Delta tables\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: CREATE OR REPLACE TABLE creates a new version, allows concurrent reads, and preserves old data if the operation fails.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 5**\n",
    "What happens when you use `INSERT OVERWRITE` with a schema mismatch?\n",
    "\n",
    "**A)** It automatically adjusts the schema  \n",
    "**B)** It ignores extra columns  \n",
    "**C)** It fails with a schema mismatch error  \n",
    "**D)** It creates a new table\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: INSERT OVERWRITE requires schema compatibility and fails if there's a mismatch, unlike CREATE OR REPLACE TABLE.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 6**\n",
    "In a MERGE operation, what does the `WHEN NOT MATCHED` clause do?\n",
    "\n",
    "**A)** Updates existing records  \n",
    "**B)** Deletes unmatched records  \n",
    "**C)** Inserts new records that don't exist in the target table  \n",
    "**D)** Skips unmatched records\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: WHEN NOT MATCHED handles records in the source that don't exist in the target table by inserting them.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 7**\n",
    "What does the `from_json()` function require to parse JSON strings?\n",
    "\n",
    "**A)** Only the JSON column name  \n",
    "**B)** A schema definition or schema inference  \n",
    "**C)** A temporary view  \n",
    "**D)** The JSON file path\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: from_json() requires a schema to properly parse JSON strings into structured data.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 8**\n",
    "What is the purpose of the `explode()` function?\n",
    "\n",
    "**A)** To combine multiple arrays into one  \n",
    "**B)** To convert a single row with an array into multiple rows  \n",
    "**C)** To remove duplicates from arrays  \n",
    "**D)** To sort array elements\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: explode() transforms each element in an array into a separate row.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 9**\n",
    "What does `collect_set()` do differently from `collect_list()`?\n",
    "\n",
    "**A)** It collects arrays instead of individual elements  \n",
    "**B)** It maintains order while collect_list doesn't  \n",
    "**C)** It returns unique values only  \n",
    "**D)** It works only with numeric data\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: collect_set() collects unique elements while collect_list() can include duplicates.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 10**\n",
    "In the context of `collect_set(books.book_id)` followed by `flatten()` and `array_distinct()`, what is the purpose of the `flatten()` function?\n",
    "\n",
    "**A)** To remove duplicates  \n",
    "**B)** To merge nested arrays into a single-level array  \n",
    "**C)** To sort the array elements  \n",
    "**D)** To convert strings to lowercase\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: flatten() merges nested arrays (array of arrays) into a single flat array.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 11**\n",
    "What does the PIVOT operation do?\n",
    "\n",
    "**A)** Converts columns to rows  \n",
    "**B)** Converts rows to columns  \n",
    "**C)** Sorts data by multiple columns  \n",
    "**D)** Filters data based on conditions\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: PIVOT transforms row-based data into column-based format, creating columns from unique values in a specified column.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 12**\n",
    "In a PIVOT operation, what does the aggregation function (like `sum()`) do?\n",
    "\n",
    "**A)** It counts the number of rows  \n",
    "**B)** It computes values for each pivot column  \n",
    "**C)** It sorts the pivot columns  \n",
    "**D)** It filters the pivot data\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: The aggregation function computes the values that will populate each pivot column.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 13**\n",
    "What is a STRUCT in Databricks SQL?\n",
    "\n",
    "**A)** A table creation statement  \n",
    "**B)** A complex nested data type that groups multiple fields  \n",
    "**C)** A type of JOIN operation  \n",
    "**D)** A function for string manipulation\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: STRUCT is like a row within a row, grouping multiple fields together into a single column.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 14**\n",
    "Which higher-order function would you use to keep only array elements that satisfy a condition?\n",
    "\n",
    "**A)** `transform()`  \n",
    "**B)** `filter()`  \n",
    "**C)** `reduce()`  \n",
    "**D)** `exists()`\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: filter() keeps elements that satisfy a specified condition.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 15**\n",
    "What does the `transform()` higher-order function do?\n",
    "\n",
    "**A)** Filters array elements  \n",
    "**B)** Applies a function to each element in an array  \n",
    "**C)** Checks if any element matches a condition  \n",
    "**D)** Aggregates array elements\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: transform() applies a lambda function to each element in an array, returning a new array.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 16**\n",
    "In the expression `FILTER(books, i -> i.quantity >= 2)`, what does `i` represent?\n",
    "\n",
    "**A)** The index position  \n",
    "**B)** The entire books array  \n",
    "**C)** Each individual element in the books array  \n",
    "**D)** A counter variable\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: In lambda expressions for higher-order functions, 'i' represents each individual element being processed.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 17**\n",
    "What is the main purpose of User-Defined Functions (UDFs) in Databricks?\n",
    "\n",
    "**A)** To replace all built-in functions  \n",
    "**B)** To extend functionality when built-in functions are insufficient  \n",
    "**C)** To improve query performance  \n",
    "**D)** To create temporary views\n",
    "\n",
    "**Correct Answer: B**  \n",
    "*Explanation: UDFs extend Databricks' built-in functionality for custom logic that cannot be achieved with native functions.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 18**\n",
    "Which statement about SQL UDFs is correct?\n",
    "\n",
    "**A)** They can only return string values  \n",
    "**B)** They are always session-scoped  \n",
    "**C)** They can be defined using CREATE FUNCTION syntax  \n",
    "**D)** They require Python programming knowledge\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: SQL UDFs are defined using CREATE FUNCTION syntax and are written in pure SQL.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 19**\n",
    "What does the `rescued_data` column contain when using `read_files()`?\n",
    "\n",
    "**A)** File metadata information  \n",
    "**B)** Successfully parsed data  \n",
    "**C)** Values that don't match the inferred schema  \n",
    "**D)** Duplicate records\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: rescued_data stores values that don't match the expected schema as JSON strings.*\n",
    "\n",
    "---\n",
    "\n",
    "### **Question 20**\n",
    "In a MERGE operation, if you get an error about `rescued_data` during INSERT, what is the best solution?\n",
    "\n",
    "**A)** Drop the rescued_data column  \n",
    "**B)** Use INSERT * clause  \n",
    "**C)** Explicitly specify column names in the INSERT clause  \n",
    "**D)** Change the source data format\n",
    "\n",
    "**Correct Answer: C**  \n",
    "*Explanation: Explicitly specifying column names in the INSERT clause avoids mapping errors with internal columns like rescued_data.*\n",
    "\n",
    "---\n",
    "\n",
    "## **Answer Key Summary**\n",
    "1. B | 2. B | 3. C | 4. B | 5. C\n",
    "6. C | 7. B | 8. B | 9. C | 10. B\n",
    "11. B | 12. B | 13. B | 14. B | 15. B\n",
    "16. C | 17. B | 18. C | 19. C | 20. C\n",
    "\n",
    "---\n",
    "\n",
    "## **Scoring Guide**\n",
    "- **18-20 correct**: Excellent understanding\n",
    "- **15-17 correct**: Good grasp of concepts\n",
    "- **12-14 correct**: Satisfactory knowledge\n",
    "- **Below 12**: Review recommended"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.5MCQs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
