{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a7b5e39-4f9b-4c50-bf13-a201484491db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Structured Streaming: Complete Guide\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 1: Title Slide\n",
    "# Apache Spark Structured Streaming\n",
    "## A Comprehensive Guide to Stream Processing\n",
    "\n",
    "**Subtitle:** From Basic Concepts to Advanced Implementation\n",
    "**Version:** Apache Spark 3.5.1\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 2: What is Structured Streaming?\n",
    "\n",
    "## Definition\n",
    "- **Scalable and fault-tolerant** stream processing engine\n",
    "- Built on **Spark SQL engine**\n",
    "- Express streaming computation same way as **batch computation**\n",
    "- **Incremental and continuous** execution\n",
    "- **End-to-end exactly-once** fault-tolerance guarantees\n",
    "\n",
    "## Key Benefits\n",
    "✅ **Fast** - Low latency processing (100ms+)  \n",
    "✅ **Scalable** - Handles massive data volumes  \n",
    "✅ **Fault-tolerant** - Automatic recovery from failures  \n",
    "✅ **Unified** - Same API for batch and streaming  \n",
    "✅ **Exactly-once** - No data loss or duplication  \n",
    "\n",
    "---\n",
    "\n",
    "### Slide 3: Processing Models\n",
    "\n",
    "## Two Processing Modes\n",
    "\n",
    "### 1. Micro-batch Processing (Default)\n",
    "- Processes data as **small batch jobs**\n",
    "- **End-to-end latencies**: 100 milliseconds+\n",
    "- **Exactly-once** fault-tolerance guarantees\n",
    "- Best for most use cases\n",
    "\n",
    "### 2. Continuous Processing\n",
    "- **Ultra-low latency**: 1 millisecond+\n",
    "- **At-least-once** guarantees\n",
    "- Available since Spark 2.3\n",
    "- For latency-critical applications\n",
    "\n",
    "```python\n",
    "# Micro-batch (default)\n",
    "query = df.writeStream.start()\n",
    "\n",
    "# Continuous processing\n",
    "query = df.writeStream.trigger(continuous='1 second').start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 4: Programming Model - Core Concept\n",
    "\n",
    "## The \"Infinite Table\" Model\n",
    "\n",
    "### Input Stream = Unbounded Table\n",
    "- Each data record = New row appended to table\n",
    "- Streaming computation = Query on infinite table\n",
    "- Results updated incrementally as new data arrives\n",
    "\n",
    "### Visual Representation:\n",
    "```\n",
    "Input Stream     →     Input Table      →     Result Table     →     Output\n",
    "[data1, data2]          | data1 |             | result1 |           | result1 |\n",
    "[data3, data4]          | data2 |      →      | result2 |     →     | result2 |\n",
    "[data5, data6]          | data3 |             | result3 |           | result3 |\n",
    "                        | data4 |\n",
    "                        | data5 |\n",
    "                        | data6 |\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 5: Quick Example - Streaming Word Count\n",
    "\n",
    "## Python Code Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create streaming DataFrame from socket\n",
    "lines = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Transform: split lines into words and count\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Output to console\n",
    "query = wordCounts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 6: Output Modes\n",
    "\n",
    "## Three Output Modes\n",
    "\n",
    "### 1. Append Mode (Default)\n",
    "- Only **new rows** added to Result Table\n",
    "- Each row output **only once**\n",
    "- Best for: select, where, map, filter, joins\n",
    "\n",
    "### 2. Complete Mode\n",
    "- **Entire Result Table** output every trigger\n",
    "- Best for: aggregation queries\n",
    "- Higher resource usage\n",
    "\n",
    "### 3. Update Mode\n",
    "- Only **updated rows** since last trigger\n",
    "- Available since Spark 2.1.1\n",
    "- Balance between Append and Complete\n",
    "\n",
    "```python\n",
    "# Append mode (default)\n",
    "query = df.writeStream.outputMode(\"append\").start()\n",
    "\n",
    "# Complete mode\n",
    "query = df.writeStream.outputMode(\"complete\").start()\n",
    "\n",
    "# Update mode\n",
    "query = df.writeStream.outputMode(\"update\").start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 7: Creating Streaming DataFrames\n",
    "\n",
    "## Input Sources\n",
    "\n",
    "### 1. File Source\n",
    "```python\n",
    "# Read streaming files\n",
    "df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"/path/to/directory\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### 2. Kafka Source\n",
    "```python\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic1\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### 3. Socket Source (Testing)\n",
    "```python\n",
    "# Read from socket (testing only)\n",
    "df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 8: Source Compatibility Matrix\n",
    "\n",
    "## Fault-Tolerance Support\n",
    "\n",
    "| Source | Fault-Tolerant | Notes |\n",
    "|--------|----------------|-------|\n",
    "| **File Source** | ✅ Yes | Supports most file formats |\n",
    "| **Kafka Source** | ✅ Yes | Production recommended |\n",
    "| **Socket Source** | ❌ No | Testing only |\n",
    "| **Rate Source** | ✅ Yes | Benchmarking/testing |\n",
    "\n",
    "## File Source Options\n",
    "```python\n",
    "df = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/data/events/\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 10) \\\n",
    "    .option(\"latestFirst\", \"true\") \\\n",
    "    .option(\"fileNameOnly\", \"false\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 9: Basic Operations\n",
    "\n",
    "## Supported Operations\n",
    "\n",
    "### Selection, Projection, Aggregation\n",
    "```python\n",
    "# Selection and filtering\n",
    "filtered_df = streaming_df.select(\"name\", \"age\") \\\n",
    "    .where(\"age > 21\")\n",
    "\n",
    "# Aggregation\n",
    "agg_df = streaming_df.groupBy(\"category\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"), \n",
    "         avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "# Register as temp view\n",
    "streaming_df.createOrReplaceTempView(\"streaming_data\")\n",
    "result = spark.sql(\"SELECT category, COUNT(*) FROM streaming_data GROUP BY category\")\n",
    "```\n",
    "\n",
    "### Check if DataFrame is Streaming\n",
    "```python\n",
    "if df.isStreaming:\n",
    "    print(\"This is a streaming DataFrame\")\n",
    "else:\n",
    "    print(\"This is a static DataFrame\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 10: Window Operations - Time-based Aggregations\n",
    "\n",
    "## Event-Time Windows\n",
    "\n",
    "### Concept\n",
    "- Aggregate data based on **event timestamps**\n",
    "- Handle **late-arriving data**\n",
    "- **Tumbling** vs **Sliding** windows\n",
    "\n",
    "### Python Example - Sliding Window\n",
    "```python\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "# 10-minute windows, sliding every 5 minutes\n",
    "windowed_counts = events \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\"),\n",
    "        col(\"word\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Output schema: [window, word, count]\n",
    "query = windowed_counts.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 11: Window Types Visual\n",
    "\n",
    "## Three Types of Time Windows\n",
    "\n",
    "### 1. Tumbling Windows (Fixed)\n",
    "```\n",
    "|----10min----|----10min----|----10min----|\n",
    "12:00    12:10    12:20    12:30\n",
    "```\n",
    "\n",
    "### 2. Sliding Windows (Overlapping)\n",
    "```\n",
    "|----10min----|\n",
    "     |----10min----|\n",
    "          |----10min----|\n",
    "12:00  12:05  12:10  12:15\n",
    "```\n",
    "\n",
    "### 3. Session Windows (Dynamic)\n",
    "```python\n",
    "# Session windows with gap duration\n",
    "session_df = events.groupBy(\n",
    "    session_window(col(\"timestamp\"), \"5 minutes\"),\n",
    "    col(\"user_id\")\n",
    ").count()\n",
    "\n",
    "# Dynamic gap duration\n",
    "session_df = events.groupBy(\n",
    "    session_window(col(\"timestamp\"), \n",
    "                  when(col(\"activity_type\") == \"click\", \"30 seconds\")\n",
    "                  .otherwise(\"5 minutes\")),\n",
    "    col(\"user_id\")\n",
    ").count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 12: Watermarking - Handling Late Data\n",
    "\n",
    "## What is Watermarking?\n",
    "\n",
    "### Problem\n",
    "- **Late data** can arrive out of order\n",
    "- System needs to know when to **stop waiting** for late data\n",
    "- **Memory management** for stateful operations\n",
    "\n",
    "### Solution: Watermarks\n",
    "- Define **maximum expected delay** for late data\n",
    "- Automatically **clean up old state**\n",
    "- Balance between **correctness** and **resource usage**\n",
    "\n",
    "```python\n",
    "# Watermark example\n",
    "result = events \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"5 minutes\"),\n",
    "        col(\"word\")\n",
    "    ) \\\n",
    "    .count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 13: Watermarking Visual Example\n",
    "\n",
    "## How Watermarking Works\n",
    "\n",
    "### Timeline Example:\n",
    "```\n",
    "Current Time: 12:14\n",
    "Watermark: 12:04 (10 minutes behind)\n",
    "\n",
    "Data Processing:\n",
    "- (12:07, \"cat\") → ✅ Processed (within watermark)\n",
    "- (12:09, \"dog\") → ✅ Processed (within watermark)  \n",
    "- (12:02, \"bird\") → ❌ Dropped (beyond watermark)\n",
    "```\n",
    "\n",
    "### Watermark Calculation\n",
    "```\n",
    "Watermark = Max Event Time Seen - Watermark Delay\n",
    "```\n",
    "\n",
    "### Python Implementation\n",
    "```python\n",
    "# Set watermark delay\n",
    "df_with_watermark = streaming_df \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")\n",
    "\n",
    "# Use in aggregation\n",
    "result = df_with_watermark \\\n",
    "    .groupBy(window(\"event_time\", \"5 minutes\")) \\\n",
    "    .count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 14: Join Operations\n",
    "\n",
    "## Types of Joins Supported\n",
    "\n",
    "### 1. Stream-Static Joins\n",
    "```python\n",
    "# Join streaming data with static reference data\n",
    "static_df = spark.read.table(\"user_profiles\")\n",
    "streaming_df = spark.readStream.table(\"user_events\")\n",
    "\n",
    "joined = streaming_df.join(static_df, \"user_id\")\n",
    "```\n",
    "\n",
    "### 2. Stream-Stream Joins\n",
    "```python\n",
    "# Join two streaming DataFrames\n",
    "impressions = spark.readStream.table(\"ad_impressions\")\n",
    "clicks = spark.readStream.table(\"ad_clicks\")\n",
    "\n",
    "# Inner join with time constraints\n",
    "result = impressions \\\n",
    "    .withWatermark(\"imp_time\", \"2 hours\") \\\n",
    "    .join(\n",
    "        clicks.withWatermark(\"click_time\", \"3 hours\"),\n",
    "        expr(\"\"\"\n",
    "            impression_id = click_impression_id AND\n",
    "            click_time >= imp_time AND\n",
    "            click_time <= imp_time + interval 1 hour\n",
    "        \"\"\")\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 15: Join Support Matrix\n",
    "\n",
    "## Supported Join Types\n",
    "\n",
    "| Left Input | Right Input | Join Type | Support |\n",
    "|------------|-------------|-----------|---------|\n",
    "| **Static** | **Static** | All types | ✅ Always |\n",
    "| **Stream** | **Static** | Inner, Left Outer, Semi | ✅ Stateless |\n",
    "| **Static** | **Stream** | Inner, Right Outer | ✅ Stateless |\n",
    "| **Stream** | **Stream** | Inner | ✅ With watermark |\n",
    "| **Stream** | **Stream** | Outer | ✅ With watermark + constraints |\n",
    "\n",
    "### Important Notes:\n",
    "- Stream-stream joins **require watermarks** for state cleanup\n",
    "- **Time constraints** needed for outer joins\n",
    "- **Append mode only** for joins (as of Spark 2.4+)\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 16: Streaming Deduplication\n",
    "\n",
    "## Remove Duplicate Records\n",
    "\n",
    "### With Watermark (Recommended)\n",
    "```python\n",
    "# Deduplicate with watermark\n",
    "deduplicated = streaming_df \\\n",
    "    .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .dropDuplicates([\"user_id\", \"transaction_id\"])\n",
    "\n",
    "# Within watermark window\n",
    "deduplicated_windowed = streaming_df \\\n",
    "    .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .dropDuplicatesWithinWatermark([\"user_id\", \"transaction_id\"])\n",
    "```\n",
    "\n",
    "### Without Watermark (Unbounded State)\n",
    "```python\n",
    "# Caution: State grows indefinitely\n",
    "deduplicated = streaming_df \\\n",
    "    .dropDuplicates([\"user_id\", \"transaction_id\"])\n",
    "```\n",
    "\n",
    "### Use Cases:\n",
    "- **Exactly-once processing** from unreliable sources\n",
    "- **Data quality** improvement\n",
    "- **Idempotent operations**\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 17: Arbitrary Stateful Operations\n",
    "\n",
    "## Advanced State Management\n",
    "\n",
    "### mapGroupsWithState\n",
    "```python\n",
    "from pyspark.sql.streaming import GroupState\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define state update function\n",
    "def update_user_session(key, values, state):\n",
    "    if state.hasTimedOut:\n",
    "        return []\n",
    "    \n",
    "    # Update state logic\n",
    "    current_state = state.getOption if state.exists else {}\n",
    "    \n",
    "    # Process values and update state\n",
    "    for value in values:\n",
    "        # Custom logic here\n",
    "        pass\n",
    "    \n",
    "    state.update(new_state)\n",
    "    state.setTimeoutDuration(\"10 minutes\")\n",
    "    \n",
    "    return [output_row]\n",
    "\n",
    "# Apply stateful operation\n",
    "result = streaming_df \\\n",
    "    .groupByKey(lambda x: x.user_id) \\\n",
    "    .mapGroupsWithState(\n",
    "        outputStructType,\n",
    "        stateStructType,\n",
    "        GroupStateTimeout.ProcessingTimeTimeout\n",
    "    )(update_user_session)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 18: Output Sinks\n",
    "\n",
    "## Built-in Output Sinks\n",
    "\n",
    "### 1. File Sink\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/output/path\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 2. Kafka Sink\n",
    "```python\n",
    "query = df.select(to_json(struct(\"*\")).alias(\"value\")) \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"output-topic\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 3. Console Sink (Debugging)\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"numRows\", 20) \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 19: Sink Capabilities Matrix\n",
    "\n",
    "## Sink Support and Features\n",
    "\n",
    "| Sink Type | Output Modes | Fault-Tolerant | Use Case |\n",
    "|-----------|-------------|----------------|----------|\n",
    "| **File** | Append | ✅ Exactly-once | Data Lake, Analytics |\n",
    "| **Kafka** | Append, Update, Complete | ✅ At-least-once | Message Streaming |\n",
    "| **Console** | Append, Update, Complete | ❌ Debug only | Development |\n",
    "| **Memory** | Append, Complete | ❌ Debug only | Testing |\n",
    "| **ForeachBatch** | All modes | Depends on impl. | Custom Logic |\n",
    "\n",
    "### Custom Sink Example:\n",
    "```python\n",
    "def process_batch(df, epoch_id):\n",
    "    # Custom processing logic\n",
    "    df.write.mode(\"append\").saveAsTable(\"my_table\")\n",
    "    \n",
    "query = streaming_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 20: Triggers - Controlling Execution\n",
    "\n",
    "## Trigger Types\n",
    "\n",
    "### 1. Default (Micro-batch)\n",
    "```python\n",
    "# Process as fast as possible\n",
    "query = df.writeStream.start()\n",
    "```\n",
    "\n",
    "### 2. Fixed Interval\n",
    "```python\n",
    "# Process every 30 seconds\n",
    "query = df.writeStream \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 3. One-time (Batch)\n",
    "```python\n",
    "# Process available data once and stop\n",
    "query = df.writeStream \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 4. Continuous\n",
    "```python\n",
    "# Ultra-low latency processing\n",
    "query = df.writeStream \\\n",
    "    .trigger(continuous='1 second') \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 21: Checkpointing and Fault Tolerance\n",
    "\n",
    "## Checkpoint Location\n",
    "\n",
    "### Purpose:\n",
    "- **Recovery** from failures\n",
    "- **Exactly-once** processing guarantees\n",
    "- **State management** for stateful operations\n",
    "- **Progress tracking**\n",
    "\n",
    "### Implementation:\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"s3://bucket/checkpoints/query1\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### What's Stored:\n",
    "- **Offsets** - Progress in input sources\n",
    "- **State** - Intermediate computation state  \n",
    "- **Metadata** - Query configuration\n",
    "- **WAL** - Write-ahead logs\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 22: Monitoring Streaming Queries\n",
    "\n",
    "## Query Management and Monitoring\n",
    "\n",
    "### Basic Monitoring:\n",
    "```python\n",
    "# Start query\n",
    "query = df.writeStream \\\n",
    "    .queryName(\"my_streaming_query\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Monitor progress\n",
    "print(f\"Query ID: {query.id}\")\n",
    "print(f\"Query Name: {query.name}\")\n",
    "print(f\"Is Active: {query.isActive}\")\n",
    "\n",
    "# Get progress information\n",
    "progress = query.lastProgress\n",
    "print(f\"Batch ID: {progress['batchId']}\")\n",
    "print(f\"Input Rows: {progress['inputRowsPerSecond']}\")\n",
    "print(f\"Process Rows: {progress['processedRowsPerSecond']}\")\n",
    "\n",
    "# Wait for termination\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Advanced Monitoring:\n",
    "```python\n",
    "# Custom progress listener\n",
    "class MyProgressListener:\n",
    "    def onQueryProgress(self, event):\n",
    "        progress = event.progress\n",
    "        print(f\"Batch {progress.batchId} processed {progress.numInputRows} rows\")\n",
    "\n",
    "spark.streams.addListener(MyProgressListener())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 23: State Store Management\n",
    "\n",
    "## State Store Types\n",
    "\n",
    "### 1. HDFS State Store (Default)\n",
    "- **In-memory** state with **HDFS backup**\n",
    "- **Versioned** key-value store\n",
    "- Good for **moderate state sizes**\n",
    "\n",
    "### 2. RocksDB State Store\n",
    "```python\n",
    "# Configure RocksDB state store\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\"\n",
    ")\n",
    "\n",
    "# RocksDB configuration\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.rocksdb.compactOnCommit\", \"false\")\n",
    "```\n",
    "\n",
    "### When to Use RocksDB:\n",
    "- **Millions of keys** in state\n",
    "- **Large state size** (> JVM memory)\n",
    "- **GC pressure** issues\n",
    "- **Better performance** for large state\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 24: Performance Optimization\n",
    "\n",
    "## Best Practices for Performance\n",
    "\n",
    "### 1. Resource Configuration\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.streaming.metricsEnabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.streaming.numRecentProgressUpdates\", \"10\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "### 2. Partitioning Strategy\n",
    "```python\n",
    "# Repartition for better parallelism\n",
    "repartitioned = streaming_df.repartition(col(\"partition_key\"))\n",
    "\n",
    "# Write with partitioning\n",
    "query = repartitioned.writeStream \\\n",
    "    .partitionBy(\"date\", \"hour\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 3. State Optimization\n",
    "```python\n",
    "# Use watermarks to limit state size\n",
    "optimized = streaming_df \\\n",
    "    .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .groupBy(\"key\") \\\n",
    "    .count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 25: Error Handling and Debugging\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### 1. Schema Inference\n",
    "```python\n",
    "# Disable schema inference for files (recommended)\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", \"false\")\n",
    "\n",
    "# Provide explicit schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"/path/to/data\")\n",
    "```\n",
    "\n",
    "### 2. Debugging Techniques\n",
    "```python\n",
    "# Debug with console sink\n",
    "debug_query = streaming_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"numRows\", 50) \\\n",
    "    .start()\n",
    "\n",
    "# Check query plan\n",
    "streaming_df.explain(True)\n",
    "\n",
    "# Monitor metrics\n",
    "print(query.status)\n",
    "print(query.recentProgress)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 26: Complete Example - Real-time Analytics\n",
    "\n",
    "## End-to-End Implementation\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RealTimeAnalytics\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation.deleteOnExit\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"action\", StringType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])\n",
    "\n",
    "# Read from Kafka\n",
    "raw_events = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"user-events\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON data\n",
    "events = raw_events.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Real-time aggregations with watermarking\n",
    "hourly_stats = events \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"1 hour\", \"15 minutes\"),\n",
    "        \"action\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        sum(\"price\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "    )\n",
    "\n",
    "# Write to Delta Lake\n",
    "query = hourly_stats.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .table(\"real_time_analytics\")\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 27: Advanced Features Summary\n",
    "\n",
    "## Key Advanced Capabilities\n",
    "\n",
    "### ✅ **Watermarking**\n",
    "- Handle late-arriving data\n",
    "- Automatic state cleanup\n",
    "- Configurable delay thresholds\n",
    "\n",
    "### ✅ **Multiple Join Types**\n",
    "- Stream-static joins\n",
    "- Stream-stream joins with time constraints\n",
    "- Outer joins with watermarks\n",
    "\n",
    "### ✅ **Stateful Operations**\n",
    "- Custom state management\n",
    "- Session tracking\n",
    "- Complex event processing\n",
    "\n",
    "### ✅ **Fault Tolerance**\n",
    "- Exactly-once processing\n",
    "- Automatic recovery\n",
    "- Checkpoint-based recovery\n",
    "\n",
    "### ✅ **Performance Features**\n",
    "- RocksDB state store\n",
    "- Adaptive query execution\n",
    "- Predicate pushdown\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 28: Migration and Compatibility\n",
    "\n",
    "## Upgrading and Migration\n",
    "\n",
    "### From DStreams to Structured Streaming\n",
    "```python\n",
    "# Old DStreams approach (deprecated)\n",
    "# ssc = StreamingContext(sc, 2)\n",
    "# lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# New Structured Streaming approach\n",
    "lines = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "words = lines.select(explode(split(\"value\", \" \")).alias(\"word\"))\n",
    "```\n",
    "\n",
    "### Version Compatibility\n",
    "- **Checkpoint compatibility** across minor versions\n",
    "- **Schema evolution** support\n",
    "- **Graceful degradation** for unsupported features\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 29: Production Deployment Checklist\n",
    "\n",
    "## Production Readiness\n",
    "\n",
    "### ✅ **Resource Planning**\n",
    "- Memory sizing for state stores\n",
    "- CPU cores for parallelism  \n",
    "- Network bandwidth for throughput\n",
    "\n",
    "### ✅ **Monitoring Setup**\n",
    "- Query progress tracking\n",
    "- Custom metrics collection\n",
    "- Alerting on failures\n",
    "\n",
    "### ✅ **Fault Tolerance**\n",
    "- Reliable checkpoint storage\n",
    "- Multiple Kafka brokers\n",
    "- Cluster redundancy\n",
    "\n",
    "### ✅ **Testing Strategy**\n",
    "- Unit tests for transformations\n",
    "- Integration tests with real data\n",
    "- Load testing for capacity planning\n",
    "\n",
    "### ✅ **Operational Procedures**\n",
    "- Deployment automation\n",
    "- Rollback procedures\n",
    "- Monitoring dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 30: Best Practices Summary\n",
    "\n",
    "## Key Recommendations\n",
    "\n",
    "### 🎯 **Schema Management**\n",
    "- Always provide **explicit schemas**\n",
    "- Plan for **schema evolution**\n",
    "- Use **compatible data types**\n",
    "\n",
    "### 🎯 **State Management**  \n",
    "- Use **watermarks** appropriately\n",
    "- Choose **right state store** (HDFS vs RocksDB)\n",
    "- Monitor **state size growth**\n",
    "\n",
    "### 🎯 **Performance**\n",
    "- **Partition data** appropriately  \n",
    "- **Tune trigger intervals**\n",
    "- **Optimize join conditions**\n",
    "\n",
    "### 🎯 **Reliability**\n",
    "- Use **fault-tolerant sources/sinks**\n",
    "- Configure **proper checkpointing**\n",
    "- Implement **monitoring and alerting**\n",
    "\n",
    "### 🎯 **Testing**\n",
    "- Test with **realistic data volumes**\n",
    "- Validate **exactly-once semantics**\n",
    "- Plan for **failure scenarios**\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 31: Resources and Next Steps\n",
    "\n",
    "## Learning Resources\n",
    "\n",
    "### 📖 **Official Documentation**\n",
    "- Spark Structured Streaming Guide\n",
    "- API Documentation (Python/Scala/Java)\n",
    "- Configuration Reference\n",
    "\n",
    "### 🛠 **Hands-on Practice**\n",
    "- Databricks Community Edition\n",
    "- Local Spark cluster setup\n",
    "- Sample datasets and tutorials\n",
    "\n",
    "### 🏢 **Production Examples**\n",
    "- Netflix - Real-time recommendations\n",
    "- Uber - Surge pricing algorithms  \n",
    "- Airbnb - Fraud detection systems\n",
    "\n",
    "### 🔗 **Community Resources**\n",
    "- Spark user mailing lists\n",
    "- Stack Overflow discussions\n",
    "- GitHub examples and templates\n",
    "\n",
    "---\n",
    "\n",
    "### Slide 32: Q&A and Thank You\n",
    "\n",
    "## Questions & Discussion\n",
    "\n",
    "### Common Questions:\n",
    "1. **When to use Structured Streaming vs batch processing?**\n",
    "2. **How to handle schema evolution in production?**\n",
    "3. **What are the performance implications of stateful operations?**\n",
    "4. **How to troubleshoot common streaming issues?**\n",
    "5. **Best practices for monitoring streaming applications?**\n",
    "\n",
    "### Contact Information:\n",
    "- **Spark Documentation**: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "- **Community Forums**: https://stackoverflow.com/questions/tagged/apache-spark-sql\n",
    "- **GitHub Examples**: https://github.com/apache/spark/tree/master/examples/src/main/python/sql/streaming\n",
    "\n",
    "---\n",
    "\n",
    "## Thank You!\n",
    "### Master Apache Spark Structured Streaming\n",
    "**Build robust, scalable real-time data processing applications**\n",
    "\n",
    "---\n",
    "\n",
    "*This presentation covers Spark Structured Streaming concepts from basic to advanced levels, including practical Python examples, visual explanations, and production best practices. Each slide builds upon previous concepts to ensure comprehensive understanding of stream processing with Apache Spark.*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.0-SparkStructuredStreamingGuide",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
