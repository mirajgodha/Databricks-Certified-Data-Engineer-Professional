{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deb94c1e-944c-4419-a4b4-7bd18999a6d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Advanced Security and Governance\n",
    "## Module: RBAC, Data Lineage, and Enterprise Security\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Implementing Role-Based Access Control (RBAC)](#rbac)\n",
    "2. [Enabling Data Lineage and Auditing with Unity Catalog](#lineage)\n",
    "3. [Advanced Encryption and Network Security](#security)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Implementing Role-Based Access Control (RBAC) {#rbac}\n",
    "\n",
    "### 1.1 Introduction to RBAC in Databricks\n",
    "\n",
    "Role-Based Access Control (RBAC) is a security model that controls access to resources based on user roles and permissions. In Databricks Unity Catalog, RBAC provides fine-grained access control at multiple levels.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Principals**: Users, service principals, and groups\n",
    "- **Securable Objects**: Catalogs, schemas, tables, volumes, functions\n",
    "- **Privileges**: Specific permissions (SELECT, CREATE, MODIFY, etc.)\n",
    "- **Roles**: Collections of privileges that can be assigned to principals\n",
    "\n",
    "### 1.2 Unity Catalog RBAC Hierarchy\n",
    "\n",
    "```\n",
    "Account\n",
    "├── Metastore\n",
    "    ├── Catalog\n",
    "        ├── Schema\n",
    "            ├── Table/View\n",
    "            ├── Function\n",
    "            ├── Volume\n",
    "```\n",
    "\n",
    "#### Privilege Inheritance:\n",
    "- Privileges granted at higher levels cascade down\n",
    "- More specific permissions override general ones\n",
    "- Explicit DENY always takes precedence\n",
    "\n",
    "### 1.3 Setting Up RBAC - Step by Step\n",
    "\n",
    "#### Step 1: Create Groups and Users\n",
    "\n",
    "```sql\n",
    "-- Create groups for different roles\n",
    "CREATE GROUP IF NOT EXISTS data_engineers;\n",
    "CREATE GROUP IF NOT EXISTS data_analysts;\n",
    "CREATE GROUP IF NOT EXISTS data_scientists;\n",
    "CREATE GROUP IF NOT EXISTS business_users;\n",
    "\n",
    "-- Add users to groups (Admin Console or SQL)\n",
    "-- Note: User management typically done through Admin Console\n",
    "```\n",
    "\n",
    "#### Step 2: Create Service Principals for Applications\n",
    "\n",
    "```sql\n",
    "-- Create service principal for ETL processes\n",
    "CREATE SERVICE PRINCIPAL IF NOT EXISTS 'etl-service-principal';\n",
    "\n",
    "-- Create service principal for reporting tools\n",
    "CREATE SERVICE PRINCIPAL IF NOT EXISTS 'reporting-service-principal';\n",
    "```\n",
    "\n",
    "#### Step 3: Create Catalogs with Appropriate Ownership\n",
    "\n",
    "```sql\n",
    "-- Create catalogs for different environments\n",
    "CREATE CATALOG IF NOT EXISTS development\n",
    "COMMENT 'Development environment catalog';\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS staging\n",
    "COMMENT 'Staging environment catalog';\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS production\n",
    "COMMENT 'Production environment catalog';\n",
    "\n",
    "-- Set catalog owners\n",
    "ALTER CATALOG development OWNER TO data_engineers;\n",
    "ALTER CATALOG production OWNER TO `admin@company.com`;\n",
    "```\n",
    "\n",
    "#### Step 4: Implement Schema-Level Access Control\n",
    "\n",
    "```sql\n",
    "-- Create schemas with specific purposes\n",
    "CREATE SCHEMA IF NOT EXISTS production.sales\n",
    "COMMENT 'Sales data and analytics';\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS production.finance\n",
    "COMMENT 'Financial data - restricted access';\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS production.marketing\n",
    "COMMENT 'Marketing data and campaigns';\n",
    "\n",
    "-- Grant schema-level permissions\n",
    "GRANT USE CATALOG ON CATALOG production TO data_analysts;\n",
    "GRANT USE SCHEMA ON SCHEMA production.sales TO data_analysts;\n",
    "GRANT SELECT ON SCHEMA production.sales TO business_users;\n",
    "\n",
    "-- Restrict access to sensitive financial data\n",
    "GRANT USE SCHEMA ON SCHEMA production.finance TO data_engineers;\n",
    "GRANT SELECT ON SCHEMA production.finance TO `finance-team`;\n",
    "```\n",
    "\n",
    "### 1.4 Table and Column-Level Security\n",
    "\n",
    "#### Row-Level Security with Views\n",
    "\n",
    "```sql\n",
    "-- Create base table with sensitive data\n",
    "CREATE TABLE production.sales.customer_transactions (\n",
    "    transaction_id STRING,\n",
    "    customer_id STRING,\n",
    "    amount DECIMAL(10,2),\n",
    "    transaction_date DATE,\n",
    "    region STRING,\n",
    "    customer_ssn STRING,  -- Sensitive data\n",
    "    customer_email STRING\n",
    ");\n",
    "\n",
    "-- Create view with row-level filtering\n",
    "CREATE VIEW production.sales.regional_transactions AS\n",
    "SELECT \n",
    "    transaction_id,\n",
    "    customer_id,\n",
    "    amount,\n",
    "    transaction_date,\n",
    "    region,\n",
    "    -- Mask sensitive data based on user groups\n",
    "    CASE \n",
    "        WHEN is_member('finance-team') THEN customer_ssn\n",
    "        ELSE 'XXX-XX-XXXX'\n",
    "    END as customer_ssn,\n",
    "    customer_email\n",
    "FROM production.sales.customer_transactions\n",
    "WHERE \n",
    "    -- Row-level filtering based on user's region\n",
    "    CASE \n",
    "        WHEN is_member('regional-managers') THEN region = current_user_region()\n",
    "        WHEN is_member('data_engineers') THEN 1=1  -- Full access\n",
    "        ELSE region IN ('North', 'South')  -- Limited regions for others\n",
    "    END;\n",
    "\n",
    "-- Grant access to view instead of base table\n",
    "GRANT SELECT ON VIEW production.sales.regional_transactions TO data_analysts;\n",
    "REVOKE SELECT ON TABLE production.sales.customer_transactions FROM data_analysts;\n",
    "```\n",
    "\n",
    "#### Column-Level Masking with Dynamic Views\n",
    "\n",
    "```sql\n",
    "-- Create dynamic masking view\n",
    "CREATE VIEW production.sales.masked_customer_data AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    -- Dynamic masking based on user roles\n",
    "    CASE \n",
    "        WHEN is_member('pii-readers') THEN customer_email\n",
    "        ELSE regexp_replace(customer_email, '(.{2}).*(@.*)', '$1***$2')\n",
    "    END as customer_email,\n",
    "    \n",
    "    CASE \n",
    "        WHEN is_member('finance-team') THEN amount\n",
    "        ELSE NULL\n",
    "    END as amount,\n",
    "    \n",
    "    transaction_date,\n",
    "    region\n",
    "FROM production.sales.customer_transactions;\n",
    "```\n",
    "\n",
    "#### Using GRANT on Specific Columns\n",
    "\n",
    "```sql\n",
    "-- Allow analyst to query only specific columns\n",
    "GRANT SELECT (id, name, department)\n",
    "ON TABLE sales_db.employee\n",
    "TO `analyst_group`;\n",
    "```\n",
    "\n",
    "### 1.5 Advanced RBAC Patterns\n",
    "\n",
    "#### Pattern 1: Environment-Based Access Control\n",
    "\n",
    "```sql\n",
    "-- Development environment - Open access for data engineers\n",
    "GRANT ALL PRIVILEGES ON CATALOG development TO data_engineers;\n",
    "GRANT USE CATALOG, USE SCHEMA, SELECT ON CATALOG development TO data_scientists;\n",
    "\n",
    "-- Staging environment - Read-only for testing\n",
    "GRANT USE CATALOG, USE SCHEMA, SELECT ON CATALOG staging TO data_analysts;\n",
    "GRANT CREATE SCHEMA, CREATE TABLE ON CATALOG staging TO data_engineers;\n",
    "\n",
    "-- Production environment - Strict controls\n",
    "GRANT USE CATALOG ON CATALOG production TO data_analysts;\n",
    "-- Table-level grants only after approval process\n",
    "```\n",
    "\n",
    "#### Pattern 2: Time-Based Access Control\n",
    "\n",
    "```sql\n",
    "-- Create function to check business hours\n",
    "CREATE FUNCTION production.security.is_business_hours()\n",
    "RETURNS BOOLEAN\n",
    "LANGUAGE SQL\n",
    "DETERMINISTIC\n",
    "RETURN hour(current_timestamp()) BETWEEN 8 AND 18 \n",
    "       AND dayofweek(current_timestamp()) BETWEEN 2 AND 6;\n",
    "\n",
    "-- Create view with time-based restrictions\n",
    "CREATE VIEW production.finance.business_hours_data AS\n",
    "SELECT *\n",
    "FROM production.finance.sensitive_financial_data\n",
    "WHERE production.security.is_business_hours() = true\n",
    "   OR is_member('24x7-access-group');\n",
    "```\n",
    "\n",
    "#### Pattern 3: Data Classification-Based Access\n",
    "\n",
    "```sql\n",
    "-- Tag tables with data classification\n",
    "ALTER TABLE production.sales.customer_data \n",
    "SET TAGS ('classification' = 'PII', 'sensitivity' = 'HIGH');\n",
    "\n",
    "ALTER TABLE production.marketing.campaign_data \n",
    "SET TAGS ('classification' = 'INTERNAL', 'sensitivity' = 'MEDIUM');\n",
    "\n",
    "-- Create policy-based views\n",
    "CREATE VIEW production.sales.classified_customer_data AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    -- Apply masking based on data classification and user clearance\n",
    "    CASE \n",
    "        WHEN get_table_tag('production.sales.customer_data', 'sensitivity') = 'HIGH'\n",
    "             AND NOT is_member('high-clearance-users')\n",
    "        THEN 'REDACTED'\n",
    "        ELSE customer_name\n",
    "    END as customer_name,\n",
    "    purchase_amount,\n",
    "    purchase_date\n",
    "FROM production.sales.customer_data;\n",
    "```\n",
    "\n",
    "### 1.6 Monitoring and Auditing RBAC\n",
    "\n",
    "#### Check Current Permissions\n",
    "\n",
    "```sql\n",
    "-- Show grants for a specific user\n",
    "SHOW GRANTS ON CATALOG production TO `user@company.com`;\n",
    "\n",
    "-- Show grants for a group\n",
    "SHOW GRANTS ON SCHEMA production.sales TO data_analysts;\n",
    "\n",
    "-- Show all grants on a table\n",
    "SHOW GRANTS ON TABLE production.sales.customer_transactions;\n",
    "```\n",
    "\n",
    "#### RBAC Monitoring Queries\n",
    "\n",
    "```sql\n",
    "-- Monitor who has access to sensitive data\n",
    "SELECT \n",
    "    principal,\n",
    "    principal_type,\n",
    "    privilege,\n",
    "    securable_type,\n",
    "    securable_name\n",
    "FROM system.information_schema.grants\n",
    "WHERE securable_name LIKE '%customer%'\n",
    "  AND privilege IN ('SELECT', 'MODIFY', 'ALL PRIVILEGES');\n",
    "\n",
    "-- Track recent permission changes\n",
    "SELECT \n",
    "    user_identity,\n",
    "    action_name,\n",
    "    request_params,\n",
    "    event_time\n",
    "FROM system.access.audit\n",
    "WHERE action_name LIKE '%GRANT%' OR action_name LIKE '%REVOKE%'\n",
    "ORDER BY event_time DESC;\n",
    "```\n",
    "\n",
    "### 1.7 RBAC Best Practices\n",
    "\n",
    "#### 1. Principle of Least Privilege\n",
    "```sql\n",
    "-- Grant minimum necessary permissions\n",
    "GRANT USE CATALOG ON CATALOG production TO analysts;\n",
    "GRANT USE SCHEMA ON SCHEMA production.sales TO analysts;\n",
    "GRANT SELECT ON TABLE production.sales.summary_data TO analysts;\n",
    "-- Don't grant broader permissions unless absolutely necessary\n",
    "```\n",
    "\n",
    "#### 2. Use Groups Instead of Individual User Grants\n",
    "```sql\n",
    "-- Good: Group-based permissions\n",
    "GRANT SELECT ON SCHEMA production.sales TO data_analysts;\n",
    "ALTER GROUP data_analysts ADD USER `new.analyst@company.com`;\n",
    "\n",
    "-- Avoid: Individual user permissions\n",
    "-- GRANT SELECT ON SCHEMA production.sales TO `user1@company.com`;\n",
    "-- GRANT SELECT ON SCHEMA production.sales TO `user2@company.com`;\n",
    "```\n",
    "\n",
    "#### 3. Regular Access Reviews\n",
    "```sql\n",
    "-- Create view for access review\n",
    "CREATE VIEW governance.access_review AS\n",
    "SELECT \n",
    "    principal,\n",
    "    principal_type,\n",
    "    privilege,\n",
    "    securable_type,\n",
    "    securable_name,\n",
    "    grantor,\n",
    "    grant_time\n",
    "FROM system.information_schema.grants\n",
    "WHERE securable_name LIKE 'production.%';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Enabling Data Lineage and Auditing with Unity Catalog {#lineage}\n",
    "\n",
    "### 2.1 Introduction to Data Lineage\n",
    "\n",
    "Data lineage tracks the flow of data from source to destination, including all transformations and dependencies. Unity Catalog provides automatic lineage capture and visualization.\n",
    "\n",
    "#### Benefits of Data Lineage:\n",
    "- **Impact Analysis**: Understand downstream effects of changes\n",
    "- **Root Cause Analysis**: Trace data quality issues to source\n",
    "- **Compliance**: Meet regulatory requirements for data governance\n",
    "- **Documentation**: Automatic documentation of data flows\n",
    "\n",
    "### 2.2 Unity Catalog Lineage Architecture\n",
    "\n",
    "```\n",
    "Data Sources → Unity Catalog → Lineage Graph → Governance Tools\n",
    "     ↓              ↓              ↓              ↓\n",
    "   Tables      Automatic       Visual        Compliance\n",
    "   Views       Capture        Display        Reporting\n",
    "   Functions   Metadata       Analysis       Auditing\n",
    "```\n",
    "\n",
    "### 2.3 Automatic Lineage Capture\n",
    "\n",
    "Unity Catalog automatically captures lineage for:\n",
    "\n",
    "#### SQL Operations\n",
    "```sql\n",
    "-- Create source table\n",
    "CREATE TABLE production.raw.customer_data (\n",
    "    customer_id STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    registration_date DATE\n",
    ");\n",
    "\n",
    "-- Insert sample data\n",
    "INSERT INTO production.raw.customer_data VALUES\n",
    "('C001', 'John', 'Doe', 'john.doe@email.com', '2023-01-15'),\n",
    "('C002', 'Jane', 'Smith', 'jane.smith@email.com', '2023-02-20');\n",
    "\n",
    "-- Create derived table with lineage\n",
    "CREATE TABLE production.clean.customer_profiles AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    CONCAT(first_name, ' ', last_name) as full_name,\n",
    "    LOWER(email) as normalized_email,\n",
    "    registration_date,\n",
    "    YEAR(registration_date) as registration_year\n",
    "FROM production.raw.customer_data\n",
    "WHERE email IS NOT NULL;\n",
    "\n",
    "-- Lineage is automatically captured:\n",
    "-- production.raw.customer_data → production.clean.customer_profiles\n",
    "```\n",
    "\n",
    "#### PySpark Operations\n",
    "```python\n",
    "# Read source data\n",
    "source_df = spark.table(\"production.raw.customer_data\")\n",
    "\n",
    "# Apply transformations\n",
    "cleaned_df = (source_df\n",
    "    .filter(col(\"email\").isNotNull())\n",
    "    .withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "    .withColumn(\"normalized_email\", lower(col(\"email\")))\n",
    "    .withColumn(\"registration_year\", year(col(\"registration_date\")))\n",
    "    .select(\"customer_id\", \"full_name\", \"normalized_email\", \"registration_date\", \"registration_year\")\n",
    ")\n",
    "\n",
    "# Write to destination (lineage captured automatically)\n",
    "cleaned_df.write.mode(\"overwrite\").saveAsTable(\"production.clean.customer_profiles\")\n",
    "```\n",
    "\n",
    "### 2.4 Advanced Lineage Scenarios\n",
    "\n",
    "#### Complex ETL Pipeline Lineage\n",
    "```sql\n",
    "-- Step 1: Raw data ingestion\n",
    "CREATE TABLE production.bronze.sales_events (\n",
    "    event_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    event_timestamp TIMESTAMP,\n",
    "    event_type STRING,\n",
    "    amount DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "-- Step 2: Data cleaning and validation\n",
    "CREATE TABLE production.silver.validated_sales AS\n",
    "SELECT \n",
    "    event_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    event_timestamp,\n",
    "    event_type,\n",
    "    amount,\n",
    "    -- Add data quality flags\n",
    "    CASE \n",
    "        WHEN amount < 0 THEN 'INVALID_AMOUNT'\n",
    "        WHEN customer_id IS NULL THEN 'MISSING_CUSTOMER'\n",
    "        ELSE 'VALID'\n",
    "    END as data_quality_flag\n",
    "FROM production.bronze.sales_events\n",
    "WHERE event_timestamp >= '2023-01-01';\n",
    "\n",
    "-- Step 3: Business logic aggregation\n",
    "CREATE TABLE production.gold.customer_sales_summary AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    COUNT(*) as total_transactions,\n",
    "    SUM(amount) as total_spent,\n",
    "    AVG(amount) as avg_transaction_amount,\n",
    "    MAX(event_timestamp) as last_purchase_date,\n",
    "    MIN(event_timestamp) as first_purchase_date\n",
    "FROM production.silver.validated_sales\n",
    "WHERE data_quality_flag = 'VALID'\n",
    "GROUP BY customer_id;\n",
    "\n",
    "-- Lineage Chain:\n",
    "-- bronze.sales_events → silver.validated_sales → gold.customer_sales_summary\n",
    "```\n",
    "\n",
    "#### Cross-Schema Lineage with Joins\n",
    "```sql\n",
    "-- Customer dimension table\n",
    "CREATE TABLE production.dimensions.customers AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    customer_tier,\n",
    "    registration_date\n",
    "FROM production.raw.customer_master;\n",
    "\n",
    "-- Product dimension table  \n",
    "CREATE TABLE production.dimensions.products AS\n",
    "SELECT \n",
    "    product_id,\n",
    "    product_name,\n",
    "    category,\n",
    "    unit_price\n",
    "FROM production.raw.product_catalog;\n",
    "\n",
    "-- Fact table with multiple lineage sources\n",
    "CREATE TABLE production.facts.enriched_sales AS\n",
    "SELECT \n",
    "    s.event_id,\n",
    "    s.customer_id,\n",
    "    c.customer_name,\n",
    "    c.customer_tier,\n",
    "    s.product_id,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    s.amount,\n",
    "    s.event_timestamp\n",
    "FROM production.silver.validated_sales s\n",
    "JOIN production.dimensions.customers c ON s.customer_id = c.customer_id\n",
    "JOIN production.dimensions.products p ON s.product_id = p.product_id;\n",
    "\n",
    "-- Multi-source lineage:\n",
    "-- validated_sales + customers + products → enriched_sales\n",
    "```\n",
    "\n",
    "### 2.5 Lineage Visualization and Analysis\n",
    "\n",
    "#### Accessing Lineage Information\n",
    "```sql\n",
    "-- Query lineage information from system tables\n",
    "SELECT \n",
    "    source_table_full_name,\n",
    "    target_table_full_name,\n",
    "    source_column_name,\n",
    "    target_column_name\n",
    "FROM system.access.table_lineage\n",
    "WHERE target_table_full_name = 'production.gold.customer_sales_summary';\n",
    "\n",
    "-- View column-level lineage\n",
    "SELECT \n",
    "    upstream_table_name,\n",
    "    upstream_column_name,\n",
    "    downstream_table_name,\n",
    "    downstream_column_name,\n",
    "    lineage_type\n",
    "FROM system.access.column_lineage\n",
    "WHERE downstream_table_name LIKE 'production.gold.%';\n",
    "```\n",
    "\n",
    "#### Creating Lineage Reports\n",
    "```sql\n",
    "-- Create lineage impact analysis view\n",
    "CREATE VIEW governance.lineage_impact_analysis AS\n",
    "WITH RECURSIVE lineage_tree AS (\n",
    "    -- Base case: direct dependencies\n",
    "    SELECT \n",
    "        source_table_full_name as root_table,\n",
    "        target_table_full_name as dependent_table,\n",
    "        1 as level\n",
    "    FROM system.access.table_lineage\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive case: indirect dependencies\n",
    "    SELECT \n",
    "        lt.root_table,\n",
    "        tl.target_table_full_name,\n",
    "        lt.level + 1\n",
    "    FROM lineage_tree lt\n",
    "    JOIN system.access.table_lineage tl \n",
    "        ON lt.dependent_table = tl.source_table_full_name\n",
    "    WHERE lt.level < 5  -- Prevent infinite recursion\n",
    ")\n",
    "SELECT \n",
    "    root_table,\n",
    "    dependent_table,\n",
    "    level,\n",
    "    'DOWNSTREAM' as relationship_type\n",
    "FROM lineage_tree;\n",
    "```\n",
    "\n",
    "### 2.6 Data Auditing with Unity Catalog\n",
    "\n",
    "#### Audit Log Overview\n",
    "Unity Catalog captures detailed audit logs for all data access and modifications:\n",
    "\n",
    "```sql\n",
    "-- View recent data access audit logs\n",
    "SELECT \n",
    "    user_identity,\n",
    "    service_name,\n",
    "    action_name,\n",
    "    request_params,\n",
    "    response,\n",
    "    event_time,\n",
    "    source_ip_address\n",
    "FROM system.access.audit\n",
    "WHERE event_time >= current_timestamp() - INTERVAL 1 DAY\n",
    "  AND action_name IN ('SELECT', 'INSERT', 'UPDATE', 'DELETE')\n",
    "ORDER BY event_time DESC;\n",
    "```\n",
    "\n",
    "#### Detailed Audit Queries\n",
    "\n",
    "```sql\n",
    "-- Monitor sensitive table access\n",
    "SELECT \n",
    "    user_identity,\n",
    "    action_name,\n",
    "    JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') as table_name,\n",
    "    event_time,\n",
    "    source_ip_address\n",
    "FROM system.access.audit\n",
    "WHERE JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') LIKE '%customer%'\n",
    "  AND action_name = 'SELECT'\n",
    "  AND event_time >= current_date() - INTERVAL 7 DAY;\n",
    "\n",
    "-- Track data modification activities\n",
    "SELECT \n",
    "    user_identity,\n",
    "    action_name,\n",
    "    JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') as affected_table,\n",
    "    JSON_EXTRACT_SCALAR(response, '$.result.operation_id') as operation_id,\n",
    "    event_time\n",
    "FROM system.access.audit\n",
    "WHERE action_name IN ('INSERT', 'UPDATE', 'DELETE', 'MERGE')\n",
    "  AND event_time >= current_date() - INTERVAL 1 DAY\n",
    "ORDER BY event_time DESC;\n",
    "\n",
    "-- Monitor permission changes\n",
    "SELECT \n",
    "    user_identity,\n",
    "    action_name,\n",
    "    request_params,\n",
    "    event_time\n",
    "FROM system.access.audit\n",
    "WHERE action_name IN ('GRANT', 'REVOKE', 'CREATE_PRINCIPAL', 'DROP_PRINCIPAL')\n",
    "ORDER BY event_time DESC;\n",
    "```\n",
    "\n",
    "### 2.7 Advanced Auditing Scenarios\n",
    "\n",
    "#### Compliance Reporting\n",
    "```sql\n",
    "-- Create GDPR compliance audit view\n",
    "CREATE VIEW governance.gdpr_audit_trail AS\n",
    "SELECT \n",
    "    user_identity,\n",
    "    action_name,\n",
    "    JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') as data_asset,\n",
    "    event_time,\n",
    "    source_ip_address,\n",
    "    session_id,\n",
    "    -- Identify PII access\n",
    "    CASE \n",
    "        WHEN JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') LIKE '%customer%' \n",
    "          OR JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') LIKE '%pii%'\n",
    "        THEN 'PII_ACCESS'\n",
    "        ELSE 'REGULAR_ACCESS'\n",
    "    END as access_type\n",
    "FROM system.access.audit\n",
    "WHERE action_name IN ('SELECT', 'INSERT', 'UPDATE', 'DELETE')\n",
    "  AND (\n",
    "    JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') LIKE '%customer%'\n",
    "    OR JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') LIKE '%personal%'\n",
    "    OR JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') LIKE '%pii%'\n",
    "  );\n",
    "```\n",
    "\n",
    "#### Anomaly Detection in Data Access\n",
    "```sql\n",
    "-- Detect unusual access patterns\n",
    "CREATE VIEW governance.access_anomalies AS\n",
    "WITH user_access_stats AS (\n",
    "    SELECT \n",
    "        user_identity,\n",
    "        COUNT(*) as daily_access_count,\n",
    "        COUNT(DISTINCT JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg')) as unique_tables_accessed,\n",
    "        MIN(event_time) as first_access,\n",
    "        MAX(event_time) as last_access,\n",
    "        COUNT(DISTINCT source_ip_address) as unique_ip_addresses\n",
    "    FROM system.access.audit\n",
    "    WHERE event_time >= current_date()\n",
    "      AND action_name = 'SELECT'\n",
    "    GROUP BY user_identity\n",
    "),\n",
    "access_thresholds AS (\n",
    "    SELECT \n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY daily_access_count) as access_threshold,\n",
    "        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY unique_tables_accessed) as table_threshold\n",
    "    FROM user_access_stats\n",
    ")\n",
    "SELECT \n",
    "    uas.user_identity,\n",
    "    uas.daily_access_count,\n",
    "    uas.unique_tables_accessed,\n",
    "    uas.unique_ip_addresses,\n",
    "    uas.first_access,\n",
    "    uas.last_access,\n",
    "    CASE \n",
    "        WHEN uas.daily_access_count > at.access_threshold THEN 'HIGH_VOLUME_ACCESS'\n",
    "        WHEN uas.unique_tables_accessed > at.table_threshold THEN 'BROAD_DATA_ACCESS'\n",
    "        WHEN uas.unique_ip_addresses > 3 THEN 'MULTIPLE_LOCATIONS'\n",
    "        ELSE 'NORMAL'\n",
    "    END as anomaly_type\n",
    "FROM user_access_stats uas\n",
    "CROSS JOIN access_thresholds at\n",
    "WHERE uas.daily_access_count > at.access_threshold\n",
    "   OR uas.unique_tables_accessed > at.table_threshold\n",
    "   OR uas.unique_ip_addresses > 3;\n",
    "```\n",
    "\n",
    "### 2.8 Lineage and Audit Best Practices\n",
    "\n",
    "#### 1. Comprehensive Metadata Tagging\n",
    "```sql\n",
    "-- Tag tables with business context\n",
    "ALTER TABLE production.sales.customer_data \n",
    "SET TAGS (\n",
    "    'business_owner' = 'sales_team',\n",
    "    'data_steward' = 'data_governance_team',\n",
    "    'compliance_requirement' = 'GDPR',\n",
    "    'refresh_frequency' = 'daily',\n",
    "    'data_quality_tier' = 'gold'\n",
    ");\n",
    "\n",
    "-- Tag sensitive columns\n",
    "ALTER TABLE production.sales.customer_data \n",
    "ALTER COLUMN customer_email \n",
    "SET TAGS ('pii' = 'true', 'encryption_required' = 'true');\n",
    "```\n",
    "\n",
    "#### 2. Automated Lineage Validation\n",
    "```sql\n",
    "-- Create function to validate expected lineage\n",
    "CREATE FUNCTION governance.validate_lineage(\n",
    "    source_table STRING,\n",
    "    target_table STRING\n",
    ") RETURNS BOOLEAN\n",
    "LANGUAGE SQL\n",
    "RETURN EXISTS (\n",
    "    SELECT 1 \n",
    "    FROM system.access.table_lineage \n",
    "    WHERE source_table_full_name = source_table \n",
    "      AND target_table_full_name = target_table\n",
    ");\n",
    "\n",
    "-- Use in data quality checks\n",
    "SELECT \n",
    "    'lineage_validation' as check_type,\n",
    "    CASE \n",
    "        WHEN governance.validate_lineage(\n",
    "            'production.raw.customer_data', \n",
    "            'production.gold.customer_summary'\n",
    "        ) THEN 'PASS'\n",
    "        ELSE 'FAIL'\n",
    "    END as result;\n",
    "```\n",
    "\n",
    "#### 3. Regular Audit Reviews\n",
    "```sql\n",
    "-- Create scheduled audit summary\n",
    "CREATE VIEW governance.daily_audit_summary AS\n",
    "SELECT \n",
    "    DATE(event_time) as audit_date,\n",
    "    action_name,\n",
    "    COUNT(*) as action_count,\n",
    "    COUNT(DISTINCT user_identity) as unique_users,\n",
    "    COUNT(DISTINCT JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg')) as unique_resources\n",
    "FROM system.access.audit\n",
    "WHERE event_time >= current_date() - INTERVAL 30 DAY\n",
    "GROUP BY DATE(event_time), action_name\n",
    "ORDER BY audit_date DESC, action_count DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Advanced Encryption and Network Security {#security}\n",
    "\n",
    "### 3.1 Introduction to Databricks Security Architecture\n",
    "\n",
    "Databricks implements a comprehensive security model with multiple layers of protection:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Application Layer                        │\n",
    "│  ┌─────────────┐ ┌─────────────┐ ┌─────────────────────────┐│\n",
    "│  │   RBAC &    │ │   Unity     │ │    Audit & Lineage     ││\n",
    "│  │ Fine-grained│ │  Catalog    │ │      Tracking          ││\n",
    "│  │ Permissions │ │ Governance  │ │                        ││\n",
    "│  └─────────────┘ └─────────────┘ └─────────────────────────┘│\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                     Data Layer                              │\n",
    "│  ┌─────────────┐ ┌─────────────┐ ┌─────────────────────────┐│\n",
    "│  │ Encryption  │ │   Secret    │ │   Data Classification  ││\n",
    "│  │ at Rest &   │ │ Management  │ │   & Data Discovery     ││\n",
    "│  │ in Transit  │ │             │ │                        ││\n",
    "│  └─────────────┘ └─────────────┘ └─────────────────────────┘│\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                   Network Layer                             │\n",
    "│  ┌─────────────┐ ┌─────────────┐ ┌─────────────────────────┐│\n",
    "│  │    VPC      │ │  Private    │ │    Network Security    ││\n",
    "│  │ Isolation & │ │ Endpoints & │ │   Groups & Firewall    ││\n",
    "│  │  Peering    │ │  IP Access  │ │        Rules           ││\n",
    "│  └─────────────┘ └─────────────┘ └─────────────────────────┘│\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                Infrastructure Layer                         │\n",
    "│  ┌─────────────┐ ┌─────────────┐ ┌─────────────────────────┐│\n",
    "│  │   Cloud     │ │  Identity   │ │   Compliance &         ││\n",
    "│  │ Provider    │ │ Provider    │ │   Certifications       ││\n",
    "│  │  Security   │ │ Integration │ │   (SOC2, HIPAA, etc.)  ││\n",
    "│  └─────────────┘ └─────────────┘ └─────────────────────────┘│\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 3.2 Encryption at Rest\n",
    "\n",
    "#### AWS Implementation\n",
    "```python\n",
    "# Configure cluster with encryption at rest\n",
    "cluster_config = {\n",
    "    \"cluster_name\": \"secure-analytics-cluster\",\n",
    "    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "    \"node_type_id\": \"i3.xlarge\",\n",
    "    \"driver_node_type_id\": \"i3.xlarge\",\n",
    "    \"num_workers\": 2,\n",
    "    \n",
    "    # Encryption at rest configuration\n",
    "    \"aws_attributes\": {\n",
    "        \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n",
    "        \"ebs_volume_count\": 1,\n",
    "        \"ebs_volume_size\": 100,\n",
    "        # Enable EBS encryption\n",
    "        \"ebs_volume_encrypted\": True,\n",
    "        # Specify KMS key for encryption\n",
    "        \"ebs_volume_kms_key_id\": \"arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012\"\n",
    "    },\n",
    "    \n",
    "    # Instance profile for S3 access with encryption\n",
    "    \"aws_attributes.instance_profile_arn\": \"arn:aws:iam::123456789012:instance-profile/databricks-s3-access\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### S3 Bucket Encryption Configuration\n",
    "```json\n",
    "{\n",
    "    \"Rules\": [\n",
    "        {\n",
    "            \"ApplyServerSideEncryptionByDefault\": {\n",
    "                \"SSEAlgorithm\": \"aws:kms\",\n",
    "                \"KMSMasterKeyID\": \"arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012\"\n",
    "            },\n",
    "            \"BucketKeyEnabled\": true\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Delta Lake Encryption at Rest\n",
    "```sql\n",
    "-- Create external location with encryption\n",
    "CREATE EXTERNAL LOCATION secure_data_location\n",
    "URL 's3://my-encrypted-bucket/data/'\n",
    "CREDENTIAL my_storage_credential\n",
    "COMMENT 'Encrypted storage location for sensitive data';\n",
    "\n",
    "-- Create table with encryption requirements\n",
    "CREATE TABLE production.sensitive.encrypted_customer_data (\n",
    "    customer_id STRING NOT NULL,\n",
    "    ssn STRING NOT NULL,\n",
    "    credit_card_number STRING,\n",
    "    bank_account_number STRING,\n",
    "    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION 's3://my-encrypted-bucket/data/customer_data/'\n",
    "TBLPROPERTIES (\n",
    "    'encryption.enabled' = 'true',\n",
    "    'classification' = 'HIGHLY_SENSITIVE',\n",
    "    'compliance' = 'PCI_DSS'\n",
    ");\n",
    "```\n",
    "\n",
    "### 3.3 Encryption in Transit\n",
    "\n",
    "#### Cluster SSL/TLS Configuration\n",
    "```python\n",
    "# Enable SSL/TLS for all cluster communications\n",
    "spark_conf = {\n",
    "    # Enable SSL for Spark UI\n",
    "    \"spark.ui.https.enabled\": \"true\",\n",
    "    \"spark.ui.https.port\": \"4040\",\n",
    "    \n",
    "    # Enable SSL for internal Spark communications\n",
    "    \"spark.ssl.enabled\": \"true\",\n",
    "    \"spark.ssl.port\": \"7077\",\n",
    "    \"spark.ssl.keyStore\": \"/databricks/ssl/keystore.jks\",\n",
    "    \"spark.ssl.keyStorePassword\": \"{{secrets/ssl/keystore-password}}\",\n",
    "    \"spark.ssl.trustStore\": \"/databricks/ssl/truststore.jks\",\n",
    "    \"spark.ssl.trustStorePassword\": \"{{secrets/ssl/truststore-password}}\",\n",
    "    \n",
    "    # Require SSL for all connections\n",
    "    \"spark.ssl.needClientAuth\": \"true\",\n",
    "    \n",
    "    # Enable encryption for shuffle operations\n",
    "    \"spark.io.encryption.enabled\": \"true\",\n",
    "    \n",
    "    # Enable encryption for RPC communications\n",
    "    \"spark.network.crypto.enabled\": \"true\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Database Connection Encryption\n",
    "```python\n",
    "# JDBC connection with SSL\n",
    "jdbc_url = \"\"\"\n",
    "jdbc:postgresql://prod-db.example.com:5432/analytics\n",
    "?ssl=true\n",
    "&sslmode=require\n",
    "&sslcert=/databricks/ssl/client-cert.pem\n",
    "&sslkey=/databricks/ssl/client-key.pem\n",
    "&sslrootcert=/databricks/ssl/ca-cert.pem\n",
    "\"\"\"\n",
    "\n",
    "# Read from encrypted database connection\n",
    "encrypted_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"sensitive_customer_data\") \\\n",
    "    .option(\"user\", dbutils.secrets.get(\"database\", \"username\")) \\\n",
    "    .option(\"password\", dbutils.secrets.get(\"database\", \"password\")) \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### 3.4 Secret Management\n",
    "\n",
    "#### Databricks Secret Scopes\n",
    "```python\n",
    "# Create secret scope (CLI command)\n",
    "# databricks secrets create-scope --scope production-secrets --backend-type DATABRICKS\n",
    "\n",
    "# Add secrets to scope (CLI command)\n",
    "# databricks secrets put --scope production-secrets --key database-password\n",
    "# databricks secrets put --scope production-secrets --key api-key\n",
    "# databricks secrets put --scope production-secrets --key encryption-key\n",
    "\n",
    "# Use secrets in code\n",
    "database_password = dbutils.secrets.get(\"production-secrets\", \"database-password\")\n",
    "api_key = dbutils.secrets.get(\"production-secrets\", \"api-key\")\n",
    "encryption_key = dbutils.secrets.get(\"production-secrets\", \"encryption-key\")\n",
    "\n",
    "# Example: Secure database connection\n",
    "connection_properties = {\n",
    "    \"user\": dbutils.secrets.get(\"production-secrets\", \"db-username\"),\n",
    "    \"password\": dbutils.secrets.get(\"production-secrets\", \"db-password\"),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(\n",
    "    url=\"jdbc:postgresql://secure-db.example.com:5432/production\",\n",
    "    table=\"customer_data\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "```\n",
    "\n",
    "#### Azure Key Vault Integration\n",
    "```python\n",
    "# Configure Azure Key Vault-backed secret scope\n",
    "# Use Azure CLI or Databricks CLI:\n",
    "# databricks secrets create-scope --scope azure-key-vault \n",
    "#   --backend-type AZURE_KEYVAULT \n",
    "#   --resource-id /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.KeyVault/vaults/<vault-name>\n",
    "\n",
    "# Access secrets from Azure Key Vault\n",
    "storage_account_key = dbutils.secrets.get(\"azure-key-vault\", \"storage-account-key\")\n",
    "client_secret = dbutils.secrets.get(\"azure-key-vault\", \"service-principal-secret\")\n",
    "\n",
    "# Configure Azure storage access with encryption\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.mystorageaccount.dfs.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.5 Network Security Implementation\n",
    "\n",
    "#### VPC Configuration for AWS\n",
    "```yaml\n",
    "# CloudFormation template for secure VPC setup\n",
    "Resources:\n",
    "  DatabricksVPC:\n",
    "    Type: AWS::EC2::VPC\n",
    "    Properties:\n",
    "      CidrBlock: 10.0.0.0/16\n",
    "      EnableDnsHostnames: true\n",
    "      EnableDnsSupport: true\n",
    "      Tags:\n",
    "        - Key: Name\n",
    "          Value: databricks-secure-vpc\n",
    "\n",
    "  # Private subnets for Databricks clusters\n",
    "  PrivateSubnet1:\n",
    "    Type: AWS::EC2::Subnet\n",
    "    Properties:\n",
    "      VpcId: !Ref DatabricksVPC\n",
    "      CidrBlock: 10.0.1.0/24\n",
    "      AvailabilityZone: !Select [0, !GetAZs '']\n",
    "      MapPublicIpOnLaunch: false\n",
    "\n",
    "  PrivateSubnet2:\n",
    "    Type: AWS::EC2::Subnet\n",
    "    Properties:\n",
    "      VpcId: !Ref DatabricksVPC\n",
    "      CidrBlock: 10.0.2.0/24\n",
    "      AvailabilityZone: !Select [1, !GetAZs '']\n",
    "      MapPublicIpOnLaunch: false\n",
    "\n",
    "  # Security Group for Databricks clusters\n",
    "  DatabricksSecurityGroup:\n",
    "    Type: AWS::EC2::SecurityGroup\n",
    "    Properties:\n",
    "      GroupDescription: Security group for Databricks clusters\n",
    "      VpcId: !Ref DatabricksVPC\n",
    "      SecurityGroupIngress:\n",
    "        # Allow internal communication between cluster nodes\n",
    "        - IpProtocol: -1\n",
    "          SourceSecurityGroupId: !Ref DatabricksSecurityGroup\n",
    "        # Allow HTTPS from corporate network only\n",
    "        - IpProtocol: tcp\n",
    "          FromPort: 443\n",
    "          ToPort: 443\n",
    "          CidrIp: 192.168.0.0/16  # Corporate network CIDR\n",
    "        # Allow SSH from bastion host only\n",
    "        - IpProtocol: tcp\n",
    "          FromPort: 22\n",
    "          ToPort: 22\n",
    "          SourceSecurityGroupId: !Ref BastionSecurityGroup\n",
    "      SecurityGroupEgress:\n",
    "        # Allow HTTPS outbound for package downloads\n",
    "        - IpProtocol: tcp\n",
    "          FromPort: 443\n",
    "          ToPort: 443\n",
    "          CidrIp: 0.0.0.0/0\n",
    "        # Allow HTTP outbound for package downloads\n",
    "        - IpProtocol: tcp\n",
    "          FromPort: 80\n",
    "          ToPort: 80\n",
    "          CidrIp: 0.0.0.0/0\n",
    "```\n",
    "\n",
    "#### IP Access Lists Configuration\n",
    "```python\n",
    "# Configure IP access lists via Databricks API\n",
    "import requests\n",
    "\n",
    "# Define allowed IP ranges\n",
    "allowed_ips = [\n",
    "    \"192.168.1.0/24\",    # Corporate office\n",
    "    \"10.0.0.0/16\",       # VPN users\n",
    "    \"203.0.113.0/24\"     # Partner organization\n",
    "]\n",
    "\n",
    "# Create IP access list\n",
    "access_list_config = {\n",
    "    \"label\": \"corporate-access-list\",\n",
    "    \"list_type\": \"ALLOW\",\n",
    "    \"ip_addresses\": allowed_ips,\n",
    "    \"enabled\": True\n",
    "}\n",
    "\n",
    "# API call to create access list (requires admin token)\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {admin_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{databricks_instance}/api/2.0/ip-access-lists\",\n",
    "    json=access_list_config,\n",
    "    headers=headers\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.6 Private Link and VPC Endpoints\n",
    "\n",
    "#### AWS Private Link Setup\n",
    "```yaml\n",
    "# VPC Endpoint for Databricks workspace\n",
    "DatabricksVPCEndpoint:\n",
    "  Type: AWS::EC2::VPCEndpoint\n",
    "  Properties:\n",
    "    VpcId: !Ref DatabricksVPC\n",
    "    ServiceName: com.amazonaws.vpce.us-west-2.vpce-svc-databricks\n",
    "    VpcEndpointType: Interface\n",
    "    SubnetIds:\n",
    "      - !Ref PrivateSubnet1\n",
    "      - !Ref PrivateSubnet2\n",
    "    SecurityGroupIds:\n",
    "      - !Ref DatabricksVPCEndpointSecurityGroup\n",
    "    PolicyDocument:\n",
    "      Version: '2012-10-17'\n",
    "      Statement:\n",
    "        - Effect: Allow\n",
    "          Principal: '*'\n",
    "          Action:\n",
    "            - databricks:*\n",
    "          Resource: '*'\n",
    "          Condition:\n",
    "            StringEquals:\n",
    "              'aws:PrincipalArn': \n",
    "                - 'arn:aws:iam::123456789012:role/DatabricksInstanceProfile'\n",
    "\n",
    "# S3 VPC Endpoint for data access\n",
    "S3VPCEndpoint:\n",
    "  Type: AWS::EC2::VPCEndpoint\n",
    "  Properties:\n",
    "    VpcId: !Ref DatabricksVPC\n",
    "    ServiceName: com.amazonaws.us-west-2.s3\n",
    "    VpcEndpointType: Gateway\n",
    "    RouteTableIds:\n",
    "      - !Ref PrivateRouteTable\n",
    "```\n",
    "\n",
    "#### Azure Private Link Configuration\n",
    "```json\n",
    "{\n",
    "    \"type\": \"Microsoft.Network/privateEndpoints\",\n",
    "    \"apiVersion\": \"2021-03-01\",\n",
    "    \"name\": \"databricks-private-endpoint\",\n",
    "    \"location\": \"[resourceGroup().location]\",\n",
    "    \"properties\": {\n",
    "        \"subnet\": {\n",
    "            \"id\": \"[resourceId('Microsoft.Network/virtualNetworks/subnets', variables('vnetName'), variables('subnetName'))]\"\n",
    "        },\n",
    "        \"privateLinkServiceConnections\": [\n",
    "            {\n",
    "                \"name\": \"databricks-connection\",\n",
    "                \"properties\": {\n",
    "                    \"privateLinkServiceId\": \"[resourceId('Microsoft.Databricks/workspaces', variables('workspaceName'))]\",\n",
    "                    \"groupIds\": [\"databricks_ui_api\"],\n",
    "                    \"requestMessage\": \"Please approve this connection\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3.7 Data Classification and Discovery\n",
    "\n",
    "#### Automated Data Classification\n",
    "```sql\n",
    "-- Enable automatic data discovery and classification\n",
    "ALTER CATALOG production \n",
    "SET PROPERTIES ('databricks.data_discovery.enabled' = 'true');\n",
    "\n",
    "-- Create classification rules\n",
    "CREATE FUNCTION governance.classify_column(column_name STRING, sample_data STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "DETERMINISTIC\n",
    "RETURN \n",
    "  CASE \n",
    "    WHEN column_name LIKE '%ssn%' OR column_name LIKE '%social%' \n",
    "      THEN 'PII_SSN'\n",
    "    WHEN column_name LIKE '%email%' \n",
    "      THEN 'PII_EMAIL'\n",
    "    WHEN column_name LIKE '%phone%' OR column_name LIKE '%mobile%'\n",
    "      THEN 'PII_PHONE'\n",
    "    WHEN column_name LIKE '%credit%' OR column_name LIKE '%card%'\n",
    "      THEN 'PCI_CREDIT_CARD'\n",
    "    WHEN regexp_like(sample_data, '^[0-9]{3}-[0-9]{2}-[0-9]{4}$')\n",
    "      THEN 'PII_SSN'\n",
    "    WHEN regexp_like(sample_data, '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "      THEN 'PII_EMAIL'\n",
    "    ELSE 'UNCLASSIFIED'\n",
    "  END;\n",
    "\n",
    "-- Apply classification to existing tables\n",
    "CREATE VIEW governance.classified_columns AS\n",
    "SELECT \n",
    "    table_catalog,\n",
    "    table_schema,\n",
    "    table_name,\n",
    "    column_name,\n",
    "    data_type,\n",
    "    governance.classify_column(column_name, '') as classification\n",
    "FROM information_schema.columns\n",
    "WHERE table_catalog = 'production';\n",
    "```\n",
    "\n",
    "#### Implement Data Masking Based on Classification\n",
    "```sql\n",
    "-- Create masking functions for different data types\n",
    "CREATE FUNCTION governance.mask_ssn(ssn STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "DETERMINISTIC\n",
    "RETURN \n",
    "  CASE \n",
    "    WHEN is_member('pii-readers') THEN ssn\n",
    "    ELSE regexp_replace(ssn, '^[0-9]{3}-[0-9]{2}', 'XXX-XX')\n",
    "  END;\n",
    "\n",
    "CREATE FUNCTION governance.mask_email(email STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "DETERMINISTIC\n",
    "RETURN \n",
    "  CASE \n",
    "    WHEN is_member('pii-readers') THEN email\n",
    "    ELSE regexp_replace(email, '^(.{2}).*(@.*)$', '$1***$2')\n",
    "  END;\n",
    "\n",
    "CREATE FUNCTION governance.mask_credit_card(cc STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "DETERMINISTIC\n",
    "RETURN \n",
    "  CASE \n",
    "    WHEN is_member('pci-readers') THEN cc\n",
    "    ELSE regexp_replace(cc, '^[0-9]{4}-[0-9]{4}-[0-9]{4}', 'XXXX-XXXX-XXXX')\n",
    "  END;\n",
    "\n",
    "-- Create automatically masked view\n",
    "CREATE VIEW production.secure.masked_customer_data AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    governance.mask_ssn(social_security_number) as social_security_number,\n",
    "    governance.mask_email(email_address) as email_address,\n",
    "    governance.mask_credit_card(credit_card_number) as credit_card_number,\n",
    "    customer_name,\n",
    "    registration_date\n",
    "FROM production.raw.customer_data;\n",
    "```\n",
    "\n",
    "### 3.8 Compliance and Monitoring\n",
    "\n",
    "#### GDPR Compliance Implementation\n",
    "```sql\n",
    "-- Create GDPR data subject rights implementation\n",
    "CREATE TABLE governance.data_subject_requests (\n",
    "    request_id STRING,\n",
    "    data_subject_id STRING,\n",
    "    request_type STRING, -- 'ACCESS', 'DELETION', 'PORTABILITY', 'RECTIFICATION'\n",
    "    request_date TIMESTAMP,\n",
    "    status STRING, -- 'PENDING', 'IN_PROGRESS', 'COMPLETED', 'REJECTED'\n",
    "    completion_date TIMESTAMP,\n",
    "    requestor_email STRING\n",
    ");\n",
    "\n",
    "-- Data deletion procedure for GDPR right to be forgotten\n",
    "CREATE PROCEDURE governance.process_data_deletion(subject_id STRING)\n",
    "LANGUAGE SQL\n",
    "AS\n",
    "BEGIN\n",
    "  -- Log the deletion request\n",
    "  INSERT INTO governance.data_subject_requests VALUES (\n",
    "    uuid(),\n",
    "    subject_id,\n",
    "    'DELETION',\n",
    "    current_timestamp(),\n",
    "    'IN_PROGRESS',\n",
    "    NULL,\n",
    "    current_user()\n",
    "  );\n",
    "  \n",
    "  -- Delete from all tables containing the subject's data\n",
    "  DELETE FROM production.sales.customer_transactions WHERE customer_id = subject_id;\n",
    "  DELETE FROM production.marketing.customer_preferences WHERE customer_id = subject_id;\n",
    "  DELETE FROM production.support.customer_interactions WHERE customer_id = subject_id;\n",
    "  \n",
    "  -- Update request status\n",
    "  UPDATE governance.data_subject_requests \n",
    "  SET status = 'COMPLETED', completion_date = current_timestamp()\n",
    "  WHERE data_subject_id = subject_id AND request_type = 'DELETION' AND status = 'IN_PROGRESS';\n",
    "END;\n",
    "```\n",
    "\n",
    "#### SOC 2 Compliance Monitoring\n",
    "```sql\n",
    "-- Create SOC 2 control monitoring\n",
    "CREATE VIEW governance.soc2_access_controls AS\n",
    "SELECT \n",
    "    'Access Control' as control_area,\n",
    "    COUNT(DISTINCT user_identity) as unique_users_count,\n",
    "    COUNT(*) as total_access_events,\n",
    "    COUNT(CASE WHEN action_name = 'SELECT' THEN 1 END) as read_operations,\n",
    "    COUNT(CASE WHEN action_name IN ('INSERT', 'UPDATE', 'DELETE') THEN 1 END) as write_operations,\n",
    "    DATE(event_time) as audit_date\n",
    "FROM system.access.audit\n",
    "WHERE event_time >= current_date() - INTERVAL 30 DAYS\n",
    "GROUP BY DATE(event_time);\n",
    "\n",
    "-- Monitor privileged access\n",
    "CREATE VIEW governance.privileged_access_monitoring AS\n",
    "SELECT \n",
    "    user_identity,\n",
    "    action_name,\n",
    "    JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') as resource_accessed,\n",
    "    event_time,\n",
    "    'PRIVILEGED_ACCESS' as alert_type\n",
    "FROM system.access.audit\n",
    "WHERE user_identity IN (\n",
    "    SELECT principal \n",
    "    FROM system.information_schema.grants \n",
    "    WHERE privilege IN ('ALL PRIVILEGES', 'MODIFY', 'CREATE')\n",
    ")\n",
    "AND event_time >= current_timestamp() - INTERVAL 1 DAY;\n",
    "```\n",
    "\n",
    "### 3.9 Security Best Practices Implementation\n",
    "\n",
    "#### 1. Defense in Depth\n",
    "```python\n",
    "# Multi-layer security configuration\n",
    "security_config = {\n",
    "    # Layer 1: Network Security\n",
    "    \"network\": {\n",
    "        \"vpc_id\": \"vpc-12345678\",\n",
    "        \"private_subnets\": [\"subnet-12345678\", \"subnet-87654321\"],\n",
    "        \"security_groups\": [\"sg-databricks-restricted\"],\n",
    "        \"ip_access_lists\": [\"corporate-offices\", \"vpn-users\"]\n",
    "    },\n",
    "    \n",
    "    # Layer 2: Identity and Access Management\n",
    "    \"iam\": {\n",
    "        \"identity_provider\": \"SAML_SSO\",\n",
    "        \"mfa_required\": True,\n",
    "        \"session_timeout\": 480,  # 8 hours\n",
    "        \"rbac_enabled\": True\n",
    "    },\n",
    "    \n",
    "    # Layer 3: Data Protection\n",
    "    \"data_protection\": {\n",
    "        \"encryption_at_rest\": \"AES_256_KMS\",\n",
    "        \"encryption_in_transit\": \"TLS_1_3\",\n",
    "        \"data_classification\": \"AUTOMATIC\",\n",
    "        \"data_masking\": \"ROLE_BASED\"\n",
    "    },\n",
    "    \n",
    "    # Layer 4: Monitoring and Auditing\n",
    "    \"monitoring\": {\n",
    "        \"audit_logging\": \"COMPREHENSIVE\",\n",
    "        \"real_time_alerts\": True,\n",
    "        \"anomaly_detection\": True,\n",
    "        \"compliance_reporting\": \"AUTOMATED\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. Zero Trust Architecture\n",
    "```sql\n",
    "-- Implement zero trust data access model\n",
    "CREATE VIEW governance.zero_trust_access AS\n",
    "SELECT \n",
    "    user_identity,\n",
    "    resource_accessed,\n",
    "    access_granted,\n",
    "    verification_factors,\n",
    "    risk_score,\n",
    "    decision\n",
    "FROM (\n",
    "    SELECT \n",
    "        user_identity,\n",
    "        JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg') as resource_accessed,\n",
    "        -- Evaluate multiple trust factors\n",
    "        CASE \n",
    "            WHEN source_ip_address IN (SELECT ip FROM governance.trusted_ips) THEN 1\n",
    "            ELSE 0\n",
    "        END +\n",
    "        CASE \n",
    "            WHEN user_identity IN (SELECT user FROM governance.verified_users) THEN 1\n",
    "            ELSE 0\n",
    "        END +\n",
    "        CASE \n",
    "            WHEN session_id IN (SELECT session FROM governance.mfa_verified_sessions) THEN 1\n",
    "            ELSE 0\n",
    "        END as verification_factors,\n",
    "        \n",
    "        -- Calculate risk score\n",
    "        CASE \n",
    "            WHEN action_name IN ('DELETE', 'DROP') THEN 3\n",
    "            WHEN action_name IN ('UPDATE', 'INSERT') THEN 2\n",
    "            WHEN action_name = 'SELECT' THEN 1\n",
    "            ELSE 0\n",
    "        END as risk_score,\n",
    "        \n",
    "        -- Make access decision\n",
    "        CASE \n",
    "            WHEN verification_factors >= 2 AND risk_score <= 2 THEN 'ALLOW'\n",
    "            WHEN verification_factors >= 3 THEN 'ALLOW'\n",
    "            ELSE 'DENY'\n",
    "        END as decision,\n",
    "        \n",
    "        action_name IN ('SELECT', 'INSERT', 'UPDATE', 'DELETE') as access_granted\n",
    "    FROM system.access.audit\n",
    "    WHERE event_time >= current_timestamp() - INTERVAL 1 HOUR\n",
    ");\n",
    "```\n",
    "\n",
    "#### 3. Incident Response Automation\n",
    "```python\n",
    "# Automated security incident response\n",
    "def security_incident_response(alert_type, severity, details):\n",
    "    \"\"\"\n",
    "    Automated response to security incidents\n",
    "    \"\"\"\n",
    "    \n",
    "    if severity == \"CRITICAL\":\n",
    "        # Immediate actions for critical incidents\n",
    "        actions = [\n",
    "            \"disable_user_account\",\n",
    "            \"revoke_access_tokens\", \n",
    "            \"notify_security_team\",\n",
    "            \"initiate_forensic_logging\"\n",
    "        ]\n",
    "    elif severity == \"HIGH\":\n",
    "        # Actions for high severity incidents\n",
    "        actions = [\n",
    "            \"require_mfa_reauthentication\",\n",
    "            \"restrict_network_access\",\n",
    "            \"notify_data_steward\",\n",
    "            \"enhanced_monitoring\"\n",
    "        ]\n",
    "    else:\n",
    "        # Actions for medium/low severity\n",
    "        actions = [\n",
    "            \"log_incident\",\n",
    "            \"notify_user\",\n",
    "            \"schedule_security_review\"\n",
    "        ]\n",
    "    \n",
    "    # Execute automated responses\n",
    "    for action in actions:\n",
    "        execute_security_action(action, details)\n",
    "    \n",
    "    # Create incident ticket\n",
    "    create_incident_ticket(alert_type, severity, details, actions)\n",
    "\n",
    "# Example: Detect and respond to anomalous access\n",
    "anomalous_access_query = \"\"\"\n",
    "SELECT \n",
    "    user_identity,\n",
    "    COUNT(*) as access_count,\n",
    "    COUNT(DISTINCT JSON_EXTRACT_SCALAR(request_params, '$.full_name_arg')) as unique_resources,\n",
    "    'ANOMALOUS_ACCESS' as alert_type,\n",
    "    'HIGH' as severity\n",
    "FROM system.access.audit\n",
    "WHERE event_time >= current_timestamp() - INTERVAL 1 HOUR\n",
    "  AND action_name = 'SELECT'\n",
    "GROUP BY user_identity\n",
    "HAVING access_count > 100 OR unique_resources > 50\n",
    "\"\"\"\n",
    "\n",
    "# Execute monitoring query and trigger response\n",
    "anomalous_users = spark.sql(anomalous_access_query).collect()\n",
    "for user in anomalous_users:\n",
    "    security_incident_response(\n",
    "        user.alert_type, \n",
    "        user.severity, \n",
    "        {\"user\": user.user_identity, \"access_count\": user.access_count}\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### RBAC Implementation\n",
    "- **Principle of Least Privilege**: Grant minimum necessary permissions\n",
    "- **Group-Based Management**: Use groups instead of individual user grants\n",
    "- **Hierarchical Permissions**: Leverage Unity Catalog's hierarchical model\n",
    "- **Regular Access Reviews**: Implement automated access review processes\n",
    "\n",
    "### Data Lineage and Auditing\n",
    "- **Automatic Capture**: Unity Catalog automatically captures lineage for SQL and Spark operations\n",
    "- **Comprehensive Auditing**: All data access and modifications are logged\n",
    "- **Impact Analysis**: Use lineage for understanding downstream effects of changes\n",
    "- **Compliance Reporting**: Leverage audit logs for regulatory compliance\n",
    "\n",
    "### Advanced Security\n",
    "- **Defense in Depth**: Implement multiple layers of security controls\n",
    "- **Encryption Everywhere**: Encrypt data at rest and in transit\n",
    "- **Network Isolation**: Use VPCs, private endpoints, and IP restrictions\n",
    "- **Zero Trust Model**: Verify every access request regardless of source\n",
    "- **Automated Response**: Implement automated incident response procedures\n",
    "\n",
    "### Best Practices\n",
    "1. **Start with Security**: Design security into your architecture from the beginning\n",
    "2. **Automate Everything**: Use automation for monitoring, compliance, and incident response\n",
    "3. **Regular Reviews**: Conduct regular security and access reviews\n",
    "4. **Documentation**: Maintain comprehensive documentation of security policies and procedures\n",
    "5. **Training**: Ensure all team members understand security requirements and procedures\n",
    "\n",
    "This comprehensive training module provides the foundation for implementing enterprise-grade security and governance in Databricks environments."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5.2-DatabricksAdvancedSecurityandGovernance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
