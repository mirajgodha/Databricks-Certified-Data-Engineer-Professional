{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ecc9e0-e897-470e-8061-a2584c6a0baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Job Demo: Complete ETL Pipeline\n",
    "## 4 Notebook Series for Automated Data Processing\n",
    "\n",
    "### Overview\n",
    "This demo creates a complete ETL pipeline with 4 connected notebooks that can be orchestrated as a Databricks Job. The pipeline simulates a real-world e-commerce data processing scenario.\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook 1: Data Generation and Ingestion**\n",
    "**File: `01_data_ingestion.py`**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Ingestion Pipeline - Step 1\n",
    "# MAGIC ## Generate and Ingest Raw E-commerce Data\n",
    "# MAGIC \n",
    "# MAGIC **Purpose:** Create realistic e-commerce data and store in Bronze layer\n",
    "# MAGIC **Output:** Raw CSV and JSON files in Delta format\n",
    "# MAGIC **Job Role:** Data Ingestion\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Setup and Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration parameters (can be passed as job parameters)\n",
    "dbutils.widgets.text(\"base_path\", \"/tmp/demo_pipeline\", \"Base Path\")\n",
    "dbutils.widgets.text(\"num_customers\", \"1000\", \"Number of Customers\")\n",
    "dbutils.widgets.text(\"num_orders\", \"5000\", \"Number of Orders\")\n",
    "dbutils.widgets.text(\"run_date\", str(datetime.now().date()), \"Run Date\")\n",
    "\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "num_customers = int(dbutils.widgets.get(\"num_customers\"))\n",
    "num_orders = int(dbutils.widgets.get(\"num_orders\"))\n",
    "run_date = dbutils.widgets.get(\"run_date\")\n",
    "\n",
    "print(f\"Pipeline Configuration:\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "print(f\"Customers: {num_customers}, Orders: {num_orders}\")\n",
    "print(f\"Run Date: {run_date}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Create Raw Data Generators\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def generate_customers(num_customers):\n",
    "    \"\"\"Generate realistic customer data\"\"\"\n",
    "    \n",
    "    customers = []\n",
    "    cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\"]\n",
    "    states = [\"NY\", \"CA\", \"IL\", \"TX\", \"AZ\", \"PA\", \"TX\", \"CA\", \"TX\", \"CA\"]\n",
    "    \n",
    "    for i in range(num_customers):\n",
    "        customer = {\n",
    "            \"customer_id\": f\"CUST_{i+1:06d}\",\n",
    "            \"first_name\": random.choice([\"John\", \"Jane\", \"Mike\", \"Sarah\", \"David\", \"Lisa\", \"Chris\", \"Emma\", \"Alex\", \"Maria\"]),\n",
    "            \"last_name\": random.choice([\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\"]),\n",
    "            \"email\": f\"customer{i+1}@email.com\",\n",
    "            \"phone\": f\"+1-{random.randint(100,999)}-{random.randint(100,999)}-{random.randint(1000,9999)}\",\n",
    "            \"city\": random.choice(cities),\n",
    "            \"state\": random.choice(states),\n",
    "            \"zip_code\": f\"{random.randint(10000, 99999)}\",\n",
    "            \"registration_date\": (datetime.now() - timedelta(days=random.randint(1, 365))).isoformat(),\n",
    "            \"customer_tier\": random.choice([\"Bronze\", \"Silver\", \"Gold\", \"Platinum\"]),\n",
    "            \"total_lifetime_value\": round(random.uniform(100, 10000), 2)\n",
    "        }\n",
    "        customers.append(customer)\n",
    "    \n",
    "    return customers\n",
    "\n",
    "def generate_products():\n",
    "    \"\"\"Generate product catalog\"\"\"\n",
    "    \n",
    "    products = [\n",
    "        {\"product_id\": \"PROD_001\", \"name\": \"Wireless Headphones\", \"category\": \"Electronics\", \"price\": 129.99, \"cost\": 65.00},\n",
    "        {\"product_id\": \"PROD_002\", \"name\": \"Smart Watch\", \"category\": \"Electronics\", \"price\": 299.99, \"cost\": 150.00},\n",
    "        {\"product_id\": \"PROD_003\", \"name\": \"Laptop Bag\", \"category\": \"Accessories\", \"price\": 79.99, \"cost\": 25.00},\n",
    "        {\"product_id\": \"PROD_004\", \"name\": \"Bluetooth Speaker\", \"category\": \"Electronics\", \"price\": 89.99, \"cost\": 35.00},\n",
    "        {\"product_id\": \"PROD_005\", \"name\": \"Phone Case\", \"category\": \"Accessories\", \"price\": 24.99, \"cost\": 8.00},\n",
    "        {\"product_id\": \"PROD_006\", \"name\": \"Tablet Stand\", \"category\": \"Accessories\", \"price\": 39.99, \"cost\": 12.00},\n",
    "        {\"product_id\": \"PROD_007\", \"name\": \"USB Cable\", \"category\": \"Accessories\", \"price\": 19.99, \"cost\": 5.00},\n",
    "        {\"product_id\": \"PROD_008\", \"name\": \"Power Bank\", \"category\": \"Electronics\", \"price\": 49.99, \"cost\": 20.00},\n",
    "        {\"product_id\": \"PROD_009\", \"name\": \"Wireless Mouse\", \"category\": \"Electronics\", \"price\": 34.99, \"cost\": 15.00},\n",
    "        {\"product_id\": \"PROD_010\", \"name\": \"Keyboard\", \"category\": \"Electronics\", \"price\": 79.99, \"cost\": 30.00}\n",
    "    ]\n",
    "    \n",
    "    return products\n",
    "\n",
    "def generate_orders(num_orders, customers, products):\n",
    "    \"\"\"Generate realistic order data\"\"\"\n",
    "    \n",
    "    orders = []\n",
    "    order_items = []\n",
    "    \n",
    "    for i in range(num_orders):\n",
    "        order_id = f\"ORDER_{i+1:08d}\"\n",
    "        customer_id = random.choice(customers)[\"customer_id\"]\n",
    "        order_date = datetime.now() - timedelta(days=random.randint(0, 30))\n",
    "        \n",
    "        # Generate 1-5 items per order\n",
    "        num_items = random.randint(1, 5)\n",
    "        order_total = 0\n",
    "        \n",
    "        for item_seq in range(num_items):\n",
    "            product = random.choice(products)\n",
    "            quantity = random.randint(1, 3)\n",
    "            item_total = product[\"price\"] * quantity\n",
    "            order_total += item_total\n",
    "            \n",
    "            order_items.append({\n",
    "                \"order_id\": order_id,\n",
    "                \"item_sequence\": item_seq + 1,\n",
    "                \"product_id\": product[\"product_id\"],\n",
    "                \"product_name\": product[\"name\"],\n",
    "                \"category\": product[\"category\"],\n",
    "                \"unit_price\": product[\"price\"],\n",
    "                \"quantity\": quantity,\n",
    "                \"item_total\": round(item_total, 2)\n",
    "            })\n",
    "        \n",
    "        orders.append({\n",
    "            \"order_id\": order_id,\n",
    "            \"customer_id\": customer_id,\n",
    "            \"order_date\": order_date.isoformat(),\n",
    "            \"order_status\": random.choice([\"Pending\", \"Processing\", \"Shipped\", \"Delivered\", \"Cancelled\"]),\n",
    "            \"payment_method\": random.choice([\"Credit Card\", \"Debit Card\", \"PayPal\", \"Cash\"]),\n",
    "            \"shipping_address\": f\"{random.randint(100, 999)} Main St\",\n",
    "            \"order_total\": round(order_total, 2),\n",
    "            \"discount_amount\": round(random.uniform(0, order_total * 0.2), 2),\n",
    "            \"tax_amount\": round(order_total * 0.08, 2),\n",
    "            \"created_timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    return orders, order_items\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Generate and Save Raw Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating customers...\")\n",
    "customers_data = generate_customers(num_customers)\n",
    "\n",
    "print(\"Generating products...\")\n",
    "products_data = generate_products()\n",
    "\n",
    "print(\"Generating orders...\")\n",
    "orders_data, order_items_data = generate_orders(num_orders, customers_data, products_data)\n",
    "\n",
    "print(f\"Generated: {len(customers_data)} customers, {len(orders_data)} orders, {len(order_items_data)} order items\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "customers_df = spark.createDataFrame(customers_data)\n",
    "products_df = spark.createDataFrame(products_data)\n",
    "orders_df = spark.createDataFrame(orders_data)\n",
    "order_items_df = spark.createDataFrame(order_items_data)\n",
    "\n",
    "print(\"DataFrames created successfully\")\n",
    "print(f\"Customers: {customers_df.count()} rows\")\n",
    "print(f\"Products: {products_df.count()} rows\") \n",
    "print(f\"Orders: {orders_df.count()} rows\")\n",
    "print(f\"Order Items: {order_items_df.count()} rows\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save to Bronze Layer (Raw Data)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create bronze layer paths\n",
    "bronze_path = f\"{base_path}/bronze\"\n",
    "\n",
    "# Save as Delta tables (Bronze layer)\n",
    "print(\"Saving to Bronze layer...\")\n",
    "\n",
    "customers_df.write.mode(\"overwrite\").option(\"path\", f\"{bronze_path}/customers\").saveAsTable(\"bronze_customers\")\n",
    "products_df.write.mode(\"overwrite\").option(\"path\", f\"{bronze_path}/products\").saveAsTable(\"bronze_products\")\n",
    "orders_df.write.mode(\"overwrite\").option(\"path\", f\"{bronze_path}/orders\").saveAsTable(\"bronze_orders\")\n",
    "order_items_df.write.mode(\"overwrite\").option(\"path\", f\"{bronze_path}/order_items\").saveAsTable(\"bronze_order_items\")\n",
    "\n",
    "print(\"Bronze layer data saved successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Basic data quality checks\n",
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "print(f\"Customers table: {spark.table('bronze_customers').count()} records\")\n",
    "print(f\"Products table: {spark.table('bronze_products').count()} records\")\n",
    "print(f\"Orders table: {spark.table('bronze_orders').count()} records\")\n",
    "print(f\"Order Items table: {spark.table('bronze_order_items').count()} records\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\\\nDuplicate customers: {customers_df.count() - customers_df.dropDuplicates(['customer_id']).count()}\")\n",
    "print(f\"Duplicate orders: {orders_df.count() - orders_df.dropDuplicates(['order_id']).count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Job Success Metrics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create job metrics for monitoring\n",
    "job_metrics = {\n",
    "    \"job_name\": \"data_ingestion\",\n",
    "    \"run_date\": run_date,\n",
    "    \"records_processed\": {\n",
    "        \"customers\": customers_df.count(),\n",
    "        \"products\": products_df.count(), \n",
    "        \"orders\": orders_df.count(),\n",
    "        \"order_items\": order_items_df.count()\n",
    "    },\n",
    "    \"execution_time\": datetime.now().isoformat(),\n",
    "    \"status\": \"SUCCESS\"\n",
    "}\n",
    "\n",
    "# Store metrics (could be written to a monitoring table)\n",
    "print(\"Job Metrics:\", job_metrics)\n",
    "\n",
    "# Pass data to next notebook via temp view or widgets\n",
    "dbutils.notebook.exit(\"SUCCESS\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook 2: Data Cleansing and Validation**\n",
    "**File: `02_data_cleansing.py`**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Cleansing Pipeline - Step 2\n",
    "# MAGIC ## Clean and Validate Bronze Data\n",
    "# MAGIC \n",
    "# MAGIC **Purpose:** Apply data quality rules and create Silver layer\n",
    "# MAGIC **Input:** Bronze layer tables\n",
    "# MAGIC **Output:** Cleansed Silver tables\n",
    "# MAGIC **Job Role:** Data Cleansing\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration and Parameters\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get parameters from job\n",
    "dbutils.widgets.text(\"base_path\", \"/tmp/demo_pipeline\", \"Base Path\")\n",
    "dbutils.widgets.text(\"data_quality_threshold\", \"0.95\", \"Data Quality Threshold\")\n",
    "\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "quality_threshold = float(dbutils.widgets.get(\"data_quality_threshold\"))\n",
    "\n",
    "print(f\"Cleansing Configuration:\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "print(f\"Quality Threshold: {quality_threshold}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read Bronze Layer Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Read bronze tables\n",
    "bronze_customers = spark.table(\"bronze_customers\")\n",
    "bronze_products = spark.table(\"bronze_products\")\n",
    "bronze_orders = spark.table(\"bronze_orders\")\n",
    "bronze_order_items = spark.table(\"bronze_order_items\")\n",
    "\n",
    "print(\"Bronze data loaded:\")\n",
    "print(f\"Customers: {bronze_customers.count()}\")\n",
    "print(f\"Products: {bronze_products.count()}\")\n",
    "print(f\"Orders: {bronze_orders.count()}\")\n",
    "print(f\"Order Items: {bronze_order_items.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Cleansing Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def clean_customers(df):\n",
    "    \"\"\"Apply data quality rules to customers\"\"\"\n",
    "    \n",
    "    cleaned_df = df.filter(\n",
    "        # Remove invalid data\n",
    "        col(\"customer_id\").isNotNull() &\n",
    "        col(\"email\").isNotNull() &\n",
    "        col(\"first_name\").isNotNull() &\n",
    "        col(\"last_name\").isNotNull()\n",
    "    ).withColumn(\n",
    "        # Standardize email format\n",
    "        \"email\", lower(col(\"email\"))\n",
    "    ).withColumn(\n",
    "        # Standardize names\n",
    "        \"first_name\", initcap(col(\"first_name\"))\n",
    "    ).withColumn(\n",
    "        \"last_name\", initcap(col(\"last_name\"))\n",
    "    ).withColumn(\n",
    "        # Validate phone format\n",
    "        \"phone_cleaned\", regexp_replace(col(\"phone\"), \"[^0-9]\", \"\")\n",
    "    ).withColumn(\n",
    "        # Add data quality flags\n",
    "        \"is_valid_email\", col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\")\n",
    "    ).withColumn(\n",
    "        \"is_valid_phone\", length(col(\"phone_cleaned\")) >= 10\n",
    "    ).withColumn(\n",
    "        # Add processed timestamp\n",
    "        \"processed_date\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def clean_orders(df):\n",
    "    \"\"\"Apply data quality rules to orders\"\"\"\n",
    "    \n",
    "    cleaned_df = df.filter(\n",
    "        # Remove invalid orders\n",
    "        col(\"order_id\").isNotNull() &\n",
    "        col(\"customer_id\").isNotNull() &\n",
    "        col(\"order_total\") > 0\n",
    "    ).withColumn(\n",
    "        # Parse order date\n",
    "        \"order_date_parsed\", to_timestamp(col(\"order_date\"))\n",
    "    ).withColumn(\n",
    "        # Calculate net amount\n",
    "        \"net_amount\", col(\"order_total\") - col(\"discount_amount\") + col(\"tax_amount\")\n",
    "    ).withColumn(\n",
    "        # Validate order status\n",
    "        \"is_valid_status\", col(\"order_status\").isin([\"Pending\", \"Processing\", \"Shipped\", \"Delivered\", \"Cancelled\"])\n",
    "    ).withColumn(\n",
    "        # Add business rules\n",
    "        \"is_large_order\", col(\"order_total\") > 500\n",
    "    ).withColumn(\n",
    "        \"order_age_days\", datediff(current_date(), col(\"order_date_parsed\"))\n",
    "    ).withColumn(\n",
    "        \"processed_date\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def clean_order_items(df):\n",
    "    \"\"\"Apply data quality rules to order items\"\"\"\n",
    "    \n",
    "    cleaned_df = df.filter(\n",
    "        # Remove invalid items\n",
    "        col(\"order_id\").isNotNull() &\n",
    "        col(\"product_id\").isNotNull() &\n",
    "        col(\"quantity\") > 0 &\n",
    "        col(\"unit_price\") > 0\n",
    "    ).withColumn(\n",
    "        # Validate calculated totals\n",
    "        \"calculated_total\", round(col(\"unit_price\") * col(\"quantity\"), 2)\n",
    "    ).withColumn(\n",
    "        \"total_variance\", abs(col(\"item_total\") - col(\"calculated_total\"))\n",
    "    ).withColumn(\n",
    "        \"is_total_accurate\", col(\"total_variance\") < 0.01\n",
    "    ).withColumn(\n",
    "        \"processed_date\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Apply Data Cleansing\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Applying data cleansing rules...\")\n",
    "\n",
    "# Clean each dataset\n",
    "silver_customers = clean_customers(bronze_customers)\n",
    "silver_products = bronze_products.withColumn(\"processed_date\", current_timestamp())  # Products need minimal cleaning\n",
    "silver_orders = clean_orders(bronze_orders)\n",
    "silver_order_items = clean_order_items(bronze_order_items)\n",
    "\n",
    "print(\"Data cleansing completed\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Quality Assessment\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calculate quality metrics\n",
    "def calculate_quality_metrics(original_df, cleaned_df, entity_name):\n",
    "    \"\"\"Calculate data quality metrics\"\"\"\n",
    "    \n",
    "    original_count = original_df.count()\n",
    "    cleaned_count = cleaned_df.count()\n",
    "    retention_rate = cleaned_count / original_count if original_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\\\n{entity_name} Quality Metrics:\")\n",
    "    print(f\"  Original records: {original_count}\")\n",
    "    print(f\"  Cleaned records: {cleaned_count}\")\n",
    "    print(f\"  Retention rate: {retention_rate:.2%}\")\n",
    "    print(f\"  Records removed: {original_count - cleaned_count}\")\n",
    "    \n",
    "    return {\n",
    "        \"entity\": entity_name,\n",
    "        \"original_count\": original_count,\n",
    "        \"cleaned_count\": cleaned_count,\n",
    "        \"retention_rate\": retention_rate,\n",
    "        \"quality_passed\": retention_rate >= quality_threshold\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each entity\n",
    "quality_report = []\n",
    "quality_report.append(calculate_quality_metrics(bronze_customers, silver_customers, \"Customers\"))\n",
    "quality_report.append(calculate_quality_metrics(bronze_orders, silver_orders, \"Orders\"))\n",
    "quality_report.append(calculate_quality_metrics(bronze_order_items, silver_order_items, \"Order Items\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Additional Data Quality Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Email validation check\n",
    "invalid_emails = silver_customers.filter(col(\"is_valid_email\") == False).count()\n",
    "print(f\"\\\\nAdditional Quality Checks:\")\n",
    "print(f\"Invalid email addresses: {invalid_emails}\")\n",
    "\n",
    "# Order total validation\n",
    "order_total_issues = silver_order_items.filter(col(\"is_total_accurate\") == False).count()\n",
    "print(f\"Order total calculation issues: {order_total_issues}\")\n",
    "\n",
    "# Orphaned order items (orders without customer)\n",
    "orphaned_items = silver_order_items.join(\n",
    "    silver_customers, \n",
    "    silver_order_items.order_id.startswith(silver_customers.customer_id), \n",
    "    \"left_anti\"\n",
    ").count()\n",
    "print(f\"Orphaned order items: {orphaned_items}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save Silver Layer Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create silver layer path\n",
    "silver_path = f\"{base_path}/silver\"\n",
    "\n",
    "print(\"Saving to Silver layer...\")\n",
    "\n",
    "# Save cleaned data as Delta tables\n",
    "silver_customers.write.mode(\"overwrite\").option(\"path\", f\"{silver_path}/customers\").saveAsTable(\"silver_customers\")\n",
    "silver_products.write.mode(\"overwrite\").option(\"path\", f\"{silver_path}/products\").saveAsTable(\"silver_products\")\n",
    "silver_orders.write.mode(\"overwrite\").option(\"path\", f\"{silver_path}/orders\").saveAsTable(\"silver_orders\")\n",
    "silver_order_items.write.mode(\"overwrite\").option(\"path\", f\"{silver_path}/order_items\").saveAsTable(\"silver_order_items\")\n",
    "\n",
    "print(\"Silver layer data saved successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Quality Gate Check\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Determine if quality standards are met\n",
    "overall_quality_passed = all([report[\"quality_passed\"] for report in quality_report])\n",
    "\n",
    "print(f\"\\\\n=== QUALITY GATE ASSESSMENT ===\")\n",
    "print(f\"Quality Threshold: {quality_threshold:.1%}\")\n",
    "print(f\"Overall Quality Status: {'PASSED' if overall_quality_passed else 'FAILED'}\")\n",
    "\n",
    "if not overall_quality_passed:\n",
    "    print(\"\\\\nQuality issues detected:\")\n",
    "    for report in quality_report:\n",
    "        if not report[\"quality_passed\"]:\n",
    "            print(f\"  - {report['entity']}: {report['retention_rate']:.1%} retention rate\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Job Output and Metrics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create comprehensive job output\n",
    "job_result = {\n",
    "    \"job_name\": \"data_cleansing\",\n",
    "    \"execution_time\": datetime.now().isoformat(),\n",
    "    \"quality_passed\": overall_quality_passed,\n",
    "    \"quality_report\": quality_report,\n",
    "    \"silver_tables_created\": [\n",
    "        \"silver_customers\", \n",
    "        \"silver_products\", \n",
    "        \"silver_orders\", \n",
    "        \"silver_order_items\"\n",
    "    ],\n",
    "    \"additional_checks\": {\n",
    "        \"invalid_emails\": invalid_emails,\n",
    "        \"order_total_issues\": order_total_issues,\n",
    "        \"orphaned_items\": orphaned_items\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Job execution completed\")\n",
    "print(\"Result:\", job_result)\n",
    "\n",
    "# Exit with status\n",
    "dbutils.notebook.exit(\"SUCCESS\" if overall_quality_passed else \"QUALITY_FAILED\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook 3: Data Transformation and Enrichment**\n",
    "**File: `03_data_transformation.py`**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Transformation Pipeline - Step 3\n",
    "# MAGIC ## Transform and Enrich Silver Data for Analytics\n",
    "# MAGIC \n",
    "# MAGIC **Purpose:** Create business-ready Gold layer tables\n",
    "# MAGIC **Input:** Silver layer tables\n",
    "# MAGIC **Output:** Aggregated and enriched Gold tables\n",
    "# MAGIC **Job Role:** Data Transformation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dbutils.widgets.text(\"base_path\", \"/tmp/demo_pipeline\", \"Base Path\")\n",
    "dbutils.widgets.text(\"analysis_date\", str(datetime.now().date()), \"Analysis Date\")\n",
    "\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "analysis_date = dbutils.widgets.get(\"analysis_date\")\n",
    "\n",
    "print(f\"Transformation Configuration:\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "print(f\"Analysis Date: {analysis_date}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Read Silver Layer Data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Load silver tables\n",
    "silver_customers = spark.table(\"silver_customers\")\n",
    "silver_products = spark.table(\"silver_products\")\n",
    "silver_orders = spark.table(\"silver_orders\")\n",
    "silver_order_items = spark.table(\"silver_order_items\")\n",
    "\n",
    "print(\"Silver data loaded:\")\n",
    "print(f\"Customers: {silver_customers.count()}\")\n",
    "print(f\"Products: {silver_products.count()}\")\n",
    "print(f\"Orders: {silver_orders.count()}\")\n",
    "print(f\"Order Items: {silver_order_items.count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Business Logic Transformations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Customer analytics\n",
    "def create_customer_analytics():\n",
    "    \"\"\"Create customer analytics table\"\"\"\n",
    "    \n",
    "    # Calculate customer metrics\n",
    "    customer_orders = silver_orders.groupBy(\"customer_id\").agg(\n",
    "        count(\"order_id\").alias(\"total_orders\"),\n",
    "        sum(\"order_total\").alias(\"total_spent\"),\n",
    "        avg(\"order_total\").alias(\"avg_order_value\"),\n",
    "        max(\"order_date_parsed\").alias(\"last_order_date\"),\n",
    "        min(\"order_date_parsed\").alias(\"first_order_date\")\n",
    "    )\n",
    "    \n",
    "    # Calculate customer segments\n",
    "    customer_analytics = silver_customers.join(customer_orders, \"customer_id\", \"left\").fillna(0).withColumn(\n",
    "        \"customer_segment\",\n",
    "        when(col(\"total_orders\") >= 10, \"VIP\")\n",
    "        .when(col(\"total_orders\") >= 5, \"Loyal\")\n",
    "        .when(col(\"total_orders\") >= 2, \"Regular\")\n",
    "        .otherwise(\"New\")\n",
    "    ).withColumn(\n",
    "        \"days_since_last_order\",\n",
    "        datediff(current_date(), col(\"last_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"customer_lifetime_days\",\n",
    "        datediff(current_date(), col(\"first_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"is_active_customer\",\n",
    "        col(\"days_since_last_order\") <= 90\n",
    "    )\n",
    "    \n",
    "    return customer_analytics\n",
    "\n",
    "# Product performance analytics\n",
    "def create_product_analytics():\n",
    "    \"\"\"Create product performance table\"\"\"\n",
    "    \n",
    "    # Calculate product metrics\n",
    "    product_sales = silver_order_items.groupBy(\"product_id\").agg(\n",
    "        count(\"order_id\").alias(\"total_orders\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "        sum(\"item_total\").alias(\"total_revenue\"),\n",
    "        avg(\"unit_price\").alias(\"avg_selling_price\"),\n",
    "        countDistinct(\"order_id\").alias(\"unique_orders\")\n",
    "    )\n",
    "    \n",
    "    # Add product information and rankings\n",
    "    product_analytics = silver_products.join(product_sales, \"product_id\", \"left\").fillna(0).withColumn(\n",
    "        \"revenue_rank\",\n",
    "        row_number().over(Window.orderBy(desc(\"total_revenue\")))\n",
    "    ).withColumn(\n",
    "        \"quantity_rank\", \n",
    "        row_number().over(Window.orderBy(desc(\"total_quantity_sold\")))\n",
    "    ).withColumn(\n",
    "        \"profit_margin\",\n",
    "        round((col(\"price\") - col(\"cost\")) / col(\"price\") * 100, 2)\n",
    "    ).withColumn(\n",
    "        \"performance_category\",\n",
    "        when(col(\"revenue_rank\") <= 3, \"Top Performer\")\n",
    "        .when(col(\"revenue_rank\") <= 10, \"High Performer\")\n",
    "        .when(col(\"total_revenue\") > 0, \"Regular Performer\")\n",
    "        .otherwise(\"No Sales\")\n",
    "    )\n",
    "    \n",
    "    return product_analytics\n",
    "\n",
    "# Sales analytics by time period\n",
    "def create_sales_analytics():\n",
    "    \"\"\"Create time-based sales analytics\"\"\"\n",
    "    \n",
    "    # Daily sales summary\n",
    "    daily_sales = silver_orders.withColumn(\"order_date\", to_date(\"order_date_parsed\")).groupBy(\"order_date\").agg(\n",
    "        count(\"order_id\").alias(\"daily_orders\"),\n",
    "        sum(\"order_total\").alias(\"daily_revenue\"),\n",
    "        avg(\"order_total\").alias(\"avg_order_value\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    ).withColumn(\n",
    "        \"day_of_week\", dayofweek(\"order_date\")\n",
    "    ).withColumn(\n",
    "        \"day_name\",\n",
    "        when(col(\"day_of_week\") == 1, \"Sunday\")\n",
    "        .when(col(\"day_of_week\") == 2, \"Monday\")\n",
    "        .when(col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "        .when(col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "        .when(col(\"day_of_week\") == 5, \"Thursday\")\n",
    "        .when(col(\"day_of_week\") == 6, \"Friday\")\n",
    "        .when(col(\"day_of_week\") == 7, \"Saturday\")\n",
    "    )\n",
    "    \n",
    "    # Add moving averages\n",
    "    window_spec = Window.orderBy(\"order_date\").rowsBetween(-6, 0)\n",
    "    \n",
    "    sales_analytics = daily_sales.withColumn(\n",
    "        \"seven_day_avg_revenue\",\n",
    "        round(avg(\"daily_revenue\").over(window_spec), 2)\n",
    "    ).withColumn(\n",
    "        \"seven_day_avg_orders\",\n",
    "        round(avg(\"daily_orders\").over(window_spec), 2)\n",
    "    )\n",
    "    \n",
    "    return sales_analytics\n",
    "\n",
    "# Category performance\n",
    "def create_category_analytics():\n",
    "    \"\"\"Create category performance analytics\"\"\"\n",
    "    \n",
    "    category_performance = silver_order_items.join(\n",
    "        silver_products.select(\"product_id\", \"category\"), \"product_id\"\n",
    "    ).groupBy(\"category\").agg(\n",
    "        sum(\"item_total\").alias(\"category_revenue\"),\n",
    "        sum(\"quantity\").alias(\"category_quantity\"),\n",
    "        count(\"order_id\").alias(\"category_orders\"),\n",
    "        countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "        avg(\"unit_price\").alias(\"avg_price\")\n",
    "    ).withColumn(\n",
    "        \"revenue_percentage\",\n",
    "        round(col(\"category_revenue\") / sum(\"category_revenue\").over(Window.partitionBy()) * 100, 2)\n",
    "    )\n",
    "    \n",
    "    return category_performance\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Execute Transformations\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Creating business analytics tables...\")\n",
    "\n",
    "# Create all analytics tables\n",
    "gold_customer_analytics = create_customer_analytics()\n",
    "gold_product_analytics = create_product_analytics()\n",
    "gold_sales_analytics = create_sales_analytics()\n",
    "gold_category_analytics = create_category_analytics()\n",
    "\n",
    "print(\"Transformations completed\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Enrichment\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create a comprehensive order fact table\n",
    "def create_order_fact():\n",
    "    \"\"\"Create enriched order fact table\"\"\"\n",
    "    \n",
    "    order_fact = silver_orders.alias(\"o\").join(\n",
    "        silver_customers.select(\"customer_id\", \"customer_tier\", \"state\").alias(\"c\"), \"customer_id\"\n",
    "    ).join(\n",
    "        silver_order_items.groupBy(\"order_id\").agg(\n",
    "            sum(\"quantity\").alias(\"total_items\"),\n",
    "            countDistinct(\"product_id\").alias(\"unique_products\")\n",
    "        ).alias(\"oi\"), \"order_id\"\n",
    "    ).select(\n",
    "        col(\"o.order_id\"),\n",
    "        col(\"o.customer_id\"),\n",
    "        col(\"c.customer_tier\"),\n",
    "        col(\"c.state\"),\n",
    "        col(\"o.order_date_parsed\").alias(\"order_date\"),\n",
    "        col(\"o.order_status\"),\n",
    "        col(\"o.payment_method\"),\n",
    "        col(\"o.order_total\"),\n",
    "        col(\"o.discount_amount\"),\n",
    "        col(\"o.tax_amount\"),\n",
    "        col(\"o.net_amount\"),\n",
    "        col(\"oi.total_items\"),\n",
    "        col(\"oi.unique_products\"),\n",
    "        col(\"o.is_large_order\"),\n",
    "        col(\"o.order_age_days\")\n",
    "    ).withColumn(\n",
    "        \"order_complexity\",\n",
    "        when(col(\"unique_products\") >= 5, \"Complex\")\n",
    "        .when(col(\"unique_products\") >= 3, \"Moderate\")\n",
    "        .otherwise(\"Simple\")\n",
    "    ).withColumn(\n",
    "        \"discount_percentage\",\n",
    "        round(col(\"discount_amount\") / col(\"order_total\") * 100, 2)\n",
    "    )\n",
    "    \n",
    "    return order_fact\n",
    "\n",
    "gold_order_fact = create_order_fact()\n",
    "\n",
    "print(\"Order fact table created\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save Gold Layer Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create gold layer path\n",
    "gold_path = f\"{base_path}/gold\"\n",
    "\n",
    "print(\"Saving to Gold layer...\")\n",
    "\n",
    "# Save all gold tables\n",
    "gold_customer_analytics.write.mode(\"overwrite\").option(\"path\", f\"{gold_path}/customer_analytics\").saveAsTable(\"gold_customer_analytics\")\n",
    "gold_product_analytics.write.mode(\"overwrite\").option(\"path\", f\"{gold_path}/product_analytics\").saveAsTable(\"gold_product_analytics\")\n",
    "gold_sales_analytics.write.mode(\"overwrite\").option(\"path\", f\"{gold_path}/sales_analytics\").saveAsTable(\"gold_sales_analytics\")\n",
    "gold_category_analytics.write.mode(\"overwrite\").option(\"path\", f\"{gold_path}/category_analytics\").saveAsTable(\"gold_category_analytics\")\n",
    "gold_order_fact.write.mode(\"overwrite\").option(\"path\", f\"{gold_path}/order_fact\").saveAsTable(\"gold_order_fact\")\n",
    "\n",
    "print(\"Gold layer tables saved successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Business Insights Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate business insights\n",
    "print(\"=== BUSINESS INSIGHTS SUMMARY ===\")\n",
    "\n",
    "# Customer insights\n",
    "total_customers = gold_customer_analytics.count()\n",
    "vip_customers = gold_customer_analytics.filter(col(\"customer_segment\") == \"VIP\").count()\n",
    "active_customers = gold_customer_analytics.filter(col(\"is_active_customer\") == True).count()\n",
    "\n",
    "print(f\"\\\\nCustomer Analysis:\")\n",
    "print(f\"  Total customers: {total_customers}\")\n",
    "print(f\"  VIP customers: {vip_customers} ({vip_customers/total_customers:.1%})\")\n",
    "print(f\"  Active customers: {active_customers} ({active_customers/total_customers:.1%})\")\n",
    "\n",
    "# Product insights\n",
    "top_product = gold_product_analytics.orderBy(desc(\"total_revenue\")).first()\n",
    "print(f\"\\\\nProduct Analysis:\")\n",
    "print(f\"  Top revenue product: {top_product['name']} (${top_product['total_revenue']:,.2f})\")\n",
    "\n",
    "# Sales insights\n",
    "total_revenue = gold_sales_analytics.agg(sum(\"daily_revenue\")).collect()[0][0]\n",
    "avg_daily_revenue = gold_sales_analytics.agg(avg(\"daily_revenue\")).collect()[0][0]\n",
    "\n",
    "print(f\"\\\\nSales Analysis:\")\n",
    "print(f\"  Total revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"  Average daily revenue: ${avg_daily_revenue:,.2f}\")\n",
    "\n",
    "# Category insights\n",
    "top_category = gold_category_analytics.orderBy(desc(\"category_revenue\")).first()\n",
    "print(f\"\\\\nCategory Analysis:\")\n",
    "print(f\"  Top category: {top_category['category']} (${top_category['category_revenue']:,.2f})\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Job Completion\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create final job metrics\n",
    "transformation_metrics = {\n",
    "    \"job_name\": \"data_transformation\",\n",
    "    \"execution_time\": datetime.now().isoformat(),\n",
    "    \"gold_tables_created\": [\n",
    "        \"gold_customer_analytics\",\n",
    "        \"gold_product_analytics\", \n",
    "        \"gold_sales_analytics\",\n",
    "        \"gold_category_analytics\",\n",
    "        \"gold_order_fact\"\n",
    "    ],\n",
    "    \"business_metrics\": {\n",
    "        \"total_customers\": total_customers,\n",
    "        \"vip_customers\": vip_customers,\n",
    "        \"active_customers\": active_customers,\n",
    "        \"total_revenue\": float(total_revenue),\n",
    "        \"avg_daily_revenue\": float(avg_daily_revenue)\n",
    "    },\n",
    "    \"status\": \"SUCCESS\"\n",
    "}\n",
    "\n",
    "print(\"Transformation job completed successfully\")\n",
    "print(\"Metrics:\", transformation_metrics)\n",
    "\n",
    "dbutils.notebook.exit(\"SUCCESS\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook 4: Data Quality Monitoring and Reporting**\n",
    "**File: `04_data_monitoring.py`**\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Monitoring and Reporting - Step 4\n",
    "# MAGIC ## Monitor Pipeline Health and Generate Reports\n",
    "# MAGIC \n",
    "# MAGIC **Purpose:** Monitor data quality and generate business reports\n",
    "# MAGIC **Input:** All layer tables (Bronze, Silver, Gold)\n",
    "# MAGIC **Output:** Quality reports and business dashboards\n",
    "# MAGIC **Job Role:** Data Monitoring & Reporting\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuration\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dbutils.widgets.text(\"base_path\", \"/tmp/demo_pipeline\", \"Base Path\")\n",
    "dbutils.widgets.text(\"report_date\", str(datetime.now().date()), \"Report Date\")\n",
    "dbutils.widgets.text(\"alert_threshold\", \"0.9\", \"Alert Threshold\")\n",
    "\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "report_date = dbutils.widgets.get(\"report_date\")\n",
    "alert_threshold = float(dbutils.widgets.get(\"alert_threshold\"))\n",
    "\n",
    "print(f\"Monitoring Configuration:\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "print(f\"Report Date: {report_date}\")\n",
    "print(f\"Alert Threshold: {alert_threshold}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Quality Monitoring\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def check_table_health(table_name, expected_min_rows=0):\n",
    "    \"\"\"Check basic table health metrics\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = spark.table(table_name)\n",
    "        row_count = df.count()\n",
    "        \n",
    "        # Check for null values in key columns\n",
    "        total_columns = len(df.columns)\n",
    "        null_checks = []\n",
    "        \n",
    "        for col_name in df.columns:\n",
    "            null_count = df.filter(col(col_name).isNull()).count()\n",
    "            null_percentage = null_count / row_count if row_count > 0 else 0\n",
    "            null_checks.append({\n",
    "                \"column\": col_name,\n",
    "                \"null_count\": null_count,\n",
    "                \"null_percentage\": null_percentage\n",
    "            })\n",
    "        \n",
    "        # Overall health score\n",
    "        avg_null_percentage = sum([check[\"null_percentage\"] for check in null_checks]) / total_columns\n",
    "        health_score = 1 - avg_null_percentage\n",
    "        \n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"row_count\": row_count,\n",
    "            \"column_count\": total_columns,\n",
    "            \"health_score\": health_score,\n",
    "            \"meets_min_rows\": row_count >= expected_min_rows,\n",
    "            \"null_analysis\": null_checks,\n",
    "            \"status\": \"HEALTHY\" if health_score >= alert_threshold and row_count >= expected_min_rows else \"UNHEALTHY\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"table_name\": table_name,\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"ERROR\"\n",
    "        }\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Checking table health across all layers...\")\n",
    "\n",
    "# Check all tables\n",
    "table_health_report = []\n",
    "\n",
    "# Bronze layer\n",
    "bronze_tables = [\"bronze_customers\", \"bronze_products\", \"bronze_orders\", \"bronze_order_items\"]\n",
    "for table in bronze_tables:\n",
    "    table_health_report.append(check_table_health(table, expected_min_rows=1))\n",
    "\n",
    "# Silver layer\n",
    "silver_tables = [\"silver_customers\", \"silver_products\", \"silver_orders\", \"silver_order_items\"]  \n",
    "for table in silver_tables:\n",
    "    table_health_report.append(check_table_health(table, expected_min_rows=1))\n",
    "\n",
    "# Gold layer\n",
    "gold_tables = [\"gold_customer_analytics\", \"gold_product_analytics\", \"gold_sales_analytics\", \"gold_category_analytics\", \"gold_order_fact\"]\n",
    "for table in gold_tables:\n",
    "    table_health_report.append(check_table_health(table, expected_min_rows=1))\n",
    "\n",
    "print(\"Table health checks completed\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Data Quality Report\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate comprehensive quality report\n",
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "print(f\"Report Date: {report_date}\")\n",
    "print(f\"Report Time: {datetime.now()}\")\n",
    "\n",
    "healthy_tables = [report for report in table_health_report if report.get(\"status\") == \"HEALTHY\"]\n",
    "unhealthy_tables = [report for report in table_health_report if report.get(\"status\") == \"UNHEALTHY\"]\n",
    "error_tables = [report for report in table_health_report if report.get(\"status\") == \"ERROR\"]\n",
    "\n",
    "print(f\"\\\\nOverall Status:\")\n",
    "print(f\"  Healthy tables: {len(healthy_tables)}\")\n",
    "print(f\"  Unhealthy tables: {len(unhealthy_tables)}\")\n",
    "print(f\"  Error tables: {len(error_tables)}\")\n",
    "print(f\"  Total tables: {len(table_health_report)}\")\n",
    "\n",
    "# Detailed report for each layer\n",
    "for layer in [\"bronze\", \"silver\", \"gold\"]:\n",
    "    layer_tables = [report for report in table_health_report if report[\"table_name\"].startswith(layer)]\n",
    "    layer_health = sum([report.get(\"health_score\", 0) for report in layer_tables]) / len(layer_tables) if layer_tables else 0\n",
    "    \n",
    "    print(f\"\\\\n{layer.upper()} Layer Health: {layer_health:.2%}\")\n",
    "    for table_report in layer_tables:\n",
    "        status_icon = \"‚úÖ\" if table_report.get(\"status\") == \"HEALTHY\" else \"‚ùå\"\n",
    "        print(f\"  {status_icon} {table_report['table_name']}: {table_report.get('row_count', 'N/A')} rows, {table_report.get('health_score', 0):.2%} health\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Business Metrics Dashboard\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def generate_business_dashboard():\n",
    "    \"\"\"Generate key business metrics\"\"\"\n",
    "    \n",
    "    print(\"\\\\n=== BUSINESS DASHBOARD ===\")\n",
    "    \n",
    "    try:\n",
    "        # Customer metrics\n",
    "        customer_metrics = spark.table(\"gold_customer_analytics\")\n",
    "        total_customers = customer_metrics.count()\n",
    "        active_customers = customer_metrics.filter(col(\"is_active_customer\") == True).count()\n",
    "        vip_customers = customer_metrics.filter(col(\"customer_segment\") == \"VIP\").count()\n",
    "        \n",
    "        print(f\"\\\\nüìä Customer Metrics:\")\n",
    "        print(f\"   Total Customers: {total_customers:,}\")\n",
    "        print(f\"   Active Customers: {active_customers:,} ({active_customers/total_customers:.1%})\")\n",
    "        print(f\"   VIP Customers: {vip_customers:,} ({vip_customers/total_customers:.1%})\")\n",
    "        \n",
    "        # Sales metrics\n",
    "        sales_metrics = spark.table(\"gold_sales_analytics\")\n",
    "        total_revenue = sales_metrics.agg(sum(\"daily_revenue\")).collect()[0][0] or 0\n",
    "        total_orders = sales_metrics.agg(sum(\"daily_orders\")).collect()[0][0] or 0\n",
    "        avg_order_value = total_revenue / total_orders if total_orders > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\nüí∞ Sales Metrics:\")\n",
    "        print(f\"   Total Revenue: ${total_revenue:,.2f}\")\n",
    "        print(f\"   Total Orders: {total_orders:,}\")\n",
    "        print(f\"   Average Order Value: ${avg_order_value:.2f}\")\n",
    "        \n",
    "        # Product metrics\n",
    "        product_metrics = spark.table(\"gold_product_analytics\")\n",
    "        total_products = product_metrics.count()\n",
    "        top_performer_count = product_metrics.filter(col(\"performance_category\") == \"Top Performer\").count()\n",
    "        no_sales_count = product_metrics.filter(col(\"performance_category\") == \"No Sales\").count()\n",
    "        \n",
    "        print(f\"\\\\nüõçÔ∏è Product Metrics:\")\n",
    "        print(f\"   Total Products: {total_products:,}\")\n",
    "        print(f\"   Top Performers: {top_performer_count:,}\")\n",
    "        print(f\"   Products with No Sales: {no_sales_count:,}\")\n",
    "        \n",
    "        # Category metrics\n",
    "        category_metrics = spark.table(\"gold_category_analytics\")\n",
    "        top_category = category_metrics.orderBy(desc(\"category_revenue\")).first()\n",
    "        \n",
    "        print(f\"\\\\nüì¶ Category Metrics:\")\n",
    "        print(f\"   Top Category: {top_category['category']}\")\n",
    "        print(f\"   Top Category Revenue: ${top_category['category_revenue']:,.2f}\")\n",
    "        print(f\"   Total Categories: {category_metrics.count()}\")\n",
    "        \n",
    "        return {\n",
    "            \"customers\": {\n",
    "                \"total\": total_customers,\n",
    "                \"active\": active_customers,\n",
    "                \"vip\": vip_customers\n",
    "            },\n",
    "            \"sales\": {\n",
    "                \"total_revenue\": float(total_revenue),\n",
    "                \"total_orders\": int(total_orders),\n",
    "                \"avg_order_value\": float(avg_order_value)\n",
    "            },\n",
    "            \"products\": {\n",
    "                \"total\": total_products,\n",
    "                \"top_performers\": top_performer_count,\n",
    "                \"no_sales\": no_sales_count\n",
    "            },\n",
    "            \"categories\": {\n",
    "                \"top_category\": top_category['category'],\n",
    "                \"top_revenue\": float(top_category['category_revenue'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating dashboard: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "business_metrics = generate_business_dashboard()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Anomaly Detection\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def detect_anomalies():\n",
    "    \"\"\"Detect potential data anomalies\"\"\"\n",
    "    \n",
    "    print(\"\\\\n=== ANOMALY DETECTION ===\")\n",
    "    anomalies = []\n",
    "    \n",
    "    try:\n",
    "        # Check for unusual order patterns\n",
    "        order_fact = spark.table(\"gold_order_fact\")\n",
    "        \n",
    "        # Detect very high order values (outliers)\n",
    "        avg_order_value = order_fact.agg(avg(\"order_total\")).collect()[0][0]\n",
    "        high_value_threshold = avg_order_value * 5\n",
    "        high_value_orders = order_fact.filter(col(\"order_total\") > high_value_threshold).count()\n",
    "        \n",
    "        if high_value_orders > 0:\n",
    "            anomalies.append(f\"Found {high_value_orders} orders with unusually high values (>${high_value_threshold:.2f})\")\n",
    "        \n",
    "        # Check for customers with unusual activity\n",
    "        customer_analytics = spark.table(\"gold_customer_analytics\")\n",
    "        avg_orders_per_customer = customer_analytics.agg(avg(\"total_orders\")).collect()[0][0]\n",
    "        high_activity_threshold = avg_orders_per_customer * 10\n",
    "        high_activity_customers = customer_analytics.filter(col(\"total_orders\") > high_activity_threshold).count()\n",
    "        \n",
    "        if high_activity_customers > 0:\n",
    "            anomalies.append(f\"Found {high_activity_customers} customers with unusually high activity (>{high_activity_threshold:.0f} orders)\")\n",
    "        \n",
    "        # Check for data freshness\n",
    "        sales_analytics = spark.table(\"gold_sales_analytics\")\n",
    "        latest_date = sales_analytics.agg(max(\"order_date\")).collect()[0][0]\n",
    "        days_since_latest = (datetime.now().date() - latest_date).days if latest_date else 999\n",
    "        \n",
    "        if days_since_latest > 7:\n",
    "            anomalies.append(f\"Data may be stale - latest order date is {days_since_latest} days old\")\n",
    "        \n",
    "        # Check for zero revenue days\n",
    "        zero_revenue_days = sales_analytics.filter(col(\"daily_revenue\") == 0).count()\n",
    "        if zero_revenue_days > 0:\n",
    "            anomalies.append(f\"Found {zero_revenue_days} days with zero revenue\")\n",
    "        \n",
    "        if anomalies:\n",
    "            print(\"‚ö†Ô∏è Anomalies detected:\")\n",
    "            for anomaly in anomalies:\n",
    "                print(f\"   - {anomaly}\")\n",
    "        else:\n",
    "            print(\"‚úÖ No anomalies detected\")\n",
    "            \n",
    "        return anomalies\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in anomaly detection: {str(e)}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        return [error_msg]\n",
    "\n",
    "detected_anomalies = detect_anomalies()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Pipeline Health Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate overall pipeline health\n",
    "overall_health_score = sum([report.get(\"health_score\", 0) for report in table_health_report if \"health_score\" in report]) / len([report for report in table_health_report if \"health_score\" in report])\n",
    "pipeline_status = \"HEALTHY\" if overall_health_score >= alert_threshold and len(error_tables) == 0 else \"UNHEALTHY\"\n",
    "\n",
    "print(\"\\\\n=== PIPELINE HEALTH SUMMARY ===\")\n",
    "print(f\"Overall Health Score: {overall_health_score:.2%}\")\n",
    "print(f\"Pipeline Status: {pipeline_status}\")\n",
    "print(f\"Tables Status: {len(healthy_tables)}/{len(table_health_report)} healthy\")\n",
    "\n",
    "if detected_anomalies:\n",
    "    print(f\"Anomalies: {len(detected_anomalies)} detected\")\n",
    "else:\n",
    "    print(\"Anomalies: None detected\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Save Monitoring Results\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create monitoring report\n",
    "monitoring_report = {\n",
    "    \"report_metadata\": {\n",
    "        \"report_date\": report_date,\n",
    "        \"report_timestamp\": datetime.now().isoformat(),\n",
    "        \"pipeline_status\": pipeline_status,\n",
    "        \"overall_health_score\": overall_health_score\n",
    "    },\n",
    "    \"table_health\": table_health_report,\n",
    "    \"business_metrics\": business_metrics,\n",
    "    \"anomalies\": detected_anomalies,\n",
    "    \"summary\": {\n",
    "        \"healthy_tables\": len(healthy_tables),\n",
    "        \"unhealthy_tables\": len(unhealthy_tables),\n",
    "        \"error_tables\": len(error_tables),\n",
    "        \"total_tables\": len(table_health_report)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save monitoring report as JSON (could be saved to a Delta table for historical tracking)\n",
    "monitoring_path = f\"{base_path}/monitoring\"\n",
    "report_df = spark.createDataFrame([monitoring_report])\n",
    "\n",
    "# In a real scenario, you would append to a monitoring table for historical tracking\n",
    "print(\"\\\\nSaving monitoring report...\")\n",
    "print(\"Report saved to monitoring layer\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Alerting Logic\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Generate alerts based on conditions\n",
    "alerts = []\n",
    "\n",
    "if pipeline_status == \"UNHEALTHY\":\n",
    "    alerts.append(\"Pipeline health is below threshold\")\n",
    "\n",
    "if len(error_tables) > 0:\n",
    "    alerts.append(f\"{len(error_tables)} tables have errors\")\n",
    "\n",
    "if len(detected_anomalies) > 0:\n",
    "    alerts.append(f\"{len(detected_anomalies)} anomalies detected\")\n",
    "\n",
    "if business_metrics.get(\"sales\", {}).get(\"total_revenue\", 0) == 0:\n",
    "    alerts.append(\"Zero revenue detected\")\n",
    "\n",
    "print(\"\\\\n=== ALERTS ===\")\n",
    "if alerts:\n",
    "    print(\"üö® Active alerts:\")\n",
    "    for alert in alerts:\n",
    "        print(f\"   - {alert}\")\n",
    "else:\n",
    "    print(\"‚úÖ No alerts\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Job Completion\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create final monitoring output\n",
    "monitoring_result = {\n",
    "    \"job_name\": \"data_monitoring\",\n",
    "    \"execution_time\": datetime.now().isoformat(),\n",
    "    \"pipeline_status\": pipeline_status,\n",
    "    \"overall_health_score\": overall_health_score,\n",
    "    \"alerts\": alerts,\n",
    "    \"summary\": {\n",
    "        \"tables_monitored\": len(table_health_report),\n",
    "        \"healthy_tables\": len(healthy_tables),\n",
    "        \"anomalies_detected\": len(detected_anomalies)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\\\n=== MONITORING JOB COMPLETED ===\")\n",
    "print(\"Status:\", pipeline_status)\n",
    "print(\"Health Score:\", f\"{overall_health_score:.2%}\")\n",
    "print(\"Result:\", monitoring_result)\n",
    "\n",
    "# Exit with appropriate status\n",
    "exit_status = \"SUCCESS\" if pipeline_status == \"HEALTHY\" else \"ALERTS_DETECTED\"\n",
    "dbutils.notebook.exit(exit_status)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **How to Set Up the Databricks Job**\n",
    "\n",
    "### **Step 1: Upload Notebooks**\n",
    "1. Create a folder in Databricks workspace: `/Shared/ETL_Pipeline_Demo`\n",
    "2. Upload all 4 notebook files to this folder\n",
    "\n",
    "### **Step 2: Create the Job**\n",
    "1. Go to **Workflows** ‚Üí **Jobs** ‚Üí **Create Job**\n",
    "2. Configure job with these tasks:\n",
    "3. Jobs json is available as _Demojob.json_ file.\n",
    "\n",
    "#### **Task Configuration:**\n",
    "```json\n",
    "{\n",
    "  \"job_name\": \"ETL_Pipeline_Demo\",\n",
    "  \"tasks\": [\n",
    "    {\n",
    "      \"task_key\": \"data_ingestion\",\n",
    "      \"notebook_path\": \"/Shared/ETL_Pipeline_Demo/01_data_ingestion\",\n",
    "      \"cluster_spec\": \"job_cluster\",\n",
    "      \"timeout_seconds\": 3600\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"data_cleansing\", \n",
    "      \"notebook_path\": \"/Shared/ETL_Pipeline_Demo/02_data_cleansing\",\n",
    "      \"depends_on\": [\"data_ingestion\"],\n",
    "      \"cluster_spec\": \"job_cluster\"\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"data_transformation\",\n",
    "      \"notebook_path\": \"/Shared/ETL_Pipeline_Demo/03_data_transformation\", \n",
    "      \"depends_on\": [\"data_cleansing\"],\n",
    "      \"cluster_spec\": \"job_cluster\"\n",
    "    },\n",
    "    {\n",
    "      \"task_key\": \"data_monitoring\",\n",
    "      \"notebook_path\": \"/Shared/ETL_Pipeline_Demo/04_data_monitoring\",\n",
    "      \"depends_on\": [\"data_transformation\"],\n",
    "      \"cluster_spec\": \"job_cluster\"\n",
    "    }\n",
    "  ],\n",
    "  \"schedule\": {\n",
    "    \"cron_expression\": \"0 2 * * *\",\n",
    "    \"timezone\": \"UTC\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "#### Configure job using Databicks cli\n",
    "You can also configure jobs using databricks cli. Steps for using cli\n",
    "- Install Databricks cli on you laptop\n",
    "- Get the CLI token and your databricks workspace URL\n",
    "- Configure your databricks token \n",
    "```\n",
    "databricks configure --token\n",
    "```\n",
    "- Test the cli\n",
    "```\n",
    "databricks workspace ls /\n",
    "```\n",
    "- Get the clusters ids of your workspace\n",
    "```\n",
    "databricks clusters list\n",
    "```\n",
    "- Update the cluster id in json file\n",
    "- Use the below command to create the job\n",
    "```\n",
    "databricks jobs create --json '{\n",
    "  \"name\": \"Demo-ETL-Pipeline\",\n",
    "  \"email_notifications\": {},\n",
    "  .... Full json file\n",
    "  '\n",
    "```\n",
    "\n",
    "### **Step 3: Demo Features**\n",
    "- **Data Lineage**: Bronze ‚Üí Silver ‚Üí Gold progression\n",
    "- **Error Handling**: Quality gates and validation\n",
    "- **Monitoring**: Health checks and anomaly detection\n",
    "- **Alerting**: Automated status reporting\n",
    "- **Parameterization**: Configurable via widgets\n",
    "- **Dependencies**: Tasks run in sequence with proper error handling\n",
    "\n",
    "This complete pipeline demonstrates all key Databricks Jobs features and provides a realistic ETL scenario for your training demo."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "8.1-Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
